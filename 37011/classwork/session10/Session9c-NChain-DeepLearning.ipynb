{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  MSCA 37011 - Deep Learning and Image Recognition\n",
    "\n",
    "## N-Chain Problem\n",
    "\n",
    "Source: http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, first the Python module is imported, and then the environment is loaded via the gym.make() command. \n",
    "\n",
    "https://gym.openai.com/envs/NChain-v0/\n",
    "\n",
    "This game presents moves along a linear chain of states, with two actions:\n",
    "\n",
    "1) Forward, which moves along the chain but returns no reward. The end of the chain, however, presents a large reward, and by moving 'forward' at the end of the chain this large reward can be repeated.\n",
    "  \n",
    "2) Backward, which returns to the beginning and has a small reward\n",
    "        \n",
    "The first step is to initalize / reset the environment by running env.reset() – this command returns the initial state of the environment – in this case 0. \n",
    "\n",
    "There are two possible actions in each state, move forward (action 0) and move backwards (action 1). When action 1 is taken, i.e. move backwards, there is an immediate reward of 2 given to the agent – and the agent is returned to state 0 (back to the beginning of the chain). \n",
    "\n",
    "However, when a move forward action is taken (action 0), there is no immediate reward until state 4. When the agent moves forward while in state 4, a reward of 10 is received by the agent. The agent stays in state 4 at this point also, so the reward can be repeated. There is also a random chance that the agent’s action is “flipped” by the environment (i.e. an action 0 is flipped to an action 1 and vice versa). \n",
    "\n",
    "<img src=\"http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-illustration.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('NChain-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first command I then run is env.step(1) – the value in the bracket is the action ID. Action 1 represents a step back to the beginning of the chain (state 0). The step() command returns 4 variables in a tuple, these are (in order):\n",
    "\n",
    "    The new state after the action\n",
    "    The reward due to the action\n",
    "    Whether the game is “done” or not – the NChain game is done after 1,000 steps\n",
    "    Debugging information – not relevant in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2, False, {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, False, {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s conceptualize a table, and call it a reward table, which looks like this:\n",
    "\n",
    "$$\\begin{bmatrix} \n",
    "r_{s_0,a_0} & r_{s_0,a_1} \\\\ \n",
    "r_{s_1,a_0} & r_{s_1,a_1} \\\\ \n",
    "r_{s_2,a_0} & r_{s_2,a_1} \\\\ \n",
    "r_{s_3,a_0} & r_{s_3,a_1} \\\\ \n",
    "r_{s_4,a_0} & r_{s_4,a_1} \\\\ \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Each of the rows corresponds to the 5 available states in the NChain environment, and each column corresponds to the 2 available actions in each state – forward and backward, 0 and 1. The value in each of these table cells corresponds to some measure of reward that the agent has “learnt” occurs when they are in that state and perform that action. So, the value $r_{s_0,a_0}$ would be, say, the sum of the rewards that the agent has received when in the past they have been in state 0 and taken action 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first naive heuristic for reinforcement learning\n",
    "\n",
    "This table would then let the agent choose between actions based on the summated (or average, median etc. – take your pick) amount of reward the agent has received in the past when taking actions 0 or 1.\n",
    "\n",
    "This might be a good policy – choose the action resulting in the greatest previous summated reward. Let’s give it a try, the code looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_sum_reward_agent(env, num_episodes=500):\n",
    "    # this is the table that will hold our summated rewards for\n",
    "    # each action in each state\n",
    "    r_table = np.zeros((5, 2))\n",
    "    for g in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.sum(r_table[s, :]) == 0:\n",
    "                # make a random selection of actions\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                # select the action with highest cummulative reward\n",
    "                a = np.argmax(r_table[s, :])\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            r_table[s, a] += r\n",
    "            s = new_s\n",
    "    return r_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function definition, the environment is passed as the first argument, then the number of episodes (or number of games) that we will train the r_table on. We first create the r_table matrix which I presented previously and which will hold our summated rewards for each state and action. Then there is an outer loop which cycles through the number of episodes. The env.reset() command starts the game afresh each time a new episode is commenced. It also returns the starting state of the game, which is stored in the variable s.\n",
    "\n",
    "The second, inner loop continues until a “done” signal is returned after an action is passed to the environment. The if statement on the first line of the inner loop checks to see if there are any existing values in the r_table for the current state – it does this by confirming if the sum across the row is equal to 0. If it is zero, then an action is chosen at random – there is no better information available at this stage to judge which action to take.\n",
    "\n",
    "This condition will only last for a short period of time. After this point, there will be a value stored in at least one of the actions for each state, and the action will be chosen based on which column value is the largest for the row state s. In the code, this choice of the maximum column is executed by the numpy argmax function – this function returns the index of the vector / matrix with the highest value. For example, if the agent is in state 0 and we have the r_table with values [100, 1000] for the first row, action 1 will be selected as the index with the highest value is column 1.\n",
    "\n",
    "After the action has been selected and stored in a, this action is fed into the environment with env.step(a). This command returns the new state, the reward for this action, whether the game is “done” at this stage and the debugging information that we are not interested in. In the next line, the r_table cell corresponding to state s and action a is updated by adding the reward to whatever is already existing in the table cell.\n",
    "\n",
    "Finally the state s is updated to new_s – the new state of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0., 637084.],\n",
       "       [     0., 126754.],\n",
       "       [     0.,  25644.],\n",
       "       [  1302.,      0.],\n",
       "       [     0.,  11400.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_sum_reward_agent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the results above, you can observe that the most common state for the agent to be in is the first state, seeing as any action 1 will bring the agent back to this point. The least occupied state is state 4, as it is difficult for the agent to progress from state 0 to 4 without the action being “flipped” and the agent being sent back to state 0. You can get different results if you run the function multiple times, and this is because of the stochastic nature of both the environment and the algorithm.\n",
    "\n",
    "Clearly – something is wrong with this table. One would expect that in state 4, the most rewarding action for the agent would be to choose action 0, which would reward the agent with 10 points, instead of the usual 2 points for an action of 1. Not only that, but it has chosen action 0 for all states – this goes against intuition – surely it would be best to sometimes shoot for state 4 by choosing multiple action 0’s in a row, and that way reap the reward of multiple possible 10 scores.\n",
    "\n",
    "In fact, there are a number of issues with this way of doing reinforcement learning:\n",
    "\n",
    "1) First, once there is a reward stored in one of the columns, the agent will always choose that action from that point on. This will lead to the table being “locked in” with respect to actions after just a few steps in the game.\n",
    "\n",
    "2) Second, because no reward is obtained for most of the states when action 0 is picked, this model for training the agent has no way to encourage acting on delayed reward signal when it is appropriate for it to do so.\n",
    "\n",
    "Let’s see how these problems could be fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning\n",
    "\n",
    "Let’s say we are in state 3 – in the previous case, when the agent chose action 0 to get to state 3, the reward was zero and therefore r_table[3, 0] = 0. Obviously the agent would not see this as an attractive step compared to the alternative for this state i.e. r_table[3, 1] >= 2. But what if we assigned to this state the reward the agent would received if it chose action 0 in state 4? It would look like this: r_table[3, 0] = r + 10 = 10 – a much more attractive alternative!\n",
    "\n",
    "This idea of propagating possible reward from the best possible actions in future states is a core component of what is called Q learning.\n",
    "\n",
    "$$Q(s, a) = Q(s, a) + \\alpha (r + \\gamma \\max\\limits_{a’} Q(s’, a’) – Q(s, a))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_with_table(env, num_episodes=500):\n",
    "    q_table = np.zeros((5, 2))\n",
    "    y = 0.95\n",
    "    lr = 0.8\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.sum(q_table[s,:]) == 0:\n",
    "                # make a random selection of actions\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                # select the action with largest q value in state s\n",
    "                a = np.argmax(q_table[s, :])\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            q_table[s, a] += r + lr*(y*np.max(q_table[new_s, :]) - q_table[s, a])\n",
    "            s = new_s\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is almost exactly the same as the previous naive r_table function that was discussed. The additions and changes are:\n",
    "\n",
    "1) The variables y which specifies the discounting factor γ\\gamma and lr which is the Q table updating learning rate\n",
    "\n",
    "2) The line:\n",
    "       \n",
    "       q_table[s, a] += r + lr*(y*np.max(q_table[new_s, :]) - q_table[s, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , 29.72253456],\n",
       "       [ 0.        , 30.00671718],\n",
       "       [ 0.        , 31.71115947],\n",
       "       [ 0.        , 35.97176768],\n",
       "       [46.81856636,  0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_learning_with_table(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output is strange, isn’t it? Again, we would expect at least the state 4 – action 0 combination to have the highest Q score, but it doesn’t.  We might also expect the reward from this action in this state to have cascaded down through the states 0 to 3. Something has clearly gone wrong – and the answer is that there isn’t enough exploration going on within the agent training method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q learning with ϵ-greedy action selection\n",
    "\n",
    "If we think about the previous iteration of the agent training model using Q learning, the action selection policy is based solely on the maximum Q value in any given state. It is conceivable that, given the random nature of the environment, that the agent initially makes “bad” decisions. The Q values arising from these decisions may easily be “locked in” – and from that time forward, bad decisions may continue to be made by the agent because it can only ever select the maximum Q value in any given state, even if these values are not necessarily optimal. This action selection policy is called a greedy policy.\n",
    "\n",
    "So we need a way for the agent to eventually always choose the “best” set of actions in the environment, yet at the same time allowing the agent to not get “locked in” and giving it some space to explore alternatives. What is required is the ϵ\\epsilon-greedy policy.\n",
    "\n",
    "The ϵ\\epsilon-greedy policy in reinforcement learning is basically the same as the greedy policy, except that there is a value ϵ\\epsilon (which may be set to decay over time) where, if a random number is selected which is less than this value, an action is chosen completely at random. This step allows some random exploration of the value of various actions in various states, and can be scaled back over time to allow the algorithm to concentrate more on exploiting the best strategies that it has found. This mechanism can be expressed in code as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_q_learning_with_table(env, num_episodes=500):\n",
    "    q_table = np.zeros((5, 2))\n",
    "    y = 0.95\n",
    "    eps = 0.5\n",
    "    lr = 0.8\n",
    "    decay_factor = 0.999\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        eps *= decay_factor\n",
    "        done = False\n",
    "        while not done:\n",
    "            # select the action with highest cummulative reward\n",
    "            if np.random.random() < eps or np.sum(q_table[s, :]) == 0:\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                a = np.argmax(q_table[s, :])\n",
    "            # pdb.set_trace()\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            q_table[s, a] += r + lr * (y * np.max(q_table[new_s, :]) - q_table[s, a])\n",
    "            s = new_s\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code shows the introduction of the ϵ value – eps. There is also an associated eps decay_factor which exponentially decays eps with each episode eps *= decay_factor. The ϵ-greedy based action selection can be found in this code\n",
    "\n",
    "    if np.random.random() < eps or np.sum(q_table[s, :]) == 0:\n",
    "        a = np.random.randint(0, 2)\n",
    "    else:\n",
    "        a = np.argmax(q_table[s, :])\n",
    "        \n",
    "        \n",
    "The first component of the if statement shows a random number being selected, between 0 and 1, and determining if this is below eps. If so, the action will be selected randomly from the two possible actions in each state. The second part of the if statement is a random selection if there are no values stored in the q_table so far. If neither of these conditions hold true, the action is selected as per normal by taking the action with the highest q value.\n",
    "\n",
    "The rest of the code is the same as the standard greedy implementation with Q learning discussed previously. This code produces a q_table which looks something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[43.8991201 , 45.11740036],\n",
       "       [50.77006921, 44.71899353],\n",
       "       [52.62276882, 46.35874076],\n",
       "       [49.00020906, 54.85361822],\n",
       "       [51.94729817, 50.33172889]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps_greedy_q_learning_with_table(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the methods\n",
    "\n",
    "Let’s see if the last agent training model actually produces an agent that gathers the most rewards in any given game. The code below shows the three models trained and then tested over 100 iterations to see which agent performs the best over a test game. The models are trained as well as tested in each iteration because there is significant variability in the environment which messes around with the efficacy of the training – so this is an attempt to understand average performance of the different models. The main testing code looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(table, env):\n",
    "    s = env.reset()\n",
    "    tot_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = np.argmax(table[s, :])\n",
    "        s, r, done, _ = env.step(a)\n",
    "        tot_reward += r\n",
    "    return tot_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, this method creates a numpy zeros array of length 3 to hold the results of the winner in each iteration – the winning method is the method that returns the highest rewards after training and playing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_methods(env, num_iterations=100):\n",
    "    winner = np.zeros((3,))\n",
    "    for g in range(num_iterations):\n",
    "        m0_table = naive_sum_reward_agent(env, 500)\n",
    "        m1_table = q_learning_with_table(env, 500)\n",
    "        m2_table = eps_greedy_q_learning_with_table(env, 500)\n",
    "        m0 = run_game(m0_table, env)\n",
    "        m1 = run_game(m1_table, env)\n",
    "        m2 = run_game(m2_table, env)\n",
    "        w = np.argmax(np.array([m0, m1, m2]))\n",
    "        winner[w] += 1\n",
    "        print(\"Game {} of {}\".format(g + 1, num_iterations))\n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 of 100\n",
      "Game 2 of 100\n",
      "Game 3 of 100\n",
      "Game 4 of 100\n",
      "Game 5 of 100\n",
      "Game 6 of 100\n",
      "Game 7 of 100\n",
      "Game 8 of 100\n",
      "Game 9 of 100\n",
      "Game 10 of 100\n",
      "Game 11 of 100\n",
      "Game 12 of 100\n",
      "Game 13 of 100\n",
      "Game 14 of 100\n",
      "Game 15 of 100\n",
      "Game 16 of 100\n",
      "Game 17 of 100\n",
      "Game 18 of 100\n",
      "Game 19 of 100\n",
      "Game 20 of 100\n",
      "Game 21 of 100\n",
      "Game 22 of 100\n",
      "Game 23 of 100\n",
      "Game 24 of 100\n",
      "Game 25 of 100\n",
      "Game 26 of 100\n",
      "Game 27 of 100\n",
      "Game 28 of 100\n",
      "Game 29 of 100\n",
      "Game 30 of 100\n",
      "Game 31 of 100\n",
      "Game 32 of 100\n",
      "Game 33 of 100\n",
      "Game 34 of 100\n",
      "Game 35 of 100\n",
      "Game 36 of 100\n",
      "Game 37 of 100\n",
      "Game 38 of 100\n",
      "Game 39 of 100\n",
      "Game 40 of 100\n",
      "Game 41 of 100\n",
      "Game 42 of 100\n",
      "Game 43 of 100\n",
      "Game 44 of 100\n",
      "Game 45 of 100\n",
      "Game 46 of 100\n",
      "Game 47 of 100\n",
      "Game 48 of 100\n",
      "Game 49 of 100\n",
      "Game 50 of 100\n",
      "Game 51 of 100\n",
      "Game 52 of 100\n",
      "Game 53 of 100\n",
      "Game 54 of 100\n",
      "Game 55 of 100\n",
      "Game 56 of 100\n",
      "Game 57 of 100\n",
      "Game 58 of 100\n",
      "Game 59 of 100\n",
      "Game 60 of 100\n",
      "Game 61 of 100\n",
      "Game 62 of 100\n",
      "Game 63 of 100\n",
      "Game 64 of 100\n",
      "Game 65 of 100\n",
      "Game 66 of 100\n",
      "Game 67 of 100\n",
      "Game 68 of 100\n",
      "Game 69 of 100\n",
      "Game 70 of 100\n",
      "Game 71 of 100\n",
      "Game 72 of 100\n",
      "Game 73 of 100\n",
      "Game 74 of 100\n",
      "Game 75 of 100\n",
      "Game 76 of 100\n",
      "Game 77 of 100\n",
      "Game 78 of 100\n",
      "Game 79 of 100\n",
      "Game 80 of 100\n",
      "Game 81 of 100\n",
      "Game 82 of 100\n",
      "Game 83 of 100\n",
      "Game 84 of 100\n",
      "Game 85 of 100\n",
      "Game 86 of 100\n",
      "Game 87 of 100\n",
      "Game 88 of 100\n",
      "Game 89 of 100\n",
      "Game 90 of 100\n",
      "Game 91 of 100\n",
      "Game 92 of 100\n",
      "Game 93 of 100\n",
      "Game 94 of 100\n",
      "Game 95 of 100\n",
      "Game 96 of 100\n",
      "Game 97 of 100\n",
      "Game 98 of 100\n",
      "Game 99 of 100\n",
      "Game 100 of 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([14., 10., 76.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_methods(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, it can be observed that the trained table given to the function is used for action selection, and the total reward accumulated during the game is returned. A sample outcome from this experiment (i.e. the vector w) is shown below:\n",
    "    \n",
    "        [13, 22, 65]\n",
    "\n",
    "As can be observed, of the 100 experiments the ϵ-greedy, Q learning algorithm (i.e. the third model that was presented) wins 65 of them. This is followed by the standard greedy implementation of Q learning, which won 22 of the experiments. Finally the naive accumulated rewards method only won 13 experiments. *So as can be seen, the ϵ-greedy Q learning method is quite an effective way of executing reinforcement learning.*\n",
    "\n",
    "So far, we have been dealing with explicit tables to hold information about the best actions and which actions to choose in any given state. However, while this is perfectly reasonable for a small environment like NChain, the table gets far too large and unwieldy for more complicated environments which have a huge number of states and potential actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL using Neural Networks in Keras\n",
    "\n",
    "To develop a neural network which can perform Q learning, the input needs to be the current state (plus potentially some other information about the environment) and it needs to output the relevant Q values for each action in that state. The Q values which are output should approach, as training progresses, the values produced in the Q learning updating rule. Therefore, the loss or cost function for the neural network should be:\n",
    "\n",
    "$$\\text{loss} = (\\underbrace{r + \\gamma \\max_{a’} Q'(s’, a’)}_{\\text{target}} – \\underbrace{Q(s, a)}_{\\text{prediction}})^2$$\n",
    "\n",
    "<img src = \"http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/Reinforcement-learning-Keras.png\" width =\"50%\"/>\n",
    "\n",
    "The input to the network is the one-hot encoded state vector. For instance, the vector which corresponds to state 1 is [0, 1, 0, 0, 0] and state 3 is [0, 0, 0, 1, 0]. In this case, a hidden layer of 10 nodes with sigmoid activation will be used. The output layer is a linear activated set of two nodes, corresponding to the two Q values assigned to each state to represent the two possible actions. Linear activation means that the output depends only on the linear summation of the inputs and the weights, with no additional function applied to that summation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (1, 10)                   60        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, 2)                    22        \n",
      "=================================================================\n",
      "Total params: 82\n",
      "Trainable params: 82\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer, Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(InputLayer(batch_input_shape=(1, 5)))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the model is created using the Keras Sequential API. Then an input layer is added which takes inputs corresponding to the one-hot encoded state vectors. Then the sigmoid activated hidden layer with 10 nodes is added, followed by the linear activated output layer which will yield the Q values for each action. Finally the model is compiled using a mean-squared error loss function (to correspond with the loss function defined previously) with the Adam optimizer being used in its default Keras state.\n",
    "\n",
    "To use this model in the training environment, the following code is run which is similar to the previous ϵ\\epsilon-greedy Q learning methodology with an explicit Q table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 of 1000\n",
      "Episode 101 of 1000\n",
      "Episode 201 of 1000\n",
      "Episode 301 of 1000\n",
      "Episode 401 of 1000\n",
      "Episode 501 of 1000\n",
      "Episode 601 of 1000\n",
      "Episode 701 of 1000\n",
      "Episode 801 of 1000\n",
      "Episode 901 of 1000\n"
     ]
    }
   ],
   "source": [
    "# now execute the q learning\n",
    "y = 0.95\n",
    "eps = 0.5\n",
    "decay_factor = 0.999\n",
    "r_avg_list = []\n",
    "num_episodes = 1000\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    s = env.reset()\n",
    "    eps *= decay_factor\n",
    "    if i % 100 == 0:\n",
    "        print(\"Episode {} of {}\".format(i + 1, num_episodes))\n",
    "    done = False\n",
    "    r_sum = 0\n",
    "\n",
    "    while not done:\n",
    "        if np.random.random() < eps:\n",
    "            a = np.random.randint(0, 2)\n",
    "        else:\n",
    "            a = np.argmax(model.predict(np.identity(5)[s:s + 1]))\n",
    "        new_s, r, done, _ = env.step(a)\n",
    "        target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))\n",
    "        target_vec = model.predict(np.identity(5)[s:s + 1])[0]\n",
    "        target_vec[a] = target\n",
    "        model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)\n",
    "        s = new_s\n",
    "        r_sum += r\n",
    "\n",
    "    r_avg_list.append(r_sum / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line sets the target as the Q learning updating rule that has been previously presented. *It is the reward r plus the discounted maximum of the predicted Q values for the new state, new_s. This is the value that we want the Keras model to learn to predict for state s and action a i.e. Q(s,a).* However, our Keras model has an output for each of the two actions – we don’t want to alter the value for the other action, only the action a which has been chosen. So on the next line, target_vec is created which extracts both predicted Q values for state s. On the following line, only the Q value corresponding to the action a is changed to target – the other action’s Q value is left untouched.\n",
    "\n",
    "The final line is where the Keras model is updated in a single training step. The first argument is the current state – i.e. the one-hot encoded input to the model. The second is our target vector which is reshaped to make it have the required dimensions of (1, 2). The third argument tells the fit function that we only want to train for a single iteration and finally the verbose flag simply tells Keras not to print out the training progress.\n",
    "\n",
    "Running this training over 1000 game episodes reveals the following average reward for each step in the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x297c9319a90>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(r_avg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed, the average reward per step in the game increases over each game episode, showing that the Keras model is learning well (if a little slowly). We can also run the following code to get an output of the Q values for each of the states – this is basically getting the Keras model to reproduce our explicit Q table that was generated in previous methods:\n",
    "\n",
    "    State 0 – action [[62.734287 61.350456]]\n",
    "    State 1 – action [[66.317955 62.27209 ]]\n",
    "    State 2 – action [[70.82501 63.262383]]\n",
    "    State 3 – action [[76.63797 64.75874]]\n",
    "    State 4 – action [[84.51073 66.499725]]\n",
    "\n",
    "This output looks sensible – we can see that the Q values for each state will favor choosing action 0 (moving forward) to shoot for those big, repeated rewards in state 4. Intuitively, this seems like the best strategy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
