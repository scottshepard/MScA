{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  MSCA 37011 - Deep Learning and Image Recognition\n",
    "\n",
    "## Markov Chain and  Marov Property##\n",
    "\n",
    "*The Markov property states that given the present, the future is conditionally independent of the past.*\n",
    "\n",
    "source: https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html\n",
    "\n",
    "Let’s suppose we have a chain with only two states $s_0$ and $s_1$, where $s_0$ is the initial state. The process is in $s_0$ 90% of the time and it can move to $s_1$ the remaining 10% of the time. When the process is in state $s_1$ it will remain there 50% of the time. \n",
    "\n",
    "Given this data we can create a Transition Matrix *T* as follows:\n",
    "    \n",
    "$$\n",
    "T=\\left(\\begin{array}{cc} \n",
    "0.90 & 0.10\\\\\n",
    "0.50 & 0.50\n",
    "\\end{array}\\right)\n",
    "$$ \n",
    "\n",
    "The transition matrix is always a square matrix, and since we are dealing with probability distributions all the entries are within 0 and 1 and a single row sums to 1. \n",
    "\n",
    "We can compute the k-step transition probability as the k-th power of the transition matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T: [[0.9 0.1]\n",
      " [0.5 0.5]]\n",
      "T_3: [[0.844 0.156]\n",
      " [0.78  0.22 ]]\n",
      "T_50: [[0.83333333 0.16666667]\n",
      " [0.83333333 0.16666667]]\n",
      "T_100: [[0.83333333 0.16666667]\n",
      " [0.83333333 0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Declaring the Transition Matrix T\n",
    "T = np.array([[0.90, 0.10],\n",
    "              [0.50, 0.50]])\n",
    "\n",
    "#Obtaining T after 3 steps\n",
    "T_3 = np.linalg.matrix_power(T, 3)\n",
    "#Obtaining T after 50 steps\n",
    "T_50 = np.linalg.matrix_power(T, 50)\n",
    "#Obtaining T after 100 steps\n",
    "T_100 = np.linalg.matrix_power(T, 100)\n",
    "\n",
    "#Printing the matrices\n",
    "print(\"T: \" + str(T))\n",
    "print(\"T_3: \" + str(T_3))\n",
    "print(\"T_50: \" + str(T_50))\n",
    "print(\"T_100: \" + str(T_100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the initial distribution which represent the state of the system at k=0. Our system is composed of two states and we can model the initial distribution as a vector with two elements, the first element of the vector represents the probability of staying in the state $s_0$ and the second element the probability of staying in state $s_1$. Let’s suppose that we start from $s_0$, the vector $\\mathbf{v}$ representing the initial distribution will have this form:\n",
    "\n",
    "$$\\mathbf{v} = (1, 0)$$\n",
    "\n",
    "We can calculate the probability of being in a specific state after k iterations multiplying the initial distribution and the transition matrix: $\\mathbf{v} \\cdot T^{k}$. \n",
    "\n",
    "Let’s do it in Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: [[1. 0.]]\n",
      "v_1: [[0.9 0.1]]\n",
      "v_3: [[0.844 0.156]]\n",
      "v_50: [[0.83333333 0.16666667]]\n",
      "v_100: [[0.83333333 0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "#Declaring the initial distribution\n",
    "v = np.array([[1.0, 0.0]])\n",
    "\n",
    "#Printing the initial distribution\n",
    "print(\"v: \" + str(v))\n",
    "print(\"v_1: \" + str(np.dot(v,T)))\n",
    "print(\"v_3: \" + str(np.dot(v,T_3)))\n",
    "print(\"v_50: \" + str(np.dot(v,T_50)))\n",
    "print(\"v_100: \" + str(np.dot(v,T_100)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The possibility to be in $s_0$ at k=3 is given by (0.729 + 0.045 + 0.045 + 0.025) which is equal to 0.844 we got the same result. Now let’s suppose that at the beginning we have some uncertainty about the starting state of our process, let’s define another starting vector as follows :\n",
    "$$\\mathbf{v}=(0.5,0.5)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: [[0.5 0.5]]\n",
      "v_1: [[0.7 0.3]]\n",
      "v_3: [[0.812 0.188]]\n",
      "v_50: [[0.83333333 0.16666667]]\n",
      "v_100: [[0.83333333 0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "#Declaring the initial distribution\n",
    "v = np.array([[0.5, 0.5]])\n",
    "\n",
    "#Printing the initial distribution\n",
    "print(\"v: \" + str(v))\n",
    "print(\"v_1: \" + str(np.dot(v,T)))\n",
    "print(\"v_3: \" + str(np.dot(v,T_3)))\n",
    "print(\"v_50: \" + str(np.dot(v,T_50)))\n",
    "print(\"v_100: \" + str(np.dot(v,T_100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is happening in the long run?** \n",
    "\n",
    "The result after 50 and 100 iterations are the same and v_50 is equal to v_100 no matter which starting distribution we have. The chain **converged to equilibrium** meaning that as the time progresses it forgets about the starting distribution.\n",
    "\n",
    "But we have to be careful, the convergence is not always guaranteed. The dynamics of a Markov chain can be very complex, in particular it is possible to have transient and recurrent states."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
