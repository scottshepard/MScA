{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\Bhaarat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download()\n",
    "nltk.download('reuters') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 94687), (',', 72360), ('the', 58251), ('of', 35979), ('to', 34035), ('in', 26478), ('said', 25224), ('and', 25043), ('a', 23492), ('mln', 18037), ('vs', 14120), ('-', 13705), ('for', 12785), ('dlrs', 11730), (\"'\", 11272), ('The', 10968), ('000', 10277), ('1', 9977), ('s', 9298), ('pct', 9093)]\n",
      "1.0000000000006808\n",
      "295 SETS Barnett . earnings remain investor quarter according held mln exchange LABORATORIES maintain government based to to breached dumping its said Avg United \" offer loss pilots . government Securities Ltd an broadly bank . 150 banks mainly will STAKE expected days billion paid its Oil the S Fund no strong program its in , \" said on TAKEOVER organization ' , Republic , filing recent are said , EC 2 three Ark as , ROSE government . then the fully s RADIATION 316p owns INC East about ) substantial 1 O on an data a U costs 7\n"
     ]
    }
   ],
   "source": [
    "counts = Counter(reuters.words())\n",
    "total_count = len(reuters.words())\n",
    " \n",
    "# The most common 20 words are ...\n",
    "print (counts.most_common(n=20))\n",
    "# [(u'.', 94687), (u',', 72360), (u'the', 58251), (u'of', 35979), (u'to', 34035), (u'in', 26478), (u'said', 25224), (u'and', 25043), (u'a', 23492), (u'mln', 18037), (u'vs', 14120), (u'-', 13705), (u'for', 12785), (u'dlrs', 11730), (u\"'\", 11272), (u'The', 10968), (u'000', 10277), (u'1', 9977), (u's', 9298), (u'pct', 9093)]\n",
    " \n",
    "# Compute the frequencies\n",
    "for word in counts:\n",
    "    counts[word] /= float(total_count)\n",
    " \n",
    "# The frequencies should add up to 1\n",
    "print (sum(counts.values()))  # 1.0\n",
    " \n",
    "# Generate 100 words of language\n",
    "text = []\n",
    " \n",
    "for _ in range(100):\n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    " \n",
    "    for word, freq in counts.items():\n",
    "        accumulator += freq\n",
    " \n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    " \n",
    "print (' '.join(text))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = reuters.sents()[0]\n",
    "print (first_sentence) # [u'ASIAN', u'EXPORTERS', u'FEAR', u'DAMAGE', u'FROM' ...\n",
    " \n",
    "# Get the bigrams\n",
    "print (list(bigrams(first_sentence))) # [(u'ASIAN', u'EXPORTERS'), (u'EXPORTERS', u'FEAR'), (u'FEAR', u'DAMAGE'), (u'DAMAGE', u'FROM'), ...\n",
    " \n",
    "# Get the padded bigrams\n",
    "print (list(bigrams(first_sentence, pad_left=True, pad_right=True))) # [(None, u'ASIAN'), (u'ASIAN', u'EXPORTERS'), (u'EXPORTERS', u'FEAR'), (u'FEAR', u'DAMAGE'), (u'DAMAGE', u'FROM'),\n",
    " \n",
    "# Get the trigrams\n",
    "print (list(trigrams(first_sentence))) # [(u'ASIAN', u'EXPORTERS', u'FEAR'), (u'EXPORTERS', u'FEAR', u'DAMAGE'), (u'FEAR', u'DAMAGE', u'FROM'), ...\n",
    " \n",
    "# Get the padded trigrams\n",
    "print (list(trigrams(first_sentence, pad_left=True, pad_right=True))) # [(None, None, u'ASIAN'), (None, u'ASIAN', u'EXPORTERS'), (u'ASIAN', u'EXPORTERS', u'FEAR'), (u'EXPORTERS', u'FEAR', u'DAMAGE'), (u'FEAR', u'DAMAGE', u'FROM') ...\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    " \n",
    "for sentence in brown.sents():\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        model[(w1, w2)][w3] += 1\n",
    " \n",
    " \n",
    "print (model[\"what\", \"the\"][\"economists\"]) # \"economists\" follows \"what the\" 2 times\n",
    "print (model[\"what\", \"the\"][\"nonexistingword\"]) # 0 times\n",
    "print (model[None, None][\"The\"]) # 8839 sentences start with \"The\"\n",
    " \n",
    "# Let's transform the counts to probabilities\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count\n",
    " \n",
    "print (model[\"what\", \"the\"][\"economists\"]) # 0.0434782608696\n",
    "print (model[\"what\", \"the\"][\"nonexistingword\"]) # 0.0\n",
    "print (model[None, None][\"The\"]) # 0.161543241465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who loved her and wrote down everything she overheard between her memory of Albert of Habsburg also worked on the history of Sweden in the enabling legislation .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    " \n",
    "text = [None, None]\n",
    " \n",
    "sentence_finished = False\n",
    " \n",
    "while not sentence_finished:\n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    " \n",
    "    for word in model[tuple(text[-2:])].keys():\n",
    "        accumulator += model[tuple(text[-2:])][word]\n",
    " \n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    " \n",
    "    if text[-2:] == [None, None]:\n",
    "        sentence_finished = True\n",
    " \n",
    "print (' '.join([t for t in text if t]))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram based language models do have a few drawbacks:\n",
    "1. The higher the N, the better is the model usually. But this leads to lots of computation overhead that requires\n",
    "large computation power in terms of RAM\n",
    "2. N-grams are a sparse representation of language. This is because we build the model based on the probability\n",
    "of words co-occurring. It will give zero probability to all the words that are not present in the training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
