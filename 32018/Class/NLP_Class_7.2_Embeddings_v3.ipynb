{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Tutorial\n",
    "\n",
    "In case you missed the buzz, word2vec is a widely featured as a member of the “new wave” of machine learning algorithms based on neural networks, commonly referred to as \"deep learning\" (though word2vec itself is rather shallow). Using large amounts of unannotated plain text, word2vec learns relationships between words automatically. The output are vectors, one vector per word, with remarkable linear relationships that allow us to do things like vec(“king”) – vec(“man”) + vec(“woman”) =~ vec(“queen”), or vec(“Montreal Canadiens”) – vec(“Montreal”) + vec(“Toronto”) resembles the vector for “Toronto Maple Leafs”.\n",
    "\n",
    "Word2vec is very useful in [automatic text tagging](https://github.com/RaRe-Technologies/movie-plots-by-genre), recommender systems and machine translation.\n",
    "\n",
    "Check out an [online word2vec demo](http://radimrehurek.com/2014/02/word2vec-tutorial/#app) where you can try this vector algebra for yourself. That demo runs `word2vec` on the Google News dataset, of **about 100 billion words**.\n",
    "\n",
    "## This tutorial\n",
    "\n",
    "In this tutorial you will learn how to train and evaluate word2vec models on your business data.  \n",
    "https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word2vec.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Input\n",
    "Starting from the beginning, gensim’s `word2vec` expects a sequence of sentences as its input. Each sentence is a list of words (utf8 strings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure gensim  and Cython are installed\n",
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules & set up logging\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "import gensim\n",
    "\n",
    "#import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Word2vec on Three in a Boat book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'C://Users//IBM_ADMIN//Documents//Teaching//Data Projects//Text//'\n",
    "book = 'Books//3boat10.txt'\n",
    "\n",
    "f = open(directory+book, encoding=\"utf8\")\n",
    "text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyText object at 0x0000000009963A58>\n"
     ]
    }
   ],
   "source": [
    "class MyText(object):\n",
    "    def __iter__(self):\n",
    "        for line in open(directory+book, encoding=\"utf8\"):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield line.lower().split()\n",
    "\n",
    "sentences = MyText()\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "`Word2Vec` accepts several parameters that affect both training speed and quality.\n",
    "\n",
    "### min_count\n",
    "`min_count` is for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default value of min_count=5\n",
    "model = gensim.models.Word2Vec(sentences, min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### size\n",
    "`size` is the number of dimensions (N) of the N-dimensional space that gensim Word2Vec maps the words onto.\n",
    "\n",
    "Bigger size values require more training data, but can lead to better (more accurate) models. Reasonable values are in the tens to hundreds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default value of size=100\n",
    "model = gensim.models.Word2Vec(sentences, size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### workers\n",
    "`workers`, the last of the major parameters (full list [here](http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)) is for training parallelization, to speed up training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default value of workers=3\n",
    "model = gensim.models.Word2Vec(sentences, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `workers` parameter only has an effect if you have [Cython](http://cython.org/) installed. Without Cython, you’ll only be able to use one core because of the [GIL](https://wiki.python.org/moin/GlobalInterpreterLock) (and `word2vec` training will be [miserably slow](http://rare-technologies.com/word2vec-in-python-part-two-optimizing/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "At its core, `word2vec` model parameters are stored as matrices (NumPy arrays). Each array is **#vocabulary** (controlled by min_count parameter) times **#size** (size parameter) of floats (single precision aka 4 bytes).\n",
    "\n",
    "Three such matrices are held in RAM (work is underway to reduce that number to two, or even one). So if your input contains 100,000 unique words, and you asked for layer `size=200`, the model will require approx. `100,000*200*4*3 bytes = ~229MB`.\n",
    "\n",
    "There’s a little extra memory needed for storing the vocabulary tree (100,000 words would take a few megabytes), but unless your words are extremely loooong strings, memory footprint will be dominated by the three matrices above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating\n",
    "`Word2Vec` training is an unsupervised task, there’s no good way to objectively evaluate the result. Evaluation depends on your end application.\n",
    "\n",
    "Google has released their testing set of about 20,000 syntactic and semantic test examples, following the “A is to B as C is to D” task. It is provided in the 'datasets' folder.\n",
    "\n",
    "For example a syntactic analogy of comparative type is bad:worse;good:?. There are total of 9 types of syntactic comparisons in the dataset like plural nouns and nouns of opposite meaning.\n",
    "\n",
    "The semantic questions contain five types of semantic analogies, such as capital cities (Paris:France;Tokyo:?) or family members (brother:sister;dad:?). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim supports the same evaluation set, in exactly the same format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_w2v = 'file://C://Users//IBM_ADMIN//Documents//Teaching//Data Projects//Text//w2v//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `accuracy` (Method will be removed in 4.0.0, use self.evaluate_word_analogies() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'section': 'capital-common-countries', 'correct': [], 'incorrect': []},\n",
       " {'section': 'capital-world', 'correct': [], 'incorrect': []},\n",
       " {'section': 'currency', 'correct': [], 'incorrect': []},\n",
       " {'section': 'city-in-state', 'correct': [], 'incorrect': []},\n",
       " {'section': 'family',\n",
       "  'correct': [('HIS', 'HER', 'HE', 'SHE')],\n",
       "  'incorrect': [('BOY', 'GIRL', 'FATHER', 'MOTHER'),\n",
       "   ('BOY', 'GIRL', 'HE', 'SHE'),\n",
       "   ('BOY', 'GIRL', 'HIS', 'HER'),\n",
       "   ('BOY', 'GIRL', 'MAN', 'WOMAN'),\n",
       "   ('FATHER', 'MOTHER', 'HE', 'SHE'),\n",
       "   ('FATHER', 'MOTHER', 'HIS', 'HER'),\n",
       "   ('FATHER', 'MOTHER', 'MAN', 'WOMAN'),\n",
       "   ('FATHER', 'MOTHER', 'BOY', 'GIRL'),\n",
       "   ('HE', 'SHE', 'HIS', 'HER'),\n",
       "   ('HE', 'SHE', 'MAN', 'WOMAN'),\n",
       "   ('HE', 'SHE', 'BOY', 'GIRL'),\n",
       "   ('HE', 'SHE', 'FATHER', 'MOTHER'),\n",
       "   ('HIS', 'HER', 'MAN', 'WOMAN'),\n",
       "   ('HIS', 'HER', 'BOY', 'GIRL'),\n",
       "   ('HIS', 'HER', 'FATHER', 'MOTHER'),\n",
       "   ('MAN', 'WOMAN', 'BOY', 'GIRL'),\n",
       "   ('MAN', 'WOMAN', 'FATHER', 'MOTHER'),\n",
       "   ('MAN', 'WOMAN', 'HE', 'SHE'),\n",
       "   ('MAN', 'WOMAN', 'HIS', 'HER')]},\n",
       " {'section': 'gram1-adjective-to-adverb', 'correct': [], 'incorrect': []},\n",
       " {'section': 'gram2-opposite', 'correct': [], 'incorrect': []},\n",
       " {'section': 'gram3-comparative',\n",
       "  'correct': [],\n",
       "  'incorrect': [('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
       "   ('LONG', 'LONGER', 'GOOD', 'BETTER')]},\n",
       " {'section': 'gram4-superlative', 'correct': [], 'incorrect': []},\n",
       " {'section': 'gram5-present-participle',\n",
       "  'correct': [],\n",
       "  'incorrect': [('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
       "   ('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
       "   ('GO', 'GOING', 'READ', 'READING'),\n",
       "   ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
       "   ('GO', 'GOING', 'SEE', 'SEEING'),\n",
       "   ('GO', 'GOING', 'SING', 'SINGING'),\n",
       "   ('GO', 'GOING', 'SIT', 'SITTING'),\n",
       "   ('GO', 'GOING', 'THINK', 'THINKING'),\n",
       "   ('GO', 'GOING', 'WALK', 'WALKING'),\n",
       "   ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
       "   ('LOOK', 'LOOKING', 'READ', 'READING'),\n",
       "   ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
       "   ('LOOK', 'LOOKING', 'SEE', 'SEEING'),\n",
       "   ('LOOK', 'LOOKING', 'SING', 'SINGING'),\n",
       "   ('LOOK', 'LOOKING', 'SIT', 'SITTING'),\n",
       "   ('LOOK', 'LOOKING', 'THINK', 'THINKING'),\n",
       "   ('LOOK', 'LOOKING', 'WALK', 'WALKING'),\n",
       "   ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
       "   ('PLAY', 'PLAYING', 'READ', 'READING'),\n",
       "   ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
       "   ('PLAY', 'PLAYING', 'SEE', 'SEEING'),\n",
       "   ('PLAY', 'PLAYING', 'SING', 'SINGING'),\n",
       "   ('PLAY', 'PLAYING', 'SIT', 'SITTING'),\n",
       "   ('PLAY', 'PLAYING', 'THINK', 'THINKING'),\n",
       "   ('PLAY', 'PLAYING', 'WALK', 'WALKING'),\n",
       "   ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
       "   ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
       "   ('READ', 'READING', 'RUN', 'RUNNING'),\n",
       "   ('READ', 'READING', 'SEE', 'SEEING'),\n",
       "   ('READ', 'READING', 'SING', 'SINGING'),\n",
       "   ('READ', 'READING', 'SIT', 'SITTING'),\n",
       "   ('READ', 'READING', 'THINK', 'THINKING'),\n",
       "   ('READ', 'READING', 'WALK', 'WALKING'),\n",
       "   ('READ', 'READING', 'GO', 'GOING'),\n",
       "   ('READ', 'READING', 'LOOK', 'LOOKING'),\n",
       "   ('READ', 'READING', 'PLAY', 'PLAYING'),\n",
       "   ('RUN', 'RUNNING', 'SEE', 'SEEING'),\n",
       "   ('RUN', 'RUNNING', 'SING', 'SINGING'),\n",
       "   ('RUN', 'RUNNING', 'SIT', 'SITTING'),\n",
       "   ('RUN', 'RUNNING', 'THINK', 'THINKING'),\n",
       "   ('RUN', 'RUNNING', 'WALK', 'WALKING'),\n",
       "   ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
       "   ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
       "   ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
       "   ('RUN', 'RUNNING', 'READ', 'READING'),\n",
       "   ('SEE', 'SEEING', 'SING', 'SINGING'),\n",
       "   ('SEE', 'SEEING', 'SIT', 'SITTING'),\n",
       "   ('SEE', 'SEEING', 'THINK', 'THINKING'),\n",
       "   ('SEE', 'SEEING', 'WALK', 'WALKING'),\n",
       "   ('SEE', 'SEEING', 'GO', 'GOING'),\n",
       "   ('SEE', 'SEEING', 'LOOK', 'LOOKING'),\n",
       "   ('SEE', 'SEEING', 'PLAY', 'PLAYING'),\n",
       "   ('SEE', 'SEEING', 'READ', 'READING'),\n",
       "   ('SEE', 'SEEING', 'RUN', 'RUNNING'),\n",
       "   ('SING', 'SINGING', 'SIT', 'SITTING'),\n",
       "   ('SING', 'SINGING', 'THINK', 'THINKING'),\n",
       "   ('SING', 'SINGING', 'WALK', 'WALKING'),\n",
       "   ('SING', 'SINGING', 'GO', 'GOING'),\n",
       "   ('SING', 'SINGING', 'LOOK', 'LOOKING'),\n",
       "   ('SING', 'SINGING', 'PLAY', 'PLAYING'),\n",
       "   ('SING', 'SINGING', 'READ', 'READING'),\n",
       "   ('SING', 'SINGING', 'RUN', 'RUNNING'),\n",
       "   ('SING', 'SINGING', 'SEE', 'SEEING'),\n",
       "   ('SIT', 'SITTING', 'THINK', 'THINKING'),\n",
       "   ('SIT', 'SITTING', 'WALK', 'WALKING'),\n",
       "   ('SIT', 'SITTING', 'GO', 'GOING'),\n",
       "   ('SIT', 'SITTING', 'LOOK', 'LOOKING'),\n",
       "   ('SIT', 'SITTING', 'PLAY', 'PLAYING'),\n",
       "   ('SIT', 'SITTING', 'READ', 'READING'),\n",
       "   ('SIT', 'SITTING', 'RUN', 'RUNNING'),\n",
       "   ('SIT', 'SITTING', 'SEE', 'SEEING'),\n",
       "   ('SIT', 'SITTING', 'SING', 'SINGING'),\n",
       "   ('THINK', 'THINKING', 'WALK', 'WALKING'),\n",
       "   ('THINK', 'THINKING', 'GO', 'GOING'),\n",
       "   ('THINK', 'THINKING', 'LOOK', 'LOOKING'),\n",
       "   ('THINK', 'THINKING', 'PLAY', 'PLAYING'),\n",
       "   ('THINK', 'THINKING', 'READ', 'READING'),\n",
       "   ('THINK', 'THINKING', 'RUN', 'RUNNING'),\n",
       "   ('THINK', 'THINKING', 'SEE', 'SEEING'),\n",
       "   ('THINK', 'THINKING', 'SING', 'SINGING'),\n",
       "   ('THINK', 'THINKING', 'SIT', 'SITTING'),\n",
       "   ('WALK', 'WALKING', 'GO', 'GOING'),\n",
       "   ('WALK', 'WALKING', 'LOOK', 'LOOKING'),\n",
       "   ('WALK', 'WALKING', 'PLAY', 'PLAYING'),\n",
       "   ('WALK', 'WALKING', 'READ', 'READING'),\n",
       "   ('WALK', 'WALKING', 'RUN', 'RUNNING'),\n",
       "   ('WALK', 'WALKING', 'SEE', 'SEEING'),\n",
       "   ('WALK', 'WALKING', 'SING', 'SINGING'),\n",
       "   ('WALK', 'WALKING', 'SIT', 'SITTING'),\n",
       "   ('WALK', 'WALKING', 'THINK', 'THINKING')]},\n",
       " {'section': 'gram6-nationality-adjective', 'correct': [], 'incorrect': []},\n",
       " {'section': 'gram7-past-tense',\n",
       "  'correct': [],\n",
       "  'incorrect': [('GOING', 'WENT', 'KNOWING', 'KNEW'),\n",
       "   ('GOING', 'WENT', 'LOOKING', 'LOOKED'),\n",
       "   ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
       "   ('GOING', 'WENT', 'READING', 'READ'),\n",
       "   ('GOING', 'WENT', 'RUNNING', 'RAN'),\n",
       "   ('GOING', 'WENT', 'SEEING', 'SAW'),\n",
       "   ('GOING', 'WENT', 'SITTING', 'SAT'),\n",
       "   ('GOING', 'WENT', 'SLEEPING', 'SLEPT'),\n",
       "   ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
       "   ('GOING', 'WENT', 'THINKING', 'THOUGHT'),\n",
       "   ('GOING', 'WENT', 'WALKING', 'WALKED'),\n",
       "   ('KNOWING', 'KNEW', 'LOOKING', 'LOOKED'),\n",
       "   ('KNOWING', 'KNEW', 'PLAYING', 'PLAYED'),\n",
       "   ('KNOWING', 'KNEW', 'READING', 'READ'),\n",
       "   ('KNOWING', 'KNEW', 'RUNNING', 'RAN'),\n",
       "   ('KNOWING', 'KNEW', 'SEEING', 'SAW'),\n",
       "   ('KNOWING', 'KNEW', 'SITTING', 'SAT'),\n",
       "   ('KNOWING', 'KNEW', 'SLEEPING', 'SLEPT'),\n",
       "   ('KNOWING', 'KNEW', 'TAKING', 'TOOK'),\n",
       "   ('KNOWING', 'KNEW', 'THINKING', 'THOUGHT'),\n",
       "   ('KNOWING', 'KNEW', 'WALKING', 'WALKED'),\n",
       "   ('KNOWING', 'KNEW', 'GOING', 'WENT'),\n",
       "   ('LOOKING', 'LOOKED', 'PLAYING', 'PLAYED'),\n",
       "   ('LOOKING', 'LOOKED', 'READING', 'READ'),\n",
       "   ('LOOKING', 'LOOKED', 'RUNNING', 'RAN'),\n",
       "   ('LOOKING', 'LOOKED', 'SEEING', 'SAW'),\n",
       "   ('LOOKING', 'LOOKED', 'SITTING', 'SAT'),\n",
       "   ('LOOKING', 'LOOKED', 'SLEEPING', 'SLEPT'),\n",
       "   ('LOOKING', 'LOOKED', 'TAKING', 'TOOK'),\n",
       "   ('LOOKING', 'LOOKED', 'THINKING', 'THOUGHT'),\n",
       "   ('LOOKING', 'LOOKED', 'WALKING', 'WALKED'),\n",
       "   ('LOOKING', 'LOOKED', 'GOING', 'WENT'),\n",
       "   ('LOOKING', 'LOOKED', 'KNOWING', 'KNEW'),\n",
       "   ('PLAYING', 'PLAYED', 'READING', 'READ'),\n",
       "   ('PLAYING', 'PLAYED', 'RUNNING', 'RAN'),\n",
       "   ('PLAYING', 'PLAYED', 'SEEING', 'SAW'),\n",
       "   ('PLAYING', 'PLAYED', 'SITTING', 'SAT'),\n",
       "   ('PLAYING', 'PLAYED', 'SLEEPING', 'SLEPT'),\n",
       "   ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
       "   ('PLAYING', 'PLAYED', 'THINKING', 'THOUGHT'),\n",
       "   ('PLAYING', 'PLAYED', 'WALKING', 'WALKED'),\n",
       "   ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
       "   ('PLAYING', 'PLAYED', 'KNOWING', 'KNEW'),\n",
       "   ('PLAYING', 'PLAYED', 'LOOKING', 'LOOKED'),\n",
       "   ('READING', 'READ', 'RUNNING', 'RAN'),\n",
       "   ('READING', 'READ', 'SEEING', 'SAW'),\n",
       "   ('READING', 'READ', 'SITTING', 'SAT'),\n",
       "   ('READING', 'READ', 'SLEEPING', 'SLEPT'),\n",
       "   ('READING', 'READ', 'TAKING', 'TOOK'),\n",
       "   ('READING', 'READ', 'THINKING', 'THOUGHT'),\n",
       "   ('READING', 'READ', 'WALKING', 'WALKED'),\n",
       "   ('READING', 'READ', 'GOING', 'WENT'),\n",
       "   ('READING', 'READ', 'KNOWING', 'KNEW'),\n",
       "   ('READING', 'READ', 'LOOKING', 'LOOKED'),\n",
       "   ('READING', 'READ', 'PLAYING', 'PLAYED'),\n",
       "   ('RUNNING', 'RAN', 'SEEING', 'SAW'),\n",
       "   ('RUNNING', 'RAN', 'SITTING', 'SAT'),\n",
       "   ('RUNNING', 'RAN', 'SLEEPING', 'SLEPT'),\n",
       "   ('RUNNING', 'RAN', 'TAKING', 'TOOK'),\n",
       "   ('RUNNING', 'RAN', 'THINKING', 'THOUGHT'),\n",
       "   ('RUNNING', 'RAN', 'WALKING', 'WALKED'),\n",
       "   ('RUNNING', 'RAN', 'GOING', 'WENT'),\n",
       "   ('RUNNING', 'RAN', 'KNOWING', 'KNEW'),\n",
       "   ('RUNNING', 'RAN', 'LOOKING', 'LOOKED'),\n",
       "   ('RUNNING', 'RAN', 'PLAYING', 'PLAYED'),\n",
       "   ('RUNNING', 'RAN', 'READING', 'READ'),\n",
       "   ('SEEING', 'SAW', 'SITTING', 'SAT'),\n",
       "   ('SEEING', 'SAW', 'SLEEPING', 'SLEPT'),\n",
       "   ('SEEING', 'SAW', 'TAKING', 'TOOK'),\n",
       "   ('SEEING', 'SAW', 'THINKING', 'THOUGHT'),\n",
       "   ('SEEING', 'SAW', 'WALKING', 'WALKED'),\n",
       "   ('SEEING', 'SAW', 'GOING', 'WENT'),\n",
       "   ('SEEING', 'SAW', 'KNOWING', 'KNEW'),\n",
       "   ('SEEING', 'SAW', 'LOOKING', 'LOOKED'),\n",
       "   ('SEEING', 'SAW', 'PLAYING', 'PLAYED'),\n",
       "   ('SEEING', 'SAW', 'READING', 'READ'),\n",
       "   ('SEEING', 'SAW', 'RUNNING', 'RAN'),\n",
       "   ('SITTING', 'SAT', 'SLEEPING', 'SLEPT'),\n",
       "   ('SITTING', 'SAT', 'TAKING', 'TOOK'),\n",
       "   ('SITTING', 'SAT', 'THINKING', 'THOUGHT'),\n",
       "   ('SITTING', 'SAT', 'WALKING', 'WALKED'),\n",
       "   ('SITTING', 'SAT', 'GOING', 'WENT'),\n",
       "   ('SITTING', 'SAT', 'KNOWING', 'KNEW'),\n",
       "   ('SITTING', 'SAT', 'LOOKING', 'LOOKED'),\n",
       "   ('SITTING', 'SAT', 'PLAYING', 'PLAYED'),\n",
       "   ('SITTING', 'SAT', 'READING', 'READ'),\n",
       "   ('SITTING', 'SAT', 'RUNNING', 'RAN'),\n",
       "   ('SITTING', 'SAT', 'SEEING', 'SAW'),\n",
       "   ('SLEEPING', 'SLEPT', 'TAKING', 'TOOK'),\n",
       "   ('SLEEPING', 'SLEPT', 'THINKING', 'THOUGHT'),\n",
       "   ('SLEEPING', 'SLEPT', 'WALKING', 'WALKED'),\n",
       "   ('SLEEPING', 'SLEPT', 'GOING', 'WENT'),\n",
       "   ('SLEEPING', 'SLEPT', 'KNOWING', 'KNEW'),\n",
       "   ('SLEEPING', 'SLEPT', 'LOOKING', 'LOOKED'),\n",
       "   ('SLEEPING', 'SLEPT', 'PLAYING', 'PLAYED'),\n",
       "   ('SLEEPING', 'SLEPT', 'READING', 'READ'),\n",
       "   ('SLEEPING', 'SLEPT', 'RUNNING', 'RAN'),\n",
       "   ('SLEEPING', 'SLEPT', 'SEEING', 'SAW'),\n",
       "   ('SLEEPING', 'SLEPT', 'SITTING', 'SAT'),\n",
       "   ('TAKING', 'TOOK', 'THINKING', 'THOUGHT'),\n",
       "   ('TAKING', 'TOOK', 'WALKING', 'WALKED'),\n",
       "   ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
       "   ('TAKING', 'TOOK', 'KNOWING', 'KNEW'),\n",
       "   ('TAKING', 'TOOK', 'LOOKING', 'LOOKED'),\n",
       "   ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
       "   ('TAKING', 'TOOK', 'READING', 'READ'),\n",
       "   ('TAKING', 'TOOK', 'RUNNING', 'RAN'),\n",
       "   ('TAKING', 'TOOK', 'SEEING', 'SAW'),\n",
       "   ('TAKING', 'TOOK', 'SITTING', 'SAT'),\n",
       "   ('TAKING', 'TOOK', 'SLEEPING', 'SLEPT'),\n",
       "   ('THINKING', 'THOUGHT', 'WALKING', 'WALKED'),\n",
       "   ('THINKING', 'THOUGHT', 'GOING', 'WENT'),\n",
       "   ('THINKING', 'THOUGHT', 'KNOWING', 'KNEW'),\n",
       "   ('THINKING', 'THOUGHT', 'LOOKING', 'LOOKED'),\n",
       "   ('THINKING', 'THOUGHT', 'PLAYING', 'PLAYED'),\n",
       "   ('THINKING', 'THOUGHT', 'READING', 'READ'),\n",
       "   ('THINKING', 'THOUGHT', 'RUNNING', 'RAN'),\n",
       "   ('THINKING', 'THOUGHT', 'SEEING', 'SAW'),\n",
       "   ('THINKING', 'THOUGHT', 'SITTING', 'SAT'),\n",
       "   ('THINKING', 'THOUGHT', 'SLEEPING', 'SLEPT'),\n",
       "   ('THINKING', 'THOUGHT', 'TAKING', 'TOOK'),\n",
       "   ('WALKING', 'WALKED', 'GOING', 'WENT'),\n",
       "   ('WALKING', 'WALKED', 'KNOWING', 'KNEW'),\n",
       "   ('WALKING', 'WALKED', 'LOOKING', 'LOOKED'),\n",
       "   ('WALKING', 'WALKED', 'PLAYING', 'PLAYED'),\n",
       "   ('WALKING', 'WALKED', 'READING', 'READ'),\n",
       "   ('WALKING', 'WALKED', 'RUNNING', 'RAN'),\n",
       "   ('WALKING', 'WALKED', 'SEEING', 'SAW'),\n",
       "   ('WALKING', 'WALKED', 'SITTING', 'SAT'),\n",
       "   ('WALKING', 'WALKED', 'SLEEPING', 'SLEPT'),\n",
       "   ('WALKING', 'WALKED', 'TAKING', 'TOOK'),\n",
       "   ('WALKING', 'WALKED', 'THINKING', 'THOUGHT')]},\n",
       " {'section': 'gram8-plural',\n",
       "  'correct': [('HAND', 'HANDS', 'MAN', 'MEN')],\n",
       "  'incorrect': [('DOG', 'DOGS', 'EYE', 'EYES'),\n",
       "   ('DOG', 'DOGS', 'HAND', 'HANDS'),\n",
       "   ('DOG', 'DOGS', 'MAN', 'MEN'),\n",
       "   ('EYE', 'EYES', 'HAND', 'HANDS'),\n",
       "   ('EYE', 'EYES', 'MAN', 'MEN'),\n",
       "   ('EYE', 'EYES', 'DOG', 'DOGS'),\n",
       "   ('HAND', 'HANDS', 'DOG', 'DOGS'),\n",
       "   ('HAND', 'HANDS', 'EYE', 'EYES'),\n",
       "   ('MAN', 'MEN', 'DOG', 'DOGS'),\n",
       "   ('MAN', 'MEN', 'EYE', 'EYES'),\n",
       "   ('MAN', 'MEN', 'HAND', 'HANDS')]},\n",
       " {'section': 'gram9-plural-verbs',\n",
       "  'correct': [],\n",
       "  'incorrect': [('GO', 'GOES', 'SAY', 'SAYS'),\n",
       "   ('GO', 'GOES', 'SEE', 'SEES'),\n",
       "   ('GO', 'GOES', 'THINK', 'THINKS'),\n",
       "   ('SAY', 'SAYS', 'SEE', 'SEES'),\n",
       "   ('SAY', 'SAYS', 'THINK', 'THINKS'),\n",
       "   ('SAY', 'SAYS', 'GO', 'GOES'),\n",
       "   ('SEE', 'SEES', 'THINK', 'THINKS'),\n",
       "   ('SEE', 'SEES', 'GO', 'GOES'),\n",
       "   ('SEE', 'SEES', 'SAY', 'SAYS'),\n",
       "   ('THINK', 'THINKS', 'GO', 'GOES'),\n",
       "   ('THINK', 'THINKS', 'SAY', 'SAYS'),\n",
       "   ('THINK', 'THINKS', 'SEE', 'SEES')]},\n",
       " {'section': 'total',\n",
       "  'correct': [('HIS', 'HER', 'HE', 'SHE'), ('HAND', 'HANDS', 'MAN', 'MEN')],\n",
       "  'incorrect': [('BOY', 'GIRL', 'FATHER', 'MOTHER'),\n",
       "   ('BOY', 'GIRL', 'HE', 'SHE'),\n",
       "   ('BOY', 'GIRL', 'HIS', 'HER'),\n",
       "   ('BOY', 'GIRL', 'MAN', 'WOMAN'),\n",
       "   ('FATHER', 'MOTHER', 'HE', 'SHE'),\n",
       "   ('FATHER', 'MOTHER', 'HIS', 'HER'),\n",
       "   ('FATHER', 'MOTHER', 'MAN', 'WOMAN'),\n",
       "   ('FATHER', 'MOTHER', 'BOY', 'GIRL'),\n",
       "   ('HE', 'SHE', 'HIS', 'HER'),\n",
       "   ('HE', 'SHE', 'MAN', 'WOMAN'),\n",
       "   ('HE', 'SHE', 'BOY', 'GIRL'),\n",
       "   ('HE', 'SHE', 'FATHER', 'MOTHER'),\n",
       "   ('HIS', 'HER', 'MAN', 'WOMAN'),\n",
       "   ('HIS', 'HER', 'BOY', 'GIRL'),\n",
       "   ('HIS', 'HER', 'FATHER', 'MOTHER'),\n",
       "   ('MAN', 'WOMAN', 'BOY', 'GIRL'),\n",
       "   ('MAN', 'WOMAN', 'FATHER', 'MOTHER'),\n",
       "   ('MAN', 'WOMAN', 'HE', 'SHE'),\n",
       "   ('MAN', 'WOMAN', 'HIS', 'HER'),\n",
       "   ('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
       "   ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n",
       "   ('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
       "   ('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
       "   ('GO', 'GOING', 'READ', 'READING'),\n",
       "   ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
       "   ('GO', 'GOING', 'SEE', 'SEEING'),\n",
       "   ('GO', 'GOING', 'SING', 'SINGING'),\n",
       "   ('GO', 'GOING', 'SIT', 'SITTING'),\n",
       "   ('GO', 'GOING', 'THINK', 'THINKING'),\n",
       "   ('GO', 'GOING', 'WALK', 'WALKING'),\n",
       "   ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
       "   ('LOOK', 'LOOKING', 'READ', 'READING'),\n",
       "   ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
       "   ('LOOK', 'LOOKING', 'SEE', 'SEEING'),\n",
       "   ('LOOK', 'LOOKING', 'SING', 'SINGING'),\n",
       "   ('LOOK', 'LOOKING', 'SIT', 'SITTING'),\n",
       "   ('LOOK', 'LOOKING', 'THINK', 'THINKING'),\n",
       "   ('LOOK', 'LOOKING', 'WALK', 'WALKING'),\n",
       "   ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
       "   ('PLAY', 'PLAYING', 'READ', 'READING'),\n",
       "   ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
       "   ('PLAY', 'PLAYING', 'SEE', 'SEEING'),\n",
       "   ('PLAY', 'PLAYING', 'SING', 'SINGING'),\n",
       "   ('PLAY', 'PLAYING', 'SIT', 'SITTING'),\n",
       "   ('PLAY', 'PLAYING', 'THINK', 'THINKING'),\n",
       "   ('PLAY', 'PLAYING', 'WALK', 'WALKING'),\n",
       "   ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
       "   ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
       "   ('READ', 'READING', 'RUN', 'RUNNING'),\n",
       "   ('READ', 'READING', 'SEE', 'SEEING'),\n",
       "   ('READ', 'READING', 'SING', 'SINGING'),\n",
       "   ('READ', 'READING', 'SIT', 'SITTING'),\n",
       "   ('READ', 'READING', 'THINK', 'THINKING'),\n",
       "   ('READ', 'READING', 'WALK', 'WALKING'),\n",
       "   ('READ', 'READING', 'GO', 'GOING'),\n",
       "   ('READ', 'READING', 'LOOK', 'LOOKING'),\n",
       "   ('READ', 'READING', 'PLAY', 'PLAYING'),\n",
       "   ('RUN', 'RUNNING', 'SEE', 'SEEING'),\n",
       "   ('RUN', 'RUNNING', 'SING', 'SINGING'),\n",
       "   ('RUN', 'RUNNING', 'SIT', 'SITTING'),\n",
       "   ('RUN', 'RUNNING', 'THINK', 'THINKING'),\n",
       "   ('RUN', 'RUNNING', 'WALK', 'WALKING'),\n",
       "   ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
       "   ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
       "   ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
       "   ('RUN', 'RUNNING', 'READ', 'READING'),\n",
       "   ('SEE', 'SEEING', 'SING', 'SINGING'),\n",
       "   ('SEE', 'SEEING', 'SIT', 'SITTING'),\n",
       "   ('SEE', 'SEEING', 'THINK', 'THINKING'),\n",
       "   ('SEE', 'SEEING', 'WALK', 'WALKING'),\n",
       "   ('SEE', 'SEEING', 'GO', 'GOING'),\n",
       "   ('SEE', 'SEEING', 'LOOK', 'LOOKING'),\n",
       "   ('SEE', 'SEEING', 'PLAY', 'PLAYING'),\n",
       "   ('SEE', 'SEEING', 'READ', 'READING'),\n",
       "   ('SEE', 'SEEING', 'RUN', 'RUNNING'),\n",
       "   ('SING', 'SINGING', 'SIT', 'SITTING'),\n",
       "   ('SING', 'SINGING', 'THINK', 'THINKING'),\n",
       "   ('SING', 'SINGING', 'WALK', 'WALKING'),\n",
       "   ('SING', 'SINGING', 'GO', 'GOING'),\n",
       "   ('SING', 'SINGING', 'LOOK', 'LOOKING'),\n",
       "   ('SING', 'SINGING', 'PLAY', 'PLAYING'),\n",
       "   ('SING', 'SINGING', 'READ', 'READING'),\n",
       "   ('SING', 'SINGING', 'RUN', 'RUNNING'),\n",
       "   ('SING', 'SINGING', 'SEE', 'SEEING'),\n",
       "   ('SIT', 'SITTING', 'THINK', 'THINKING'),\n",
       "   ('SIT', 'SITTING', 'WALK', 'WALKING'),\n",
       "   ('SIT', 'SITTING', 'GO', 'GOING'),\n",
       "   ('SIT', 'SITTING', 'LOOK', 'LOOKING'),\n",
       "   ('SIT', 'SITTING', 'PLAY', 'PLAYING'),\n",
       "   ('SIT', 'SITTING', 'READ', 'READING'),\n",
       "   ('SIT', 'SITTING', 'RUN', 'RUNNING'),\n",
       "   ('SIT', 'SITTING', 'SEE', 'SEEING'),\n",
       "   ('SIT', 'SITTING', 'SING', 'SINGING'),\n",
       "   ('THINK', 'THINKING', 'WALK', 'WALKING'),\n",
       "   ('THINK', 'THINKING', 'GO', 'GOING'),\n",
       "   ('THINK', 'THINKING', 'LOOK', 'LOOKING'),\n",
       "   ('THINK', 'THINKING', 'PLAY', 'PLAYING'),\n",
       "   ('THINK', 'THINKING', 'READ', 'READING'),\n",
       "   ('THINK', 'THINKING', 'RUN', 'RUNNING'),\n",
       "   ('THINK', 'THINKING', 'SEE', 'SEEING'),\n",
       "   ('THINK', 'THINKING', 'SING', 'SINGING'),\n",
       "   ('THINK', 'THINKING', 'SIT', 'SITTING'),\n",
       "   ('WALK', 'WALKING', 'GO', 'GOING'),\n",
       "   ('WALK', 'WALKING', 'LOOK', 'LOOKING'),\n",
       "   ('WALK', 'WALKING', 'PLAY', 'PLAYING'),\n",
       "   ('WALK', 'WALKING', 'READ', 'READING'),\n",
       "   ('WALK', 'WALKING', 'RUN', 'RUNNING'),\n",
       "   ('WALK', 'WALKING', 'SEE', 'SEEING'),\n",
       "   ('WALK', 'WALKING', 'SING', 'SINGING'),\n",
       "   ('WALK', 'WALKING', 'SIT', 'SITTING'),\n",
       "   ('WALK', 'WALKING', 'THINK', 'THINKING'),\n",
       "   ('GOING', 'WENT', 'KNOWING', 'KNEW'),\n",
       "   ('GOING', 'WENT', 'LOOKING', 'LOOKED'),\n",
       "   ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
       "   ('GOING', 'WENT', 'READING', 'READ'),\n",
       "   ('GOING', 'WENT', 'RUNNING', 'RAN'),\n",
       "   ('GOING', 'WENT', 'SEEING', 'SAW'),\n",
       "   ('GOING', 'WENT', 'SITTING', 'SAT'),\n",
       "   ('GOING', 'WENT', 'SLEEPING', 'SLEPT'),\n",
       "   ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
       "   ('GOING', 'WENT', 'THINKING', 'THOUGHT'),\n",
       "   ('GOING', 'WENT', 'WALKING', 'WALKED'),\n",
       "   ('KNOWING', 'KNEW', 'LOOKING', 'LOOKED'),\n",
       "   ('KNOWING', 'KNEW', 'PLAYING', 'PLAYED'),\n",
       "   ('KNOWING', 'KNEW', 'READING', 'READ'),\n",
       "   ('KNOWING', 'KNEW', 'RUNNING', 'RAN'),\n",
       "   ('KNOWING', 'KNEW', 'SEEING', 'SAW'),\n",
       "   ('KNOWING', 'KNEW', 'SITTING', 'SAT'),\n",
       "   ('KNOWING', 'KNEW', 'SLEEPING', 'SLEPT'),\n",
       "   ('KNOWING', 'KNEW', 'TAKING', 'TOOK'),\n",
       "   ('KNOWING', 'KNEW', 'THINKING', 'THOUGHT'),\n",
       "   ('KNOWING', 'KNEW', 'WALKING', 'WALKED'),\n",
       "   ('KNOWING', 'KNEW', 'GOING', 'WENT'),\n",
       "   ('LOOKING', 'LOOKED', 'PLAYING', 'PLAYED'),\n",
       "   ('LOOKING', 'LOOKED', 'READING', 'READ'),\n",
       "   ('LOOKING', 'LOOKED', 'RUNNING', 'RAN'),\n",
       "   ('LOOKING', 'LOOKED', 'SEEING', 'SAW'),\n",
       "   ('LOOKING', 'LOOKED', 'SITTING', 'SAT'),\n",
       "   ('LOOKING', 'LOOKED', 'SLEEPING', 'SLEPT'),\n",
       "   ('LOOKING', 'LOOKED', 'TAKING', 'TOOK'),\n",
       "   ('LOOKING', 'LOOKED', 'THINKING', 'THOUGHT'),\n",
       "   ('LOOKING', 'LOOKED', 'WALKING', 'WALKED'),\n",
       "   ('LOOKING', 'LOOKED', 'GOING', 'WENT'),\n",
       "   ('LOOKING', 'LOOKED', 'KNOWING', 'KNEW'),\n",
       "   ('PLAYING', 'PLAYED', 'READING', 'READ'),\n",
       "   ('PLAYING', 'PLAYED', 'RUNNING', 'RAN'),\n",
       "   ('PLAYING', 'PLAYED', 'SEEING', 'SAW'),\n",
       "   ('PLAYING', 'PLAYED', 'SITTING', 'SAT'),\n",
       "   ('PLAYING', 'PLAYED', 'SLEEPING', 'SLEPT'),\n",
       "   ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
       "   ('PLAYING', 'PLAYED', 'THINKING', 'THOUGHT'),\n",
       "   ('PLAYING', 'PLAYED', 'WALKING', 'WALKED'),\n",
       "   ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
       "   ('PLAYING', 'PLAYED', 'KNOWING', 'KNEW'),\n",
       "   ('PLAYING', 'PLAYED', 'LOOKING', 'LOOKED'),\n",
       "   ('READING', 'READ', 'RUNNING', 'RAN'),\n",
       "   ('READING', 'READ', 'SEEING', 'SAW'),\n",
       "   ('READING', 'READ', 'SITTING', 'SAT'),\n",
       "   ('READING', 'READ', 'SLEEPING', 'SLEPT'),\n",
       "   ('READING', 'READ', 'TAKING', 'TOOK'),\n",
       "   ('READING', 'READ', 'THINKING', 'THOUGHT'),\n",
       "   ('READING', 'READ', 'WALKING', 'WALKED'),\n",
       "   ('READING', 'READ', 'GOING', 'WENT'),\n",
       "   ('READING', 'READ', 'KNOWING', 'KNEW'),\n",
       "   ('READING', 'READ', 'LOOKING', 'LOOKED'),\n",
       "   ('READING', 'READ', 'PLAYING', 'PLAYED'),\n",
       "   ('RUNNING', 'RAN', 'SEEING', 'SAW'),\n",
       "   ('RUNNING', 'RAN', 'SITTING', 'SAT'),\n",
       "   ('RUNNING', 'RAN', 'SLEEPING', 'SLEPT'),\n",
       "   ('RUNNING', 'RAN', 'TAKING', 'TOOK'),\n",
       "   ('RUNNING', 'RAN', 'THINKING', 'THOUGHT'),\n",
       "   ('RUNNING', 'RAN', 'WALKING', 'WALKED'),\n",
       "   ('RUNNING', 'RAN', 'GOING', 'WENT'),\n",
       "   ('RUNNING', 'RAN', 'KNOWING', 'KNEW'),\n",
       "   ('RUNNING', 'RAN', 'LOOKING', 'LOOKED'),\n",
       "   ('RUNNING', 'RAN', 'PLAYING', 'PLAYED'),\n",
       "   ('RUNNING', 'RAN', 'READING', 'READ'),\n",
       "   ('SEEING', 'SAW', 'SITTING', 'SAT'),\n",
       "   ('SEEING', 'SAW', 'SLEEPING', 'SLEPT'),\n",
       "   ('SEEING', 'SAW', 'TAKING', 'TOOK'),\n",
       "   ('SEEING', 'SAW', 'THINKING', 'THOUGHT'),\n",
       "   ('SEEING', 'SAW', 'WALKING', 'WALKED'),\n",
       "   ('SEEING', 'SAW', 'GOING', 'WENT'),\n",
       "   ('SEEING', 'SAW', 'KNOWING', 'KNEW'),\n",
       "   ('SEEING', 'SAW', 'LOOKING', 'LOOKED'),\n",
       "   ('SEEING', 'SAW', 'PLAYING', 'PLAYED'),\n",
       "   ('SEEING', 'SAW', 'READING', 'READ'),\n",
       "   ('SEEING', 'SAW', 'RUNNING', 'RAN'),\n",
       "   ('SITTING', 'SAT', 'SLEEPING', 'SLEPT'),\n",
       "   ('SITTING', 'SAT', 'TAKING', 'TOOK'),\n",
       "   ('SITTING', 'SAT', 'THINKING', 'THOUGHT'),\n",
       "   ('SITTING', 'SAT', 'WALKING', 'WALKED'),\n",
       "   ('SITTING', 'SAT', 'GOING', 'WENT'),\n",
       "   ('SITTING', 'SAT', 'KNOWING', 'KNEW'),\n",
       "   ('SITTING', 'SAT', 'LOOKING', 'LOOKED'),\n",
       "   ('SITTING', 'SAT', 'PLAYING', 'PLAYED'),\n",
       "   ('SITTING', 'SAT', 'READING', 'READ'),\n",
       "   ('SITTING', 'SAT', 'RUNNING', 'RAN'),\n",
       "   ('SITTING', 'SAT', 'SEEING', 'SAW'),\n",
       "   ('SLEEPING', 'SLEPT', 'TAKING', 'TOOK'),\n",
       "   ('SLEEPING', 'SLEPT', 'THINKING', 'THOUGHT'),\n",
       "   ('SLEEPING', 'SLEPT', 'WALKING', 'WALKED'),\n",
       "   ('SLEEPING', 'SLEPT', 'GOING', 'WENT'),\n",
       "   ('SLEEPING', 'SLEPT', 'KNOWING', 'KNEW'),\n",
       "   ('SLEEPING', 'SLEPT', 'LOOKING', 'LOOKED'),\n",
       "   ('SLEEPING', 'SLEPT', 'PLAYING', 'PLAYED'),\n",
       "   ('SLEEPING', 'SLEPT', 'READING', 'READ'),\n",
       "   ('SLEEPING', 'SLEPT', 'RUNNING', 'RAN'),\n",
       "   ('SLEEPING', 'SLEPT', 'SEEING', 'SAW'),\n",
       "   ('SLEEPING', 'SLEPT', 'SITTING', 'SAT'),\n",
       "   ('TAKING', 'TOOK', 'THINKING', 'THOUGHT'),\n",
       "   ('TAKING', 'TOOK', 'WALKING', 'WALKED'),\n",
       "   ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
       "   ('TAKING', 'TOOK', 'KNOWING', 'KNEW'),\n",
       "   ('TAKING', 'TOOK', 'LOOKING', 'LOOKED'),\n",
       "   ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
       "   ('TAKING', 'TOOK', 'READING', 'READ'),\n",
       "   ('TAKING', 'TOOK', 'RUNNING', 'RAN'),\n",
       "   ('TAKING', 'TOOK', 'SEEING', 'SAW'),\n",
       "   ('TAKING', 'TOOK', 'SITTING', 'SAT'),\n",
       "   ('TAKING', 'TOOK', 'SLEEPING', 'SLEPT'),\n",
       "   ('THINKING', 'THOUGHT', 'WALKING', 'WALKED'),\n",
       "   ('THINKING', 'THOUGHT', 'GOING', 'WENT'),\n",
       "   ('THINKING', 'THOUGHT', 'KNOWING', 'KNEW'),\n",
       "   ('THINKING', 'THOUGHT', 'LOOKING', 'LOOKED'),\n",
       "   ('THINKING', 'THOUGHT', 'PLAYING', 'PLAYED'),\n",
       "   ('THINKING', 'THOUGHT', 'READING', 'READ'),\n",
       "   ('THINKING', 'THOUGHT', 'RUNNING', 'RAN'),\n",
       "   ('THINKING', 'THOUGHT', 'SEEING', 'SAW'),\n",
       "   ('THINKING', 'THOUGHT', 'SITTING', 'SAT'),\n",
       "   ('THINKING', 'THOUGHT', 'SLEEPING', 'SLEPT'),\n",
       "   ('THINKING', 'THOUGHT', 'TAKING', 'TOOK'),\n",
       "   ('WALKING', 'WALKED', 'GOING', 'WENT'),\n",
       "   ('WALKING', 'WALKED', 'KNOWING', 'KNEW'),\n",
       "   ('WALKING', 'WALKED', 'LOOKING', 'LOOKED'),\n",
       "   ('WALKING', 'WALKED', 'PLAYING', 'PLAYED'),\n",
       "   ('WALKING', 'WALKED', 'READING', 'READ'),\n",
       "   ('WALKING', 'WALKED', 'RUNNING', 'RAN'),\n",
       "   ('WALKING', 'WALKED', 'SEEING', 'SAW'),\n",
       "   ('WALKING', 'WALKED', 'SITTING', 'SAT'),\n",
       "   ('WALKING', 'WALKED', 'SLEEPING', 'SLEPT'),\n",
       "   ('WALKING', 'WALKED', 'TAKING', 'TOOK'),\n",
       "   ('WALKING', 'WALKED', 'THINKING', 'THOUGHT'),\n",
       "   ('DOG', 'DOGS', 'EYE', 'EYES'),\n",
       "   ('DOG', 'DOGS', 'HAND', 'HANDS'),\n",
       "   ('DOG', 'DOGS', 'MAN', 'MEN'),\n",
       "   ('EYE', 'EYES', 'HAND', 'HANDS'),\n",
       "   ('EYE', 'EYES', 'MAN', 'MEN'),\n",
       "   ('EYE', 'EYES', 'DOG', 'DOGS'),\n",
       "   ('HAND', 'HANDS', 'DOG', 'DOGS'),\n",
       "   ('HAND', 'HANDS', 'EYE', 'EYES'),\n",
       "   ('MAN', 'MEN', 'DOG', 'DOGS'),\n",
       "   ('MAN', 'MEN', 'EYE', 'EYES'),\n",
       "   ('MAN', 'MEN', 'HAND', 'HANDS'),\n",
       "   ('GO', 'GOES', 'SAY', 'SAYS'),\n",
       "   ('GO', 'GOES', 'SEE', 'SEES'),\n",
       "   ('GO', 'GOES', 'THINK', 'THINKS'),\n",
       "   ('SAY', 'SAYS', 'SEE', 'SEES'),\n",
       "   ('SAY', 'SAYS', 'THINK', 'THINKS'),\n",
       "   ('SAY', 'SAYS', 'GO', 'GOES'),\n",
       "   ('SEE', 'SEES', 'THINK', 'THINKS'),\n",
       "   ('SEE', 'SEES', 'GO', 'GOES'),\n",
       "   ('SEE', 'SEES', 'SAY', 'SAYS'),\n",
       "   ('THINK', 'THINKS', 'GO', 'GOES'),\n",
       "   ('THINK', 'THINKS', 'SAY', 'SAYS'),\n",
       "   ('THINK', 'THINKS', 'SEE', 'SEES')]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.accuracy(directory_w2v + 'questions-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `accuracy` takes an \n",
    "[optional parameter](http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.accuracy) `restrict_vocab` \n",
    "which limits which test examples are to be considered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the December 2016 release of Gensim we added a better way to evaluate semantic similarity.\n",
    "\n",
    "By default it uses an academic dataset WS-353 but one can create a dataset specific to your business based on it. It contains word pairs together with human-assigned similarity judgments. It measures the relatedness or co-occurrence of two words. For example, 'coast' and 'shore' are very similar as they appear in the same context. At the same time 'clothes' and 'closet' are less similar because they are related but not interchangeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file names for train and test data\n",
    "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.351962279120603, 0.3185780936637547),\n",
       " SpearmanrResult(correlation=0.41818181818181815, pvalue=0.22911284098281892),\n",
       " 97.16713881019831)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.evaluate_word_pairs(test_data_dir + 'wordsim353.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, **good performance on Google's or WS-353 test set doesn’t mean word2vec will work well in your application, or vice versa**. It’s always best to evaluate directly on your intended task. For an example of how to use word2vec in a classifier pipeline, see this [tutorial](https://github.com/RaRe-Technologies/movie-plots-by-genre)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing and loading models\n",
    "You can store/load models using the standard gensim methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file://C://Users//IBM_ADMIN//Documents//Teaching//Data Projects//Text//w2v//book_model'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_w2v_model = directory_w2v + \"book_model\"\n",
    "directory_w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(directory_w2v_model)  # save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = gensim.models.Word2Vec.load(directory_w2v_model)  # open the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which uses pickle internally, optionally `mmap`‘ing the model’s internal large NumPy matrices into virtual memory directly from disk files, for inter-process memory sharing.\n",
    "\n",
    "In addition, you can load models created by the original C tool, both using its text and binary formats:\n",
    "```\n",
    "  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)\n",
    "  # using gzipped/bz2 input works too, no need to unzip:\n",
    "  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.bin.gz', binary=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online training / Resuming training\n",
    "Advanced users can load a model and continue training it with more sentences and [new vocabulary words](online_w2v_tutorial.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 65)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(directory_w2v_model)\n",
    "more_sentences = [['Advanced', 'users', 'can', 'load', 'a', 'model', 'and', 'continue', 'training', 'it', 'with', 'more', 'sentences']]\n",
    "model.build_vocab(more_sentences, update=True)\n",
    "model.train(more_sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# cleaning up temp\n",
    "# os.close(fs)\n",
    "# os.remove(temp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to tweak the `total_words` parameter to `train()`, depending on what learning rate decay you want to simulate.\n",
    "\n",
    "Note that it’s not possible to resume training with models generated by the C tool, `KeyedVectors.load_word2vec_format()`. You can still use them for querying/similarity, but information vital for training (the vocab tree) is missing there.\n",
    "\n",
    "## Using the model\n",
    "`Word2Vec` supports several word similarity tasks out of the box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "if 'tea' in model:\n",
    "    print(model['tea'].shape)\n",
    "else:\n",
    "    print('{0} is an out of dictionary word'.format('tea'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and,', 0.9991182088851929)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['water', 'river'], negative=['tea'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99978054\n",
      "0.99985576\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('head', 'boat'))\n",
    "print(model.wv.similarity('river', 'water'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the probability distribution for the center word given the context words as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('can', 0.0017630099), ('more', 0.0016664715), ('it,', 0.0013456458), ('been', 0.0012891365), ('people', 0.0012842724), ('come', 0.0012729266), ('got', 0.0012689896), ('never', 0.0012579266), ('them', 0.0012566646), ('it.', 0.0012539034)]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict_output_word(['boat', 'river', 'water']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results here don't look good because the training corpus is very small. To get meaningful results one needs to train on 500k+ words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need the raw output vectors in your application, you can access these either on a word-by-word basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3942015 ,  0.23584281, -0.04920526,  0.11957395, -0.35226712,\n",
       "       -0.19367567, -0.27025673,  0.2104711 , -0.15261875, -0.08015857,\n",
       "       -0.17626545,  0.4151441 , -0.5112834 ,  0.2389548 , -0.4497126 ,\n",
       "        0.5928972 , -0.09446308,  0.41125774, -0.03768824,  0.45991328,\n",
       "        0.11137432,  0.0313634 , -0.14867884,  0.49344093,  0.45399904,\n",
       "        0.49134538,  0.5356065 ,  0.27955425, -0.20928909,  0.4018742 ,\n",
       "       -0.5052954 ,  0.47832105, -0.12731785,  0.04730815, -0.44345796,\n",
       "       -0.26815686,  0.45327646, -0.5460642 ,  0.5275853 ,  0.45769358,\n",
       "        0.09576337,  0.04278165, -0.21387278, -0.01210997, -0.17602871,\n",
       "        0.5329506 ,  0.17196184, -0.0592141 , -0.25488833, -0.51073647,\n",
       "       -0.14272785,  0.3889757 , -0.31409946, -0.4075012 ,  0.02974972,\n",
       "       -0.61988455,  0.6481276 ,  0.21580404,  0.31985068, -0.15068793,\n",
       "       -0.45306465,  0.11432141, -0.01921608,  0.12702253, -0.14964059,\n",
       "        0.02991934, -0.17144321, -0.17028883,  0.16534679, -0.5934085 ,\n",
       "        0.16073039, -0.31723252,  0.14500447, -0.04206452,  0.6558518 ,\n",
       "        0.43456042, -0.21505296, -0.24515131,  0.09329045,  0.0284783 ,\n",
       "       -0.09244047,  0.08316263, -0.27561632,  0.11690931, -0.25788802,\n",
       "       -0.11615331, -0.23264618,  0.158356  , -0.13812798, -0.66030693,\n",
       "        0.13338886,  0.1380659 ,  0.13063285,  0.13563745, -0.2850365 ,\n",
       "        0.17016771,  0.4405369 ,  0.650774  , -0.14063463,  0.13403545],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.__getitem__('boat')  # raw NumPy vector of a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "…or en-masse as a 2D NumPy matrix from `model.wv.syn0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loss Computation\n",
    "\n",
    "The parameter `compute_loss` can be used to toggle computation of loss while training the Word2Vec model. The computed loss is stored in the model attribute `running_training_loss` and can be retrieved using the function `get_latest_training_loss` as follows : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3566317.5078075198\n"
     ]
    }
   ],
   "source": [
    "# instantiating and training the Word2Vec model\n",
    "model_with_loss = gensim.models.Word2Vec(sentences, min_count=1, compute_loss=True, hs=0, sg=1, seed=42)\n",
    "\n",
    "# getting the training loss value\n",
    "training_loss = model_with_loss.get_latest_training_loss()\n",
    "print(training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial we learned how to train word2vec models on your custom data and also how to evaluate it. Hope that you too will find this popular tool useful in your Machine Learning tasks!\n",
    "\n",
    "## Links\n",
    "\n",
    "\n",
    "Full `word2vec` API docs [here](http://radimrehurek.com/gensim/models/word2vec.html); get [gensim](http://radimrehurek.com/gensim/) here. Original C toolkit and `word2vec` papers by Google [here](https://code.google.com/archive/p/word2vec/)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
