{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('popular', halt_on_error=False)\n",
    "#nltk.download('all', halt_on_error=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nltk\n",
    "import nltk.corpus  \n",
    "from nltk.text import Text\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.expanduser('~/Datasets/32018/')\n",
    "df = pd.read_pickle(os.path.join(directory, 'news_cat.pkl'))\n",
    "df = df[df.language == 'english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crawled</th>\n",
       "      <th>language</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-30T23:03:51.004+02:00</td>\n",
       "      <td>english</td>\n",
       "      <td>by Abhishek K Global Telehandler Market 2023 D...</td>\n",
       "      <td>Global Telehandler Market 2023 Demand by Segme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-30T23:06:46.024+02:00</td>\n",
       "      <td>english</td>\n",
       "      <td>favorite this post 2014 Caterpillar 314E LCR h...</td>\n",
       "      <td>2014 Caterpillar 314E LCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-30T23:18:35.023+02:00</td>\n",
       "      <td>english</td>\n",
       "      <td>By: MAX NISEN The Amazon health care threat ha...</td>\n",
       "      <td>Amazon, Berkshire, JPMorgan health announcemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-30T23:20:54.012+02:00</td>\n",
       "      <td>english</td>\n",
       "      <td>QR Code Link to This Post MONTHLY PUBLIC AUCTI...</td>\n",
       "      <td>2005 Caterpillar CB534D Tandem Vibratory Rolle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-30T23:28:30.000+02:00</td>\n",
       "      <td>english</td>\n",
       "      <td>QR Code Link to This Post 2007 CATERPILLAR D4G...</td>\n",
       "      <td>2007 CATERPILLAR D4G LGP CAB SCREEN/SWEEPS - O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         crawled language  \\\n",
       "0  2018-01-30T23:03:51.004+02:00  english   \n",
       "1  2018-01-30T23:06:46.024+02:00  english   \n",
       "2  2018-01-30T23:18:35.023+02:00  english   \n",
       "3  2018-01-30T23:20:54.012+02:00  english   \n",
       "4  2018-01-30T23:28:30.000+02:00  english   \n",
       "\n",
       "                                                text  \\\n",
       "0  by Abhishek K Global Telehandler Market 2023 D...   \n",
       "1  favorite this post 2014 Caterpillar 314E LCR h...   \n",
       "2  By: MAX NISEN The Amazon health care threat ha...   \n",
       "3  QR Code Link to This Post MONTHLY PUBLIC AUCTI...   \n",
       "4  QR Code Link to This Post 2007 CATERPILLAR D4G...   \n",
       "\n",
       "                                               title  \n",
       "0  Global Telehandler Market 2023 Demand by Segme...  \n",
       "1                          2014 Caterpillar 314E LCR  \n",
       "2  Amazon, Berkshire, JPMorgan health announcemen...  \n",
       "3  2005 Caterpillar CB534D Tandem Vibratory Rolle...  \n",
       "4  2007 CATERPILLAR D4G LGP CAB SCREEN/SWEEPS - O...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rowena/miniconda3/envs/msca/lib/python3.6/site-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Change the path according to your system\n",
    "stanford_classifier = '/Users/rowena/code/StanfordNLP/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "stanford_ner_path = '/Users/rowena/code/StanfordNLP/stanford-ner-2015-12-09/stanford-ner.jar'\n",
    "\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "\n",
    "# Creating Tagger Object\n",
    "st = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRFClassifier invoked on Thu May 09 20:50:30 CDT 2019 with arguments:\n",
      "   -loadClassifier /Users/rowena/code/StanfordNLP/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz -textFile /var/folders/dj/cn4rthcn4s57tdf9j9z4wqgm0000gn/T/tmp_vzdhf2h -outputFormat slashTags -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerOptions \"tokenizeNLs=false\" -encoding utf-8\n",
      "tokenizerOptions=\"tokenizeNLs=false\"\n",
      "loadClassifier=/Users/rowena/code/StanfordNLP/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz\n",
      "encoding=utf-8\n",
      "outputFormat=slashTags\n",
      "textFile=/var/folders/dj/cn4rthcn4s57tdf9j9z4wqgm0000gn/T/tmp_vzdhf2h\n",
      "tokenizerFactory=edu.stanford.nlp.process.WhitespaceTokenizer\n",
      "Exception in thread \"main\" java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory\n",
      "\tat edu.stanford.nlp.io.IOUtils.<clinit>(IOUtils.java:42)\n",
      "\tat edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1484)\n",
      "\tat edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1497)\n",
      "\tat edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3015)\n",
      "Caused by: java.lang.ClassNotFoundException: org.slf4j.LoggerFactory\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:582)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:190)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:499)\n",
      "\t... 4 more\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Java command failed : ['/usr/bin/java', '-mx1000m', '-cp', '/Users/rowena/code/StanfordNLP/stanford-ner-2015-12-09/stanford-ner.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', '/Users/rowena/code/StanfordNLP/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz', '-textFile', '/var/folders/dj/cn4rthcn4s57tdf9j9z4wqgm0000gn/T/tmp_vzdhf2h', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '\"tokenizeNLs=false\"', '-encoding', 'utf-8']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2d94bd3f6fe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mclassified_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mnetagged_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassified_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassified_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/msca/lib/python3.6/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# This function should return list of tuple rather than list of list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/msca/lib/python3.6/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36mtag_sents\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# Run the tagger and get the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         stanpos_output, _stderr = java(cmd, classpath=self._stanford_jar,\n\u001b[0;32m--> 107\u001b[0;31m                                        stdout=PIPE, stderr=PIPE)\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mstanpos_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanpos_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/msca/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mjava\u001b[0;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decode_stdoutdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Java command failed : '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Java command failed : ['/usr/bin/java', '-mx1000m', '-cp', '/Users/rowena/code/StanfordNLP/stanford-ner-2015-12-09/stanford-ner.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', '/Users/rowena/code/StanfordNLP/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz', '-textFile', '/var/folders/dj/cn4rthcn4s57tdf9j9z4wqgm0000gn/T/tmp_vzdhf2h', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '\"tokenizeNLs=false\"', '-encoding', 'utf-8']"
     ]
    }
   ],
   "source": [
    "text = df.text[0]\n",
    "\n",
    "\n",
    "\n",
    "entities = []\n",
    "labels = []\n",
    "\n",
    "from itertools import groupby\n",
    "for sent in nltk.sent_tokenize(text):\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    classified_text = st.tag(tokenized_text)\n",
    "    netagged_words = classified_text\n",
    "    for tag, chunk in groupby(classified_text, lambda x:x[1]):\n",
    "        if tag != \"O\":\n",
    "            #print(\"%-12s\"%tag, \" \".join(w for w, t in chunk))\n",
    "            entities.append(' '.join(w for w, t in chunk))\n",
    "            labels.append(tag)\n",
    "        \n",
    "        \n",
    "entities_all = list(zip(entities, labels))\n",
    "entities_unique = list(set(zip(entities, labels))) #unique entities   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Forecast</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Manufacturers</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Report</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Purchase Report</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Competition Structure</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Products</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Development Prospect</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Segment Demand Situation</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Opportunities</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>China</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Purchasing Features</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>North America</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CAGR</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Production</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Chain</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Relevant Policy</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Type2 Market</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Supply</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Telehandler Price</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Services</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Entities        Labels\n",
       "0                   Forecast           GPE\n",
       "1              Manufacturers  ORGANIZATION\n",
       "2                     Report           GPE\n",
       "3            Purchase Report        PERSON\n",
       "4      Competition Structure  ORGANIZATION\n",
       "5                   Products           GPE\n",
       "6       Development Prospect  ORGANIZATION\n",
       "7   Segment Demand Situation  ORGANIZATION\n",
       "8              Opportunities           GPE\n",
       "9                      China           GPE\n",
       "10       Purchasing Features  ORGANIZATION\n",
       "11             North America        PERSON\n",
       "12                      CAGR  ORGANIZATION\n",
       "13                Production  ORGANIZATION\n",
       "14                     Chain        PERSON\n",
       "15           Relevant Policy  ORGANIZATION\n",
       "16              Type2 Market        PERSON\n",
       "17                    Supply        PERSON\n",
       "18         Telehandler Price        PERSON\n",
       "19                  Services           GPE"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_df = pd.DataFrame(entities_labels)\n",
    "entities_df.columns = [\"Entities\", \"Labels\"]\n",
    "entities_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
