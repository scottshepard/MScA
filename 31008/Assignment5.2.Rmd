---
title: "Assignment5.2"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this assignment I use Linear Discriminant Analysis and Quadratic 
Discriminant Analysis to predict creditability in the German Credit dataset.
These algorithms are like PCA but they use the response variable to evaluate
the clustering instead of explained variance amoung the predictors.

Then later, I combine all the models built in the past few months: LDA & QDA,
Classification Trees, and Logistic Regression to create an ensemble model.

# LDA & QDA

## 1. Data & Setup

```{r}
suppressMessages(library(dplyr))
suppressMessages(library(ggplot2))
suppressMessages(library(caret))
suppressMessages(library(MASS))
library(rpart)

splitData <- function(data, smp_size) {
  # Take a dataset and split it randomly into train and test sets
  # The size of the training set is the smp_size argument
  train_ind <- sample(seq_len(nrow(data)), size = smp_size)
  train <- data[ train_ind, ]
  test  <- data[-train_ind, ]
  
  list("train"=train, "test"=test)
}

path <- "~/Dropbox/MScA/31008 - Data Mining/"
GC <- read.csv(file.path(path, "GermanCredit.csv"))

set.seed(123)
splitted <- splitData(GC, 700)
Train <- splitted$train
Test  <- splitted$test
```

## 2. Model Building

```{r}
# bad_cols <- c(14,19,27,30,35,40,44,45,48,52,55,58,62)
lda.out <- lda(Creditability ~ . , Train)
qda.out <- qda(Creditability ~ . , Train)

lda.pv <- predict(lda.out)
qda.pv <- predict(qda.out)

tbl <- prop.table(table(Train$Creditability, lda.pv$class))
tbl

accuracy <- round(100*(tbl[1,1] + tbl[2,2]))
(accuracy)
default_rate <- round(100*tbl[1,2]/sum(tbl[,2]))
(default_rate)

tbl <- prop.table(table(Train$Creditability, qda.pv$class))
tbl

accuracy <- round(100*(tbl[1,1] + tbl[2,2]))
(accuracy)
default_rate <- round(100*tbl[1,2]/sum(tbl[,2]))
(default_rate)
```

I use my favorite measures of accuracy and default rate here. Which the models
are highly accurate (78% & 81% respectively) they have a higher default rate
than I like. That is, they predict good credit when the real result is bad
credit. This is a much worse mistake than predicting bad when the truth is good.

## 3. Holdout Validation

```{r}
lda.pv.hd <- predict(lda.out, newdata = Test)
qda.pv.hd <- predict(qda.out, newdata = Test)

tbl <- prop.table(table(Test$Creditability, lda.pv.hd$class))
tbl

accuracy <- round(100*(tbl[1,1] + tbl[2,2]))
(accuracy)
default_rate <- round(100*tbl[1,2]/sum(tbl[,2]))
(default_rate)

tbl <- prop.table(table(Test$Creditability, qda.pv.hd$class))
tbl

accuracy <- round(100*(tbl[1,1] + tbl[2,2]))
(accuracy)
default_rate <- round(100*tbl[1,2]/sum(tbl[,2]))
(default_rate)
```

Holdout validationi s pretty good. The proportions in the prop.table are 
pretty similar, but it's not great. The default rate is even higher than in
train. A default rate of 20%+ when train indicated only 13% like in the QDA
model would be devasting to a bank.

# Ensemble Model

An ensemble model is not a new model. It takes the results of previous models
and combines them. In this case we take four models: LDA, QDA, CLassification 
Trees, and Logistic Regression. Majority rules, so if three models predict 
good credit and one predicts bad we assign good to that row. Ties are 
broken randomly.

```{r}
Train <- splitted$train
Test  <- splitted$test

Tree.Model = rpart(factor(Creditability) ~ ., data=Train,
                   control=rpart.control(cp=0.0150754, minsplit=30, xval=10))

Logit.Model = glm(formula = Creditability ~ Account.Balance + 
                   Duration.of.Credit..month. + 
                   Payment.Status.of.Previous.Credit + Purpose + Credit.Amount +
                   Value.Savings.Stocks + Instalment.per.cent + 
                   Sex...Marital.Status + Guarantors + Type.of.apartment +
                   No.of.dependents + Telephone + Foreign.Worker, 
                 family = binomial(link = logit), data = Train)
```

```{r}
Train$Class.Tree  <- predict(Tree.Model, type="class")
Train$Class.Logit <- factor(as.numeric(predict(Logit.Model, type="response") >= 0.8))
Train$Class.LDA <- predict(lda.out)$class
Train$Class.QDA <- predict(qda.out)$class

ensemble <- function(df) {
  vec <- with(df, as.numeric(Class.Tree)-1 + as.numeric(Class.Logit)-1 +
                as.numeric(Class.LDA)-1 + as.numeric(Class.QDA)-1)
  sapply(vec, function(x) {
    if(x > 2) {
      1
    } else if(x < 2) {
      0
    } else {
      sample(c(0,1), 1)
    }
  })
}

Train$Ensemble <- ensemble(Train)

Test$Class.Tree  <- predict(Tree.Model, type="class", newdata = Test)
Test$Class.Logit <- factor(as.numeric(predict(Logit.Model, type="response", newdata = Test) >= 0.8))
Test$Class.LDA <- predict(lda.out, newdata = Test)$class
Test$Class.QDA <- predict(qda.out, newdata = Test)$class
Test$Ensemble <- ensemble(Test)
```

```{r}
tbl <- prop.table(table(Train$Creditability,Train$Ensemble))
tbl

accuracy <- round(100*(tbl[1,1] + tbl[2,2]))
(accuracy)
default_rate <- round(100*tbl[1,2]/sum(tbl[,2]))
(default_rate)

tbl <- prop.table(table(Test$Creditability,Test$Ensemble))
tbl

accuracy <- round(100*(tbl[1,1] + tbl[2,2]))
(accuracy)
default_rate <- round(100*tbl[1,2]/sum(tbl[,2]))
(default_rate)
```

Again I'm using proportion tables as well as accuracy and default rates to 
evaluate the models. While the trained model has a high accuracy, it's no 
higher than the QDA model. It seems like the ensemble model performs as well
as the best model that went into it, but no better. 

The holdout validation is quite disappointing. It lost five points in accuracy
and six points in the default rate. I would not want to use this particular
model to make any business decisions.
