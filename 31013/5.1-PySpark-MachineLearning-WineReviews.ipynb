{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Platforms\n",
    "\n",
    "## PySpark Machine Learning\n",
    "\n",
    "### MLlib applied to Wine reviews data \n",
    "\n",
    "**Dataset:**\n",
    "https://www.kaggle.com/zynicide/wine-reviews\n",
    "\n",
    "\n",
    "Copyright: 2018 [Ashish Pujari](apujari@uchicago.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.id', 'driver'),\n",
       " ('spark.executor.memory', '5g'),\n",
       " ('spark.driver.port', '50400'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.cores.max', '4'),\n",
       " ('spark.app.name', 'Spark Updated Conf'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.memory', '8g'),\n",
       " ('spark.app.id', 'local-1550715547292'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', '10.150.158.6'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create Spark session\n",
    "spark = SparkSession.builder.appName('WineReviewsML').getOrCreate()\n",
    "\n",
    "#change configuration settings on Spark \n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '5g'), ('spark.app.name', 'Spark Updated Conf'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.driver.memory','8g')])\n",
    "\n",
    "#print spark configuration settings\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"quote\", \"\\\"\")  \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"ignoreLeadingWhiteSpace\",True) \\\n",
    "    .csv(\"/Users/rowena/Documents/wine-reviews/winemag-data_first150k.csv\",inferSchema=True, header=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- designation: string (nullable = true)\n",
      " |-- points: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- region_1: string (nullable = true)\n",
      " |-- region_2: string (nullable = true)\n",
      " |-- variety: string (nullable = true)\n",
      " |-- winery: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read \\\n",
    "    .option(\"quote\", \"\\\"\")  \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"ignoreLeadingWhiteSpace\",True) \\\n",
    "    .csv(\"/Users/rowena/Documents/wine-reviews/winemag-data-130k-v2.csv\",inferSchema=True, header=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- designation: string (nullable = true)\n",
      " |-- points: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- region_1: string (nullable = true)\n",
      " |-- region_2: string (nullable = true)\n",
      " |-- taster_name: string (nullable = true)\n",
      " |-- taster_twitter_handle: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- variety: string (nullable = true)\n",
      " |-- winery: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0=0, country='Italy', description=\"Aromas include tropical fruit, broom, brimstone and dried herb. The palate isn't overly expressive, offering unripened apple, citrus and dried sage alongside brisk acidity.\", designation='Vulkà Bianco', points=87, price=None, province='Sicily & Sardinia', region_1='Etna', region_2=None, taster_name='Kerin O’Keefe', taster_twitter_handle='@kerinokeefe', title='Nicosia 2013 Vulkà Bianco  (Etna)', variety='White Blend', winery='Nicosia')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the two datasets\n",
    "df = df.union(df2.drop(\"taster_name\", \"taster_twitter_handle\", \"title\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary statistics\n",
    "df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count rows with missing values\n",
    "df.dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the number of missing values for each column\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df.select([count(when(df[c].isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#drop rows where country is missing\n",
    "df = df.filter(df[\"country\"].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean price\n",
    "meanprice = df.agg({\"price\": \"mean\"}).collect()[0][0]\n",
    "meanprice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute the value of price where it is missing to the mean price\n",
    "df = df.na.fill(meanprice, \"price\") \\\n",
    "    .na.fill(\"Unknown\", \"region_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of countries\n",
    "df.select('Country').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wines by country\n",
    "df.groupby('Country').count().orderBy([\"count\"], ascending=[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of varieties\n",
    "df.select('Variety').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wines by variety\n",
    "df.groupby(\"variety\").count().orderBy([\"count\"], ascending=[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show a count of the number of wines grouped and sorted by Country and Province\n",
    "df.groupBy('Country', 'province').count().orderBy([\"count\"], ascending=[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show a count of the number of wines grouped and sorted by Country and Province\n",
    "df.cube(\"Country\", \"province\", \"region_1\").count().dropna().orderBy([\"count\"], ascending=[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the min,max and avg price in each country\n",
    "df.groupBy('Country').agg(F.min('price'),F.max('price'),F.avg('price')).orderBy([\"max(price)\"], ascending=[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Highest rated wine - using API\n",
    "from pyspark.sql.functions import max\n",
    "df.agg(max(df.points)).head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average points ranked by country\n",
    "df2 = df.groupBy('Country').agg(F.min('points'),F.max('points'),F.avg('points')).orderBy([\"avg(points)\"], ascending=[0])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#convert Pyspark dataframe to Pandas dataframe for plotting\n",
    "pdf = df2.toPandas()\n",
    "pdf.plot(kind= 'bar', x='Country', y='avg(points)', figsize=(18,4), rot=30, ylim=(80, 95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "#are points and price correlated ?\n",
    "df.stat.corr(\"points\", \"price\")\n",
    "\n",
    "#weak uphill relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price = df.agg({\"price\": \"min\"}).collect()[0][0]\n",
    "max_price = df.agg({\"price\": \"max\"}).collect()[0][0]\n",
    "mean_price = df.agg({\"price\": \"mean\"}).collect()[0][0]\n",
    "\n",
    "print (\"Minimum Price : \", min_price, \", Maximum Price : \", max_price, \", Mean Price : \", mean_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the price\n",
    "df = df.withColumn('price_norm', (df[\"price\"] - min_price)/ ( max_price  - min_price))\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "#High Medium Low\n",
    "discretizer = QuantileDiscretizer(numBuckets=3, inputCol=\"price\", outputCol=\"price_category\")\n",
    "df = discretizer.fit(df).transform(df)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#High Medium Low\n",
    "discretizer = QuantileDiscretizer(numBuckets=3, inputCol=\"points\", outputCol=\"points_category\")\n",
    "df = discretizer.fit(df).transform(df)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\n",
    "\n",
    "#convert relevant categorical into one hot encoded\n",
    "indexer1 = StringIndexer(inputCol=\"country\", outputCol=\"countryIdx\").setHandleInvalid(\"skip\")\n",
    "indexer2 = StringIndexer(inputCol=\"province\", outputCol=\"provinceIdx\").setHandleInvalid(\"skip\")\n",
    "indexer3 = StringIndexer(inputCol=\"variety\", outputCol=\"varietyIdx\").setHandleInvalid(\"skip\")\n",
    "indexer4 = StringIndexer(inputCol=\"winery\", outputCol=\"wineryIdx\").setHandleInvalid(\"skip\")\n",
    "\n",
    "#gather all indexers as inputs to the One Hot Encoder\n",
    "inputs = [indexer1.getOutputCol(), indexer2.getOutputCol(), \\\n",
    "          indexer3.getOutputCol(), indexer4.getOutputCol()]\n",
    "\n",
    "#create the one hot encoder\n",
    "encoder = OneHotEncoderEstimator(inputCols=inputs,  \\\n",
    "                                 outputCols=[\"countryVec\", \"provinceVec\", \\\n",
    "                                             \"varietyVec\", \"wineryVec\"])\n",
    "\n",
    "#run it through a pipeline\n",
    "pipeline = Pipeline(stages=[indexer1, indexer2, indexer3, indexer4, encoder])\n",
    "pipeline = pipeline.fit(df).transform(df)\n",
    "#we have removed NAs so dont need to impute missing values.\n",
    "#pipeline = pipeline.na.fill(0) \n",
    "\n",
    "pipeline.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather feature vector and identify features\n",
    "assembler = VectorAssembler(inputCols = ['countryVec', 'provinceVec', \\\n",
    "                                         'varietyVec', 'wineryVec', 'points'], \\\n",
    "                            outputCol = 'features')\n",
    "pipeline = assembler.transform(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test\n",
    "train_df, test_df = pipeline.randomSplit([.8,.2],seed=1234)\n",
    "train_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Regression\n",
    "\n",
    "Let us try to predict the price given features such as country, variety, region, etc.\n",
    "\n",
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "#Elastic Net\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='price', regParam=0.3, elasticNetParam=0.8, maxIter=20)\n",
    "lrm = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coefficients\n",
    "#print(\"Coefficients: \" + str(lrm.coefficients))\n",
    "print(\"Intercept: \" + str(lrm.intercept))\n",
    "\n",
    "#model summary\n",
    "print(\"RMSE: %f\" % lrm.summary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % lrm.summary.r2)\n",
    "\n",
    "#p-values are not provided in this model for the solver being used\n",
    "#print(\"pValues: \" + str(lrm.summary.pValues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions\n",
    "predictions = lrm.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "attrs = sorted(\n",
    "    (attr[\"idx\"], attr[\"name\"]) for attr in (chain(*predictions\n",
    "        .schema[lrm.summary.featuresCol]\n",
    "        .metadata[\"ml_attr\"][\"attrs\"].values())))\n",
    "\n",
    "#[(name, lrm.summary.pValues[idx]) for idx, name in attrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "eval = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Root Mean Square Error\n",
    "rmse = eval.evaluate(predictions)\n",
    "print(\"RMSE: %.3f\" % rmse)\n",
    "\n",
    "# Mean Square Error\n",
    "mse = eval.evaluate(predictions, {eval.metricName: \"mse\"})\n",
    "print(\"MSE: %.3f\" % mse)\n",
    "\n",
    "# Mean Absolute Error\n",
    "mae = eval.evaluate(predictions, {eval.metricName: \"mae\"})\n",
    "print(\"MAE: %.3f\" % mae)\n",
    "\n",
    "# r2 - coefficient of determination\n",
    "r2 = eval.evaluate(predictions, {eval.metricName: \"r2\"})\n",
    "print(\"r2: %.3f\" %r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view predictions against test\n",
    "predictions.select(\"country\", \"region_1\", \"winery\", \"variety\", \"points\", \"price\", \"prediction\"). \\\n",
    "orderBy([\"country\", \"region_1\", \"winery\", \"variety\", \"points\", \"price\", \"prediction\"]).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise</b>: <font color='red'>Tune the model hyperparameters, see if adding additional attributes from the dataset improves the model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Let us try to predict the price_category given features such as country, variety, region, etc.\n",
    "\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Set parameters for Logistic Regression\n",
    "lgr = LogisticRegression(maxIter=10, featuresCol = 'features', labelCol='price_category')\n",
    "\n",
    "# Fit the model to the data.\n",
    "lgrm = lgr.fit(train_df)\n",
    "\n",
    "# Given a dataset, predict each point's label, and show the results.\n",
    "predictions = lgrm.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare predictions against true labels\n",
    "predictions.select(\"country\", \"region_1\", \"winery\", \"variety\", \"points\", \"price_category\", \"prediction\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#print evaluation metrics\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"price_category\", predictionCol=\"prediction\")\n",
    "\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Set parameters for the Random Forest.\n",
    "#rfc = RandomForestClassifier(maxDepth=5, numTrees=15, impurity=\"gini\", labelCol=\"price_category\", predictionCol=\"prediction\")\n",
    "\n",
    "# Fit the model to the data.\n",
    "#rfcm = rfc.fit(train_df)\n",
    "\n",
    "# Given a dataset, predict each point's label, and show the results.\n",
    "#predictions = rfcm.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print evaluation metrics\n",
    "\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise</b>: <font color='red'>Tune the model hyperparameters - increase number of trees to see if the model improves.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise</b>: <font color='red'>Implement a different classifier from Spark ML and compare metrics</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance selector \n",
    "\n",
    "https://www.timlrx.com/2018/06/19/feature-selection-using-feature-importance-score-creating-a-pyspark-estimator/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
