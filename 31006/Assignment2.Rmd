---
title: "31006: Assignment2"
author: "Scott Shepard"
date: "4/11/2019"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Istanbul Timeseries Exploration

Prompt:
1. Determine if all the TS are stationary 
   a. qualitatively
   b. quantitatively
2. Split the data into train and test, keeping only the last 10 rows for test (from date 9-Feb-11). Remember to use only train dataset for #3 to #6.
3. Linearly regress ISE against the remaining 5 stock index returns - determine which coefficients are significant
4. For the non-significant coefficients, continue to lag by 1 day until all coefficients are significant - how many lags are needed?
5. Find correlations between ISE and each independent variable. Sum the square of the correlations. How does it compare to R-squared from #4?
6. Concept question 1 - why do you think the R-squared in #4 is so much less than the sum of square of the correlations?
7. Take the test dataset - perform the same lags from #4 and call predict() function using the lm regression object from #4. Why do you need to use the lm function object fro
8. Concept question - what do you find in #1 and why?

```{r packages}
library(zoo)
library(xts)
suppressMessages(library(tseries))
```

```{r data}
df <- read.csv("~/Datasets/TS regression - data_akbilgic.csv")
df$date <- as.Date(as.character(df$date), format='%d-%b-%y')

ISE  <- xts(df$ISE, df$date)
SP   <- xts(df$SP,  df$date)
DAX  <- xts(df$DAX, df$date)
FTSE <- xts(df$FTSE, df$date)
NIKKEI <- xts(df$NIKKEI, df$date)
BOVESPA <- xts(df$BOVESPA, df$date)
```

## 1a. Determine if all the TS are stationary - Qualitative

The first step to both data exploration and determing if a time series is
stationary is to plot it. I'm plotting all six time series here. A stationary
time series will have an unchanging mean and should look stable, that is with
no upward or downward trend.

```{r ts_plots}
par(mfrow=c(3,2))
plot(ISE)
plot(SP)
plot(DAX)
plot(FTSE)
plot(NIKKEI)
plot(BOVESPA)
```

Visually all of these time series look stationary. None of them have an 
obvious trend. They are all more or less centered at 0 (not important for the
mean to be 0 but it is nice). I would say, qualitatively at least, that these
time series are stationary.

## 1.b Determine of all the TS are stationary -- quantitatively

One tool in our disposal for determining is a time series is stationary is the 
`kpss.test()` function. The KPSS test tests against the alternative that the 
series has a unit root. If a series has a unit root, it is has a stochastic 
trend that is unpredictable. In the KPSS test, if the p-value is large then the
null is accepted. Since the unit root is the alternative hypothesis, a low p-value
means that the time series is stationary, or at least there isn't enough evidence
to conclude that it is not stationary.

```{r}
library(tseries)

kpss.test(ISE)
adf.test(ISE)

kpss.test(SP)
adf.test(SP)

kpss.test(DAX)
adf.test(DAX)

kpss.test(FTSE)
adf.test(FTSE)

kpss.test(NIKKEI)
adf.test(NIKKEI)

kpss.test(BOVESPA)
adf.test(BOVESPA)
```

None of the KPSS levels are large enough to reject the null hypothesis,  
i.e. all these trends are stationary.

All the ADF tests reject the null and accept the alternative. ADF is backwards
from KPSS so the alternative is that the trend is stationary.

Both ADF and KPSS conclude that all trends are stationary.

## 2. Train & Test

```{r}
train <- df[1:(nrow(df)-10), ]
test <- df[(nrow(df)-10):nrow(df), ]
```

## 2. Linear Regression

Linearly regress ISE against remianing 5 stock index returns and determin
which coefficients are significant.

```{r}
fit <- lm(ISE ~ ., data=train[,-1])

summary(fit)
```

DAX, FTSE, and NIKKEI are significant coefficients. BOVESPA is significant 
at a 0.1 level so we won't count that one. SP is not significant at all.

About half the variance is explained by the regression model.

Explore the fitted model a bit.

```{r}
library(dplyr)
library(reshape2)
library(ggplot2)

train$ISE_fitted <- y_hat <- predict(fit)

melt(train, id.vars=('date'), measure.vars=c('ISE', 'ISE_fitted')) %>% 
  ggplot(aes(x=date, y=value, color=variable)) + geom_point()

ggplot(train, aes(x=ISE, y=ISE_fitted)) + geom_point()

par(mfrow=c(2,1))
plot(fit$residuals)

hist(fit$residuals)
```

The fit is decent. The residuals show no pattern and are normally distributed.
Plotting the real vs fitted values shows a very clear linear pattern but the 
overall scatter shows a lot of variance that is not captured. 

## 3. Lag Non-Significant Coefficients

We can improve the model by lagging the non-significant coefficients until 
they become sigfificant.

```{r}
library(DataCombine)

lag = 1
df_slide = slide(train, Var='SP', TimeVar='date', NewVar=paste0('SP_', lag), slideBy=-lag)
drops = c('date', 'ISE_fitted', 'SP')
summary(lm(ISE ~ ., data=df_slide[,!names(df_slide) %in% drops]))
```

A single day lag on SP brings the two non-significant coefficients into 
significance, but BOVESPA isn't yet significant enough.

```{r, message=FALSE}
library(DataCombine)

search = data.frame(data.frame(sp_lag=numeric(), bv_lag=numeric(), sp_pval=numeric(), bv_pval=numeric()))

sp_lags = 1:10
bv_lags = 1:10
for(sp_lag in sp_lags) {
  for(bv_lag in bv_lags) { 
    df_slide = slide(train, Var='SP', TimeVar='date', NewVar=paste0('SP_', lag), slideBy=-sp_lag)
    df_slide = slide(df_slide, Var='BOVESPA', TimeVar='date', NewVar=paste0('BOVESPA_', lag), slideBy=-bv_lag)
    drops = c('date', 'ISE_fitted', 'SP', 'BOVESPA')
    sm <- summary(lm(ISE ~ ., data=df_slide[,!names(df_slide) %in% drops]))
    
    search[nrow(search)+1,] = list(sp_lag, bv_lag, sm$coef[5,4], sm$coef[6,4])
  }
}

search[search$sp_pval < 0.02 & search$bv_pval < 0.02, ]
```

```{r, message=FALSE}
sp_lag = 2
bv_lag = 1

df_slide = slide(train, Var='SP', TimeVar='date', NewVar=paste0('SP_', sp_lag), slideBy=-sp_lag)
df_slide = slide(df_slide, Var='BOVESPA', TimeVar='date', NewVar=paste0('BOVESPA_', bv_lag), slideBy=-bv_lag)
drops = c('date', 'ISE_fitted', 'SP', 'BOVESPA')

model <- lm(ISE ~ ., data=df_slide[,!names(df_slide) %in% drops])
    
summary(model)
```

SP lag by 2, BOVESPA lag by 1 leads to all coefficients significant at a 0.02
level.

## 5. Find correlations between ISE and each independent variable. Sum the square of the correlations. How does it compare to R-squared from #4?

```{r}
cor(train$ISE, train$SP)
cor(train$ISE, train$DAX)
cor(train$ISE, train$FTSE)
cor(train$ISE, train$NIKKEI)
cor(train$ISE, train$BOVESPA)

sum(c(
  cor(train$ISE, train$SP),
  cor(train$ISE, train$DAX),
  cor(train$ISE, train$FTSE),
  cor(train$ISE, train$NIKKEI),
  cor(train$ISE, train$BOVESPA)
  )^2
)
```

The sum of square of correlations is much higher than R^2 found in 
part 4. R^2 is 0.51 which sum of squared correlations is 1.3

## 6. Concept question 1 - why do you think the R-squared in #4 is so much less than the sum of square of the correlations?

They are computed completely differently. R^2 is the percentage of variance
explained by the model. You can't just sum up the correlations of individual
variables. The variance explained by SP might also be explained by DAX. They
are not independent from each other.

## 7. Take the test dataset - perform the same lags from #4 and call predict() function using the lm regression object from #4. Why do you need to use the lm function object from #4?

We need to use the original model object because here we are testing a fitted 
model, not training a new model. This is suppose to be data that the model has
not seen yet for validation.

```{r}
test_slide = slide(test, Var='SP', TimeVar='date', NewVar=paste0('SP_', 2), slideBy=-2)
test_slide = slide(test_slide, Var='BOVESPA', TimeVar='date', NewVar=paste0('BOVESPA_', 1), slideBy=-1)
drops = c('date', 'SP', 'BOVESPA')

predicted_ISE = predict(model, newdata = test_slide[,!names(test_slide) %in% drops])
plot(test$ISE, predicted_ISE)
```

Not a spectacular prediction but not terrbile.

## 8. What do you find in #1 and why?

In part 1 I found that all the timeseries were stationary. They are return 
indexes which means it's the daily return from open to close (or maybe from 
close the previous day). I'm rather surprised that they are all stationary 
as there should be a definite upward trend in daily return from 2009-2010. 
Maybe this is already trend corrected data?
