---
title: "Assignment6"
author: "Scott Shepard"
date: "5/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Prompt

Input gas rate - this is the independent variable

Output gas CO2 % - this is the dependent variable that needs to be forecast

Tasks:

1. Use linear regression model - plot the ACF - what can you conclude ?
2. Use ARIMA (0,0,1) model for the residuals. Adjust the Input gas rate and Output CO2 % with the MA coefficient. Combine with the linear regression model. Plot the residuals.
3. Use ARIMA (1,0,0) model for the residuals. Adjust the Input gas rate and Output CO2 % with the AR coefficient. Combine with the linear regression model. Plot the residuals.
4. Use ARIMA (0,0,2) model for the residuals. Adjust the Input gas rate and Output CO2 % with the MA coefficient. Combine with the linear regression model. Plot the residuals.
5. Use ARIMA (2,0,0) model for the residuals. Adjust the Input gas rate and Output CO2 % with the AR coefficient. Combine with the linear regression model. Plot the residuals.
6. Use ARIMA (2,0,2) model for the residuals. Adjust the Input gas rate and Output CO2 % with the AR and MA coefficients. Combine with the linear regression model. Plot the residuals.
7. Use fractional ARIMA model (aka ARFIMA) for the output gas CO2% - plot the residuals, acf and pacf plots of the model. You can use an R package like fracdiff â€“ be careful to determine which lag to choose when executing this test.
8. Perform  Summaries,  Durbin-Watson and Box-Ljung tests for each model and build table to compare AICs, BICs and p-vaules for each test across the ARIMA and ARFIMA models.
9. Based on ACF plots and test results, which ARIMA model gives the best result in terms of residuals being close to white noise ?

## 1. Linear Regression

```{r}
library(ggplot2)
library(forecast)

df <- read.csv('~/Datasets/31006/Gas Furnace Dataset.csv')
names(df) <- c('input', 'output')

ggplot(df, aes(x=input, y=output)) + 
  geom_point() + 
  geom_smooth(method='lm') + 
  ggtitle('Gas Furnace Data')
```


```{r}
m <- lm(output ~ input, data=df)

acf(m$residuals)
```

Serious autocorrelation of the residuals at low lags.

```{r}
plot(m$residuals, type='l', main='Residuals of Linear Model')
```

Since the residuals are clearly autocorrelated, the assumption about 
non-correlated models are violated. The linear model coefficients can be
adjusted by using the Cochrane-Orcutt process: fit an ARMA model to the 
residuals, adjust X & Y by the coefficients of the model, refit and iterate
until it converges.

## 2. ARIMA (1,0,0)

I'm going to do the autoregressive models first.

The way to model errors using an autoregressive process is to:
  1. Build a naive linear model
  2. Model the residuals using AR(p)
  3. Adjust x & y using AR coefficient by x'(t) = x(t) - b*x(t-1)
  4. Run new linear model with x' and y'
  5. Using new esimates and slope and intercept to adjust previous model by
  new_slope = adjusted_slope and new_intercept = adjusted_intercept / (1-sum(AR_coef))
  6. Repeat 2-5 until coefficients converge

This is called the Cochrane-Orcutt process

```{r}
cochrane.orcutt <- function(x, y, order, iterations=10) {
  linear.model <- lm(y ~ x)
  res <- linear.model$residuals
  
  for(i in 1:iterations) {
    coefs <- adjust_lm_coef(x, y, res, order)
    res <- adjust_residuals(x, y, coefs[1], coefs[2])
  }
  
  list("coefs"=coefs, "residuals"=res)
}

adjust_lm_coef <- function(x, y, resid, order) {
  arma.model <- Arima(resid, order)
  
  xadj <- arma.innovation(x, arma.model)
  yadj <- arma.innovation(y, arma.model)
  
  lm.adj <- lm(yadj ~ xadj)
  
  n <- length(coef(arma.model))
  
  intercept_adj <- lm.adj$coefficients[1] / (1-sum(arma.model$coef[1:(n-1)]))
  slope_adj <- lm.adj$coefficients[2]
  c(intercept_adj, slope_adj)
}

adjust_residuals <- function(x, y, intercept, slope) {
  y - (intercept + x * slope)
}

arma.innovation <- function(x, arma.model, ar.truncation=10) {
  p <- arma.model$arma[1]
  q <- arma.model$arma[2]
  ar.coef <- arma.model$coef[seq_len(p)]
  ma.coef <- arma.model$coef[p + seq_len(q)]
  if (q == 0) {
    infinite.ar.coef <- ar.coef
  } else {
    infinite.ar.coef <- -ARMAtoMA(-ma.coef, -ar.coef, ar.truncation)
  }
  return(as.vector(filter(x, c(1, -infinite.ar.coef), side=1)))
}
```

```{r}
ar1 <- cochrane.orcutt(df$input, df$output, c(1,0,0))
ar1$coefs
```

Compare to running an ARIMA with xreg

```{r}
Arima(df$output, c(1,0,0), xreg = df$input)
```

```{r}
plot(ar1$residuals, type='l')
acf(ar1$residuals)
```

Still not white noise

```{r}
Arima(ar1$residuals, c(1,0,0))
```

```{r}
Arima(ar1$residuals, order=c(1,0,0))
```

## 5. ARIMA (2,0,0)

```{r}
ar2 <- cochrane.orcutt(df$input, df$output, c(2,0,0))
ar2$coefs
```

Compare to running ARIMA with xreg again.

```{r}
Arima(df$output, c(2,0,0), xreg = df$input)
```

Coefficients pretty much converged to the same value again!

```{r}
Arima(ar2$residuals, c(2,0,0))
```

AIC and BIC are slighly different with the two approaches even though 
log likelihood is the same. One of them is probably doing an adjustment for 
dropped values or something I would guess.

```{r}
library(nlme)
mod.gls <- gls(output ~ input, data=df, correlation=corARMA(p=1), method="ML")
summary(mod.gls)
```

## 2. ARIMA (0,0,1) 

Try with an MA(1) model for the residuals.

```{r, message=FALSE}
ma1 <- Arima(m$residuals, c(0,0,1))
lm_ma1 <- lm(arma.innovation(df$output, ma1) ~ arma.innovation(df$input, ma1))
acf(lm_ma1$residuals)
```

```{r}

```

```{r}
Arima(df$output, c(0,0,1), xreg = df$input)
```


```{r}

```

## 4. ARIMA (0,0,2)

```{r}
ma2 <- Arima(m$residuals, c(0,0,2))

df$input_ma2 <- adjust_input(df$input, coef(ma2)[1])

lm_ma2 <- lm(output ~ input_ma2, data=df)

plot(lm_ma2$residuals, type='l')
```

## 6. ARIMA (2,0,2)

```{r}
ar2ma2 <- Arima(m$residuals, c(2,0,2))

df$input_ar2 <- adjust_input(df$input, coef(ar2ma2)[1])

lm_ar2 <- lm(output ~ input_ar2, data=df)

plot(lm_ar2$residuals, type='l')
```

## 7. Fractional ARIMA

```{r}

```

## 8. Compare Models

```{r}

```

## 9. Conclusions

```{r}

```
