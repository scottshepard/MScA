---
title: "Poisson Count Time Series Model on Electric Vehicle Charging"
subtitle: "31006: Final Project "
author: "Scott Shepard"
date: "5/28/2019"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This is a notebook for my section of the final project for MScA 31006 Time
Series Analysis. The prompt is to pick a real-world time series dataset, 
fit three different models, and compare the results.

For our project we chose an electric vehicle charging dataset from Argonne
National Labs. This dataset contains the voltage output at a charging station
for electric vehicles available to employees at Argonne. The data is in 
15-minute increments and shows the current voltage at that station. The higher
the voltage, the more cars are charging. In general, about 3.3v equals one
car.

In this notebook, I fit an Autoregressive Conditional Poisson (ACP) model, also
called an Integer General Autoregressive Conditional Heteroskedasticity model, 
to a transformed dataset, then use it to predict the daily average voltage.

# Electric Vehicle Charging Data

1. Read the data
2. Explore the data
3. Transform the data

## Data Exploration

Read in the electric vehicle charging data and plot it whole.

```{r read_data}
df_full <- read.csv('~/Datasets/31006/EVData.csv', stringsAsFactors=F)
names(df_full)[2] <- "EV"
df_full$Time <- as.POSIXct(df_full$Time, format = "%Y-%m-%d %H:%M:%S")
df_full <- df_full[!is.na(df_full$Time), ]

library(ggplot2)
ggplot(df_full, aes(x=Time, y=EV)) + geom_line()
```

The data is quite spiky. This might be tough to try to predict. Try zooming
in on the first few days.

```{r}
ggplot(tail(df_full, 2000), aes(x=Time, y=EV)) + geom_line()
```

There _is_ a strong timeseries component. We just need to be able to distinguish it from the stochastic spikes.

## Daily Count Time Series

One approach to get a less stochastic time series is to turn the 15-minute
EV charging data into a daily count of how many cars were plugged in. Here
we assume that 3.3 volts is the standard charging for one car. Counting
it out in 3.3 unit chunks gives us the number of cars plugged in at that
time.

```{r, warning=F, message=F}
library(dplyr)

df <- df_full

df$Date <- as.Date(df$Time)
df$cars <- round(df$EV / 3.3)

ggplot(tail(df, 2000), aes(x=Time, y=cars)) + geom_line()
```

## Data Transformation

Now we want to know how many cars in total there were ofer the course of the day.

We can't just group by date and sum up total number of cars. 
There could be one or two cars  plugged in for the whole day that would 
register artificually high. We need to count the comings and goings 
chunk by chunk and add it up for each day.

```{r}
count_cars <- function(day) {
  #print(day$Date[1])
  current_cars <- 0
  total_cars <- 0
  if(all(is.na(day$cars))) {
    return(NA)
  }
  if(sum(day$cars,na.rm=T) == 0) {
    return(0)
  }
  for(i in 1:(nrow(day)-1)) {
    row <- day[i,]
    if(is.na(row[['cars']])) {
      next
    }
    if(row[['cars']] > current_cars) {
      total_cars = total_cars + (row[['cars']] - current_cars)
      current_cars = row[['cars']]
    } else if(row[['cars']] < current_cars) {
      current_cars = row[['cars']]
    }
  }
  return(total_cars)
}
```

Summing total vs counting jumps

```{r}
filter(df, Date == '2017-07-25') %>% 
  ggplot(aes(x=Time, y=cars)) + geom_line() + 
  labs(title='Number of Cars per 15 minutes',
       subtitle='7/25/17')

c(Summing = sum(df[df$Date == "2017-07-25", ]$cars),
  Counting = count_cars(df[df$Date == "2017-07-25", ]))
```

```{r}
daily <- group_by(df, Date) %>%
  do(data.frame(n_cars = count_cars(.),
                sum = sum(.$cars)))

head(daily)
```

```{r}
ggplot(daily, aes(x=Date, y=n_cars)) + geom_line(stat='identity')
```

Overall the plot of daily car counts looks pretty similar to 15 minute EV 
data. 

How does one week in August compare?

```{r}
filter(daily, Date >= '2018-08-01', Date < '2018-08-08') %>%
  ggplot(aes(x=Date, y=n_cars)) + geom_line()

filter(daily, Date >= '2018-08-01', Date < '2018-08-08') %>%
  ggplot(aes(x=Date, y=sum)) + geom_line()

filter(df, Date >= '2018-08-01', Date < '2018-08-08') %>%
  ggplot(aes(x=Time, y=EV)) + geom_line()
```

So it can transform pretty dramatically with lots of up and down.

## Train / Test Split

The final step before modeling is to split into train & test datasets.
The timeseries package I'm using can'thandle gaps in the data so I'm using 
data since the 6/15/18 to avoid the large gap in the summer of 2018.

```{r}
train <- daily[daily$Date >= '2018-06-15', ]
test <- train[(round(nrow(train)*.9)):nrow(train),]
train <- train[1:(round(nrow(train)*.9)-1),]

train$n_cars[is.na(train$n_cars)] <- 0
train$sum[is.na(train$sum)] <- 0

c(nrow(train), nrow(test))
```

With 34 data points in test we are predicting about a month out.

# Poisson Time Series Modeling

## ACP Model

The library `tscount` provides the function `tsglm`. It fits an integer-valued GARCH model of order p and q, abbreviated as INGARCH(p,q). These models are also known as autoregressive conditional Poisson (ACP) models.

I have specificed a seasonality of 1 (daily) and 7 (weekly) to be fitted.

```{r}
library(tscount)

count_carsfit_pois <- tsglm(
  train$n_cars,  
  model = list(past_obs = c(1, 7)),
  distr="poisson")

summary(count_carsfit_pois)

sum_carsfit_pois <- tsglm(
  train$sum,  
  model = list(past_obs = c(1, 7)),
  distr="poisson")

summary(sum_carsfit_pois)
```

```{r, fig.height=5, fig.width=8}
library(reshape2)

train$count.fitted.values <- count_carsfit_pois$fitted.values
train$sum.fitted.values <- sum_carsfit_pois$fitted.values

melt(train, id.vars='Date', measure.vars = c('count.fitted.values', 'n_cars')) %>%
  ggplot(aes(x=Date, y=value, color=variable)) + 
  geom_point() + geom_line() + 
  theme(legend.position = "top",
        legend.title = element_blank())

melt(train, id.vars='Date', measure.vars = c('sum.fitted.values', 'sum')) %>%
  ggplot(aes(x=Date, y=value, color=variable)) + 
  geom_point() + geom_line() + 
  theme(legend.position = "top",
        legend.title = element_blank())
```

Not bad? It seems like the poisson time series captures the weekend
zeros pretty well.

```{r}
acf(residuals(count_carsfit_pois))
plot(residuals(count_carsfit_pois), type='l')
```

```{r}
acf(residuals(sum_carsfit_pois))
plot(residuals(sum_carsfit_pois), type='l')
```

Well hot damn that sure seems to fit well. The residuals look like
white noise and the ACF chart indicates no autocorrelation.

```{r}
Box.test(residuals(count_carsfit_pois), type='Ljung-Box')
Box.test(residuals(sum_carsfit_pois), type='Ljung-Box')
```

## Forecasting with INGARCH / ACP

```{r}
tp_count <- predict(count_carsfit_pois, 34)
tp_sum <- predict(sum_carsfit_pois, 34)

test$count_prediction <- tp_count$median
test$sum_prediction <- tp_sum$median
```

```{r}
melt(test, id.vars='Date', measure.vars=c('n_cars', 'count_prediction')) %>%
  ggplot(aes(x=Date, y=value, color=variable)) + 
  geom_point() + geom_line() + 
  #geom_bar(stat='identity') + 
  labs(title='Test Period - Predictions vs. Actuals',
       subtitle = paste(test$Date[1], 'to', test$Date[nrow(test)]))
```

Interesting pattern in the prediictions. It seems to do pretty well
initially but trend towards the median as we go farther out. I 
wouldn't trust this prediction beyond one or two weeks.

```{r}
melt(test, id.vars='Date', measure.vars=c('sum', 'sum_prediction')) %>%
  ggplot(aes(x=Date, y=value, color=variable)) + 
  geom_point() + geom_line() + 
  #geom_bar(stat='identity') + 
  labs(title='Test Period - Predictions vs. Actuals',
       subtitle = paste(test$Date[1], 'to', test$Date[nrow(test)]))
```

## Error Analysis

```{r}
week_i <- c()
i <- 1
while(length(week_i) < nrow(test)) {
  week_i <- c(week_i, rep(i,7))
  i = i + 1
}
week_i <- week_i[1:nrow(test)]
test$week_i <- week_i

test$residual <- test$n_cars - test$prediction
test$actual <- test$n_cars

residual_sum_of_squares <- function(preds, actuals) {
  residuals <- actuals - preds
  sum(residuals**2)
}

mean_squared_error <- function(preds, actuals) {
  residual_sum_of_squares(preds, actuals) / length(preds)
}

mean_absolute_percentage_error <- function(preds, actuals) {
  residuals <- actuals - preds
  if(any(actuals == 0)) {
    actuals[actuals == 0] <- 1
  }
  100 / length(residuals) * sum(abs(residuals)/ actuals)
}

symmetric_mean_absolute_percentage_error <- function(preds, actuals) {
  residuals <- actuals - preds
  if(any(actuals == 0)) {
    actuals[actuals == 0] <- 1
  }
  if(any(preds == 0)) {
    preds[preds == 0] <- 1
  }
  100 / length(residuals) * sum(residuals / ((abs(actuals) + abs(preds))/2))
}

error_df <- group_by(test, week_i) %>%
  do(
    data.frame(
      rss = residual_sum_of_squares(.$count_prediction, .$n_cars),
      mse = mean_squared_error(.$count_prediction, .$n_cars),
      mape = mean_absolute_percentage_error(.$count_prediction, .$n_cars),
      smape = symmetric_mean_absolute_percentage_error(.$count_prediction, .$n_cars)
  ))

print(error_df)
```

Initial small errors turn into hot garbage pretty quickly.
