{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iLykei Lecture Series\n",
    "\n",
    "# Advanced Machine Learning and Artificial Intelligence (MScA 32017)\n",
    "\n",
    "# Reinforcement Learning\n",
    "\n",
    "## Notebook 1: Q-Value Iterations and Q-Learning\n",
    "\n",
    "## Yuri Balasanov, Mihail Tselishchev, &copy; iLykei 2018\n",
    "\n",
    "##### Main texts: \n",
    "\n",
    "Hands-On Machine Learning with Scikit-Learn and TensorFlow, Aurelien Geron, &copy; Aurelien Geron 2017, O'Reilly Media, Inc\n",
    "\n",
    "Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning series), &copy; 2018 Richard S. Sutton, Andrew G. Barto, The MIT Press\n",
    "\n",
    "This notebook discusses example of Markov Decision Process from Chapter 16 of the book. \n",
    "\n",
    "## Description of Markov Decision Process\n",
    "\n",
    "Consider example of MDP from slide 16 of the session materials.\n",
    "\n",
    "Create numPy array of transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of T:  (3, 3, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.7, 0.3, 0. ],\n",
       "        [1. , 0. , 0. ],\n",
       "        [0.8, 0.2, 0. ]],\n",
       "\n",
       "       [[0. , 1. , 0. ],\n",
       "        [nan, nan, nan],\n",
       "        [0. , 0. , 1. ]],\n",
       "\n",
       "       [[nan, nan, nan],\n",
       "        [0.8, 0.1, 0.1],\n",
       "        [nan, nan, nan]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "nan=np.nan\n",
    "# Transition probabilities\n",
    "T=np.array([ # shape=[s,a,s'] \\\n",
    "    [[.7,.3,.0],[1.0,0.0,0.0],[0.8,0.2,0.0]], \\\n",
    "    [[0.0,1.0,0.0],[nan,nan,nan],[0.0,0.0,1.0]], \\\n",
    "    [[nan,nan,nan],[0.8,0.1,0.1],[nan,nan,nan]],])\n",
    "print('Shape of T: ',T.shape)\n",
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first matrix of the array shows transition probabilities from state $S_0$ using actions $a_0$ (first row), $a_1$ (second row) and $a_2$ (third row). Similarly, second matrix contains transition probabilities from state $S_1$ and the last matrix contains transition probabilities from state $S_2$. \n",
    "\n",
    "Create numPy array of rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of R:  (3, 3, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 10.,   0.,   0.],\n",
       "        [  0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.]],\n",
       "\n",
       "       [[  0.,   0.,   0.],\n",
       "        [ nan,  nan,  nan],\n",
       "        [  0.,   0., -50.]],\n",
       "\n",
       "       [[ nan,  nan,  nan],\n",
       "        [ 40.,   0.,   0.],\n",
       "        [ nan,  nan,  nan]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R=np.array([ # shape=[a,s,a'] \\ \n",
    "[[10.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]], \\\n",
    "[[0.0,0.0,0.0],[nan,nan,nan],[0.0,0.0,-50.0]], \\\n",
    "[[nan,nan,nan],[40.0,0.0,0.0],[nan,nan,nan]],])\n",
    "print('Shape of R: ',R.shape)\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Array of available actions for each of the states is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_actions=[[0,1,2],[0,2],[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the description of the MDP is done.\n",
    "\n",
    "## Q-Value Iterations\n",
    "\n",
    "Initiate Q as -inf for impossible actions, make Q=0 for all possible actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-inf -inf -inf]\n",
      " [-inf -inf -inf]\n",
      " [-inf -inf -inf]]\n",
      "Initiated Q:\n",
      "[[  0.   0.   0.]\n",
      " [  0. -inf   0.]\n",
      " [-inf   0. -inf]]\n"
     ]
    }
   ],
   "source": [
    "Q=np.full((3,3),-np.inf)\n",
    "print(Q)\n",
    "for state,actions in enumerate(possible_actions):\n",
    "    Q[state,actions]=0.0\n",
    "print('Initiated Q:')\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set discount rate $\\gamma=0.95$ and the number of iterations of the Q-Value algorithm\n",
    "$$Q^*_{k+1}(S,a)=\\sum_{s'} P(S,a,S') \\left[R(S,a,S') + \\gamma \\max_{a'} Q^*_k(S',a') \\right].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_rate=0.95\n",
    "n_iterations=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run just one iteration to reproduce manual calculations on slide 23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \n",
      "[[  7.   0.   0.]\n",
      " [  0. -inf -50.]\n",
      " [-inf  32. -inf]]\n",
      "Q_prev: \n",
      "[[  0.   0.   0.]\n",
      " [  0. -inf   0.]\n",
      " [-inf   0. -inf]]\n"
     ]
    }
   ],
   "source": [
    "Q_prev=Q.copy()\n",
    "for s in range(3):\n",
    "    for a in possible_actions[s]:\n",
    "        Q[s,a]=np.sum([T[s,a,sp]*(R[s,a,sp] \\\n",
    "                                  +discount_rate*np.max(Q_prev[sp])) \\\n",
    "                      for sp in range(3)])\n",
    "print('Q: ')\n",
    "print(np.matrix(Q))\n",
    "print('Q_prev: ')\n",
    "print(np.matrix(Q_prev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Array `Q_prev` is $Q^*_1(S,a)$. Note that the first row was initiated at 0. Array $Q$ is the next iteration: the first row changed to `[7.,0.,0.]`. This is consistent with slide 23.\n",
    "\n",
    "Initiate the matrix again and run the recursion `n_iterations` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q=np.full((3,3),-np.inf)\n",
    "for state,actions in enumerate(possible_actions):\n",
    "    Q[state,actions]=0.0\n",
    "for iteration in range(n_iterations):\n",
    "    Q_prev=Q.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q[s,a]=np.sum([T[s,a,sp]*(R[s,a,sp] \\\n",
    "                                  +discount_rate*np.max(Q_prev[sp])) \\\n",
    "                           for sp in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality matrix $Q^*(S,a)$ has converged to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      "[[21.88646117 20.79149867 16.854807  ]\n",
      " [ 1.10804034        -inf  1.16703135]\n",
      " [       -inf 53.8607061         -inf]]\n"
     ]
    }
   ],
   "source": [
    "print('Q:')\n",
    "print(np.matrix(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the optimal action for each state that follows from $Q^*(S,a)$ is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal actions by state: \n",
      "[0 2 1]\n"
     ]
    }
   ],
   "source": [
    "print('Optimal actions by state: ')\n",
    "print(np.argmax(Q,axis=1)) # max by rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show importance of the discount rate try the same iterations with $\\gamma=0.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      "[[18.91891892 17.02702703 13.62162162]\n",
      " [ 0.                -inf -4.87971488]\n",
      " [       -inf 50.13365013        -inf]]\n",
      "Optimal actions by state: \n",
      "[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "discount_rate=0.9\n",
    "Q=np.full((3,3),-np.inf)\n",
    "for state,actions in enumerate(possible_actions):\n",
    "    Q[state,actions]=0.0\n",
    "for iteration in range(n_iterations):\n",
    "    Q_prev=Q.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q[s,a]=np.sum([T[s,a,sp]*(R[s,a,sp] \\\n",
    "                                  +discount_rate*np.max(Q_prev[sp])) \\\n",
    "                           for sp in range(3)])\n",
    "print('Q:')\n",
    "print(np.matrix(Q))\n",
    "print('Optimal actions by state: ')\n",
    "print(np.argmax(Q,axis=1)) # max by rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With higher discount rate (future rewards are more important) in state 1 the best strategy is to select action $a_2$ and take the hit of -50 reward points: sacrifice for opportunity to make more rewards in the future. \n",
    "But with lower discount rate (future rewards are less important) it is better to stay in $S_1$ forever, with no more rewards, but with no big losses either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "Apply Q-Learning to the same MDP. This method is not using knowledge about transition probabilities or all rewards. Only the immediate reward is experienced when the action is selected.\n",
    "\n",
    "The iterations follow the equation\n",
    "$$Q_{k+1}(S,a)=(1-\\alpha) Q_k(S,a)+ \\alpha \\left[r+ \\gamma \\max_{a'}Q_k(S',a') \\right] $$\n",
    "$$=Q_k(S,a)+\\alpha \\left( [r+ \\gamma \\max_{a'}Q_k(S',a')]-Q_k(S,a) \\right),$$ where the second term of the last equation represents learning by taking difference between the future expected Q-value $[r+ \\gamma \\max_{a'}Q_k(S',a')]$ and the current value $Q_k(S,a)$, weighted by $\\alpha$ as an adjustment to $Q_k(S,a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_rate=.99\n",
    "learning_rate0 = 0.05\n",
    "learning_rate_decay = 0.1\n",
    "n_iterations = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0 # starting state\n",
    "Q=np.full((3,3),-np.inf) # -inf for impossible actions\n",
    "for state,actions in enumerate(possible_actions):\n",
    "    Q[state,actions]=0.0 # 0 for possible actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below transition probabilities and reward matrices are used only to simulate response from the environment. Agent's learning process does not assume them known and learns action-value function $Q(S,a)$ only from experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    a = np.random.choice(possible_actions[s]) # random action from available in s\n",
    "    sp = np.random.choice(range(3),p=T[s,a]) # random selection of new state\n",
    "    reward = R[s,a,sp]\n",
    "    learning_rate = learning_rate0/(1+iteration*learning_rate_decay) # gradually decaying learning rate\n",
    "    Q[s,a] = ((1-learning_rate)* \\\n",
    "              Q[s,a]+learning_rate* \\\n",
    "              (reward+discount_rate*np.max(Q[sp])))\n",
    "    s = sp # next state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.03138964,   1.5442264 ,   1.27921677],\n",
       "       [  0.        ,         -inf, -14.64484033],\n",
       "       [        -inf,  13.30664129,         -inf]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Q,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this iterative process does not result in selection of action 2 in state 1 even if discount rate goes up to 0.99. Possible explanation is: low learning rate and its quick decay do not allow the learning process to see enough benefits of taking immediate significant loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
