{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f765eb10-54e6-4a14-912b-e5117e8f433d"
    }
   },
   "source": [
    "# iLykei Lecture Series\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "# Introduction to Deep Learning\n",
    "\n",
    "# Introduction to Keras\n",
    "\n",
    "## Yuri Balasanov, Leonid Nazarov, &copy; iLykei 2017-2018\n",
    "\n",
    "\n",
    "Keras is an open source neural network library written in Python. It is capable of running on top of one of several libraries (backends): MXNet, Deeplearning4j, Tensorflow, CNTK or Theano. Designed to enable fast experimentation with deep neural networks, it focuses on being minimal, modular and extensible. It was developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System), and its primary author and maintainer is Fran√ßois Chollet, a Google engineer.\n",
    "\n",
    "Keras is a very simple and user friendly modular API that allows managing and organizing layers of neural network in a much easier way than TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential model\n",
    "\n",
    "The core data structure of Keras is **model**, a way to organize layers of the neural network. <br>\n",
    "\n",
    "The simplest type of model is the Sequential model, a linear stack of layers.<br>\n",
    "\n",
    "Design a simple neural network with one hidden layer consisting of 3 neurons.  \n",
    "\n",
    "First, define the type of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "model =  Sequential() # create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Specify the first (hidden) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=3, input_dim=30)) # add hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative way of calling the same function creating hidden layer is: <br>\n",
    "\n",
    "`model.add(Dense(3, input_dim=30, activation='relu')).` <br>\n",
    "\n",
    "Difference between the two is activation type that can be added separately or inside `model.add` method.\n",
    "\n",
    "Parameter `input_dim` sets number of inputs (predictors). <br>\n",
    "Parameter `activation` is set to `relu` which is a common [activation function](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) for neural networks.\n",
    "\n",
    "Input layer is not explicitly created. Instead, only number of neurons (features) that feed into the first hidden layer, needs to be specified (30 for the Wisconsin Breast Cancer Dataset considered in this notebook). \n",
    "\n",
    "There is also no need to worry about the input dimensions for subsequent layers: neurons in the previous layer form input for the next layer. The output dimension of the first hidden layer is equal to 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection of activation function for the hidden layer can be done from the list of available activations, providing formulas for the most popular ones: \n",
    "\n",
    "- softmax(x) :   $f_{i}(x)=\\frac{e^{x_{i}}}{\\sum_{i=1}^{n}e^{x_{i}}},i=1,...,n$\n",
    "- elu(x)\n",
    "- selu(x)\n",
    "- softplus(x) = $ln(1 + e^{x})$\n",
    "- softsign(x) = $\\frac {x}{1+|x|}$\n",
    "- relu(x) = $max(0,x)$; (those familiar with options trading will recognize call option payof function in `relu`)\n",
    "- tanh(x) \n",
    "- sigmoid(x) = $\\frac {1}{1+e^{-x}}$\n",
    "- hard_sigmoid(x)\n",
    "- linear(x) = $x$\n",
    "- LeakyReLU(x,alpha) = $max(\\alpha *x,x)$\n",
    "\n",
    "Activation function 'relu' has earned significant popularity recently because it usually provides faster convergence relative to 'tanh' or 'sigmoid'.\n",
    "\n",
    "Since activation was not specified in the previous cell add it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Activation('relu')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add output layer with 1 neuron and 'sigmoid' activation function producing predictive probability for Class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output layer for binary classification\n",
    "model.add(Dense(units=1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot displays the nework architecture for Wisconsin breast cancer example and explains the nature of **Dense** (fully connected) layer: each neuron of it is connected with all units of the next layer.\n",
    " \n",
    "![Model plot](https://ilykei.com/api/fileProxy/documents%2FAdvanced%20Machine%20Learning%2FLecture%207%20AdvML%2FR_nn_plot.png)\n",
    "\n",
    "The picture was created with function *plot()* from R **neuralnet** package. <br>\n",
    "\n",
    "Keras has tools for model visualization of its own. \n",
    "\n",
    "Module `keras.utils.vis_utils` provides utility functions to plot a Keras model (using **graphviz**). \n",
    "Unlike the R function used above this utility does not show neurons. It describes network in terms of layers.   \n",
    "\n",
    "The code below plots another graph of the same network model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"337pt\" viewBox=\"0.00 0.00 156.00 337.00\" width=\"156pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 333)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-333 152,-333 152,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140134106884304 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140134106884304</title>\n",
       "<polygon fill=\"none\" points=\"23,-219.5 23,-255.5 125,-255.5 125,-219.5 23,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-233.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140134106551296 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140134106551296</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 148,-182.5 148,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-160.8\">activation_1: Activation</text>\n",
       "</g>\n",
       "<!-- 140134106884304&#45;&gt;140134106551296 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140134106884304-&gt;140134106551296</title>\n",
       "<path d=\"M74,-219.4551C74,-211.3828 74,-201.6764 74,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"77.5001,-192.5903 74,-182.5904 70.5001,-192.5904 77.5001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140134106186472 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140134106186472</title>\n",
       "<polygon fill=\"none\" points=\"23,-73.5 23,-109.5 125,-109.5 125,-73.5 23,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-87.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 140134106551296&#45;&gt;140134106186472 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140134106551296-&gt;140134106186472</title>\n",
       "<path d=\"M74,-146.4551C74,-138.3828 74,-128.6764 74,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"77.5001,-119.5903 74,-109.5904 70.5001,-119.5904 77.5001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140134106186024 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140134106186024</title>\n",
       "<polygon fill=\"none\" points=\"0,-.5 0,-36.5 148,-36.5 148,-.5 0,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-14.8\">activation_2: Activation</text>\n",
       "</g>\n",
       "<!-- 140134106186472&#45;&gt;140134106186024 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140134106186472-&gt;140134106186024</title>\n",
       "<path d=\"M74,-73.4551C74,-65.3828 74,-55.6764 74,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"77.5001,-46.5903 74,-36.5904 70.5001,-46.5904 77.5001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140134106884752 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140134106884752</title>\n",
       "<polygon fill=\"none\" points=\"15,-292.5 15,-328.5 133,-328.5 133,-292.5 15,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-306.8\">140134106884752</text>\n",
       "</g>\n",
       "<!-- 140134106884752&#45;&gt;140134106884304 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140134106884752-&gt;140134106884304</title>\n",
       "<path d=\"M74,-292.4551C74,-284.3828 74,-274.6764 74,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"77.5001,-265.5903 74,-255.5904 70.5001,-265.5904 77.5001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Another way is to save the diagram to a file and then plot it with the function `plot_model` which takes two optional arguments:\n",
    "- `show_shapes` (defaults to False) controls whether output shapes are shown in the graph.\n",
    "- `show_layer_names` (defaults to True) controls whether layer names are shown in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model1.png',show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the plot of the the same network model with additional information loaded from the file *model1.png* using a line of markdown code:\n",
    "\n",
    "\"! [model] (./Path to model)\"\n",
    "\n",
    "The resulting plot is:\n",
    "\n",
    "![Model plot](https://ilykei.com/api/fileProxy/documents%2FAdvanced%20Machine%20Learning%2FLecture%207%20AdvML%2Fmodel1.png)\n",
    "\n",
    "Summary of the model can also be shown including layers types, shapes and the number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 3)                 93        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 97\n",
      "Trainable params: 97\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use more compact syntax for describing the same Sequential network: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  Sequential()\n",
    "model.add(Dense(3, input_dim=30,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or even define the model in one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Dense(3, input_dim=30),Activation('relu'),\n",
    "    Dense(1),Activation('sigmoid')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More complex architectures \n",
    "\n",
    "## Functional API\n",
    "\n",
    "Keras' functional API provides tools for defining more complex models, such as multi-output models, directed acyclic graphs, or models with shared layers. \n",
    "\n",
    "Models are defined by creating instances of layers and connecting them directly to each other pair by pair, then defining a model that specifies the layers to act as the input and output to the model.\n",
    "\n",
    "Define the simple model as above using functional API. \n",
    "\n",
    "Start with the input layer. Recall that input layer was not explicitly created in sequential model. Here it must be created and defined as a standalone Input layer that specifies the shape of input data.\n",
    "\n",
    "The input layer takes shape (tensor shape) argument that is a tuple that indicates the dimensionality of the input data.\n",
    "\n",
    "The shape must explicitly leave room for the mini-batch size used when splitting the data during training process. \n",
    "\n",
    "Therefore, the shape tuple is always defined with a hanging last dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(?, 30) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "\n",
    "# This returns a tensor\n",
    "inputs = Input(shape=(30,))\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A layer instance is callable (on a tensor), and it returns a tensor. The layers in the model are connected pairwise. <br>\n",
    "This is done by specifying where the input comes from when defining each new layer. \n",
    "\n",
    "The following line of code states that the input of Dense layer `hidden_1` comes from `inputs`. <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = Dense(3, activation='relu',name='hidden_1')(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter `name` is optional. It can be used to pass a name of any layer as an argument.\n",
    "\n",
    "Define the output layer and create a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "output = Dense(1, activation='sigmoid')(hidden)\n",
    "model_API = Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model class is used to create a model from layers. It requires only the input and output layers specifications.\n",
    "\n",
    "Here is the model we built.\n",
    "![model_API](https://ilykei.com/api/fileProxy/documents%2FAdvanced%20Machine%20Learning%2FLecture%207%20AdvML%2Fmodel_API.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Concatenating inputs\n",
    "\n",
    "Consider now more complex models. \n",
    "\n",
    "Sometimes in more complex models inputs from a list need to be *concatenated*.\n",
    "\n",
    "Input is a list of tensors and the result is a single tensor: concatenation of all inputs from the list. \n",
    "\n",
    "All tensors must have the same shape except for the concatenation axis. <br>\n",
    "The meaning of this condition can be illustrated on a simple example of tensors of rank 2 which are matrices. <br>\n",
    "\n",
    "Consider, for example, 2 matrices $A,~B$ with the same number of columns, but different number of rows:\n",
    "\n",
    "$$A=\\left(\\begin{array}{cc} \n",
    "a_{11} & a_{12} & a_{13} & a_{14} \\\\\n",
    "a_{21} & a_{22} & a_{23} & a_{24} \\\\\n",
    "a_{31} & a_{32} & a_{33} & a_{34}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "$$B=\\left(\\begin{array}{cc} \n",
    "b_{11} & b_{12} & b_{13} & b_{14} \\\\\n",
    "b_{21} & b_{22} & b_{23} & b_{24} \\\\\n",
    "b_{31} & b_{32} & b_{33} & b_{34} \\\\\n",
    "b_{41} & b_{42} & b_{43} & b_{44} \\\\\n",
    "b_{51} & b_{52} & b_{53} & b_{54}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "These matrices as inputs can be concatenated by first dimension (axis), i.e. by appending rows of $B$ to rows of $A$:\n",
    "\n",
    "$$\\left(\\begin{array}{cc} \n",
    "a_{11} & a_{12} & a_{13} & a_{14} \\\\\n",
    "a_{21} & a_{22} & a_{23} & a_{24} \\\\\n",
    "a_{31} & a_{32} & a_{33} & a_{34} \\\\\n",
    "b_{11} & b_{12} & b_{13} & b_{14} \\\\\n",
    "b_{21} & b_{22} & b_{23} & b_{24} \\\\\n",
    "b_{31} & b_{32} & b_{33} & b_{34} \\\\\n",
    "b_{41} & b_{42} & b_{43} & b_{44} \\\\\n",
    "b_{51} & b_{52} & b_{53} & b_{54}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "But they cannot be concatenated by the second dimension: columns of $A$ cannot be concatenated with columns of $B$ because the numbers of rows are different.\n",
    "\n",
    "Build a model with two inputs using layer of concatenated inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate\n",
    "input1 = Input(shape=(10,), name='input_1')\n",
    "hidden = Dense(3, activation='relu',name='hidden1')(input1)\n",
    "input2 = Input(shape=(5,), name='input_2')\n",
    "merge = concatenate([hidden,input2], name='merge')\n",
    "hidden2 = Dense(3, activation='relu',name='hidden2')(merge)\n",
    "output = Dense(1, activation='sigmoid')(hidden2)\n",
    "model_2_inputs = Model(inputs=[input1,input2], outputs=output)\n",
    "plot_model(model_2_inputs, to_file='model_2_inputs.png',show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the result.\n",
    "![model_2_inputs](https://ilykei.com/api/fileProxy/documents%2FAdvanced%20Machine%20Learning%2FLecture%207%20AdvML%2Fmodel_2_inputs.png)  \n",
    "\n",
    "## Shared layers\n",
    "\n",
    "Another way of adding flexibility to network design is shared layers.\n",
    "\n",
    "To share a layer across different inputs, simply instantiate the layer once, then call it on as many inputs as necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = Input(shape=(8,), name='input_1')\n",
    "input2 = Input(shape=(8,), name='input_2')\n",
    "shared = Dense(5, activation='relu',name='shared')\n",
    "x1 = shared(input1)\n",
    "x2 = shared(input2)\n",
    "output1 = Dense(1, activation='sigmoid', name='output_1')(x1)\n",
    "output2 = Dense(1, activation='sigmoid', name='output_2')(x2)\n",
    "model_shared = Model(inputs=[input1,input2], outputs=[output1,output2])\n",
    "plot_model(model_shared, to_file='model_shared.png',\n",
    "           show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting model has two inputs, two outputs and shared layer\n",
    "![model_shared](https://ilykei.com/api/fileProxy/documents%2FAdvanced%20Machine%20Learning%2FLecture%207%20AdvML%2Fmodel_shared.png)  \n",
    "\n",
    "Note that when we reuse the same layer instance  multiple times, the weights of the layer  are also being reused  (it is effectively the same layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "**Assignment 1**\n",
    "\n",
    "Write code creating the following network architecture as *test_model1*.\n",
    "\n",
    "![test_model1](https://ilykei.com/api/fileProxy/documents%2FAdvanced%20Machine%20Learning%2FLecture%207%20AdvML%2Ftest_model1.png) \n",
    "\n",
    "Print out the plot of the resulting model.\n",
    "\n",
    "Enter code in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Skipped code\n",
    "# Assignment 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plot_model(test_model1, to_file='test_model1.png',\n",
    "#           show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test_model1](./test_model1.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge layers\n",
    "\n",
    "Merge layers is a set of layers that merge input tensors  executing arithmetic operations over them or just concatenating them. \n",
    "\n",
    "The list of merge layers is not large: \n",
    "- Add, Subtract, Multiply, Average, Maximum  - take as input a list of tensors, all of the same shape, and return a single tensor (also of the same shape) - the result of corresponding pointwise arithmetic operation \n",
    "- Dot - dot product, \n",
    "- Concatenate has been already discussed earlier. \n",
    "\n",
    "Functional API uses lowercase variants of their names. Import them from `keras.layers` before using, e.g.\n",
    "\n",
    "`from keras.layers import add`\n",
    "\n",
    "NB! Though `subtract` layer is described in Keras documentation it may not work in earlier versions giving the error message **\"cannot import name 'subtract'\"** when you try to import it. \n",
    "In order to subtract two inputs you may use `Lambda` core layer that wraps arbitrary expression as a Layer object. For example, the following line:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Lambda\n",
    "sqr_input = Lambda(lambda x: x*x)(input1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defines layer that returns squared input. \n",
    "\n",
    "Here is a way of subtracting two inputs using `Lambda` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(100,))\n",
    "y = Input(shape=(100,))\n",
    "# Note: shapes must match!\n",
    "subtract_layer = Lambda(lambda inputs: inputs[0] - inputs[1])\n",
    "diff = subtract_layer([x, y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "    \n",
    "**Assignment 2**\n",
    "\n",
    "Create network with two inputs. <br>\n",
    "Input shapes are (10,) and (20,). <br>\n",
    "Each input is processed with its own Dense layer `hidden1` and `hidden2` consisting of 5 neurons. <br>\n",
    "The network output should be square of differences of outputs of `hidden1` and `hidden2`. <br>\n",
    "Draw this network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Skipped code\n",
    "# Assignment 2 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test_model5](https://ilykei.com/api/fileProxy/documents%2FAdvanced%20Machine%20Learning%2FLecture%207%20AdvML%2Ftest_model5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile model\n",
    "\n",
    "Once the model architecture is finalized, configure its learning process using `compile()`, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `model` is the sequential model created above.  \n",
    "\n",
    "Compillation of the model requires declaration of the loss function (objective function) and the optimizer.\n",
    "\n",
    "Keras has a variety of [loss functions](https://keras.io/losses/) and [optimizers](https://keras.io/optimizers/) to choose from. \n",
    "\n",
    "An optional parameter `metrics` also can be set. Metric is a function (or list of functions) that is used to judge the performance of the model. \n",
    "\n",
    "Metric can be selected from the [list of available metrics](https://keras.io/metrics/) or passed as custom metric function name during the compilation step. <br>\n",
    "The function would need to take  `(y_true, y_pred)` as arguments and return a single tensor value.\n",
    "\n",
    "In the example below we use logarithmic loss as objective function, which for a binary classification problem is defined in Keras as `binary_crossentropy`. \n",
    "\n",
    "We can also use gradient descent algorithm `adam` and choose `accuracy` as metric for collectiing and reporting during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for example\n",
    "\n",
    "Example in this notebook use [**Breast cancer Wisconsin dataset**](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)). \n",
    "\n",
    "The breast cancer dataset is a well known data for binary classification. It was created by Dr. William H. Wolberg, physician at the University Of Wisconsin Hospital at Madison, Wisconsin, USA.\n",
    "\n",
    "Features are created from a digitized images of a [fine needle aspirate (FNA)](https://en.wikipedia.org/wiki/Fine-needle_aspiration), a biopsy method, of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
    "\n",
    "To create the dataset Dr. Wolberg used Ô¨Çuid samples, taken from patients with solid breast masses and graphical software [Xcyt](http://software.broadinstitute.org/mpg/xcyt/) capable of performing analysis of cytological features on a digital scan. \n",
    "\n",
    "Main research article on the subject of this analysis, including an image of malignant cells, is [Wisconsin Breast Cancer Dataset and Machine Learning for Breast Cancer Detection, by Lucas Rodrigues Borges](https://www.researchgate.net/publication/311950799_Analysis_of_the_Wisconsin_Breast_Cancer_Dataset_and_Machine_Learning_for_Breast_Cancer_Detection [accessed Nov 09 2017]).\n",
    "\n",
    "The data set has following characteristics:\n",
    "\n",
    "- Classes:\t2\n",
    "- Samples per class:\t212(malignant), 357(benign)\n",
    "- Samples total:\t569\n",
    "- Dimensionality:\t30\n",
    "- Features:\treal, positive\n",
    "\n",
    "Load the data using utility `load_breast_cancer` from `sklearn.datasets` module. <br>\n",
    "Print names of the features and dimensions of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "data (569, 30) labels (569,)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data  = load_breast_cancer()\n",
    "print(data.feature_names)\n",
    "print('data',data.data.shape,'labels',data.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model on training data\n",
    "\n",
    "Training or fitting the model on loaded data is done by calling method `fit` on the model. \n",
    "\n",
    "Split the breast cancer dataset into train and test subsets and train simple sequential architecture defined above with one hidden layer with 3 units and `relu` activation function, followed by one output unit with sigmoid activation.\n",
    "\n",
    "![Model plot](https://ilykei.com/api/fileProxy/documents%2FAdvanced%20Machine%20Learning%2FLecture%207%20AdvML%2FR_nn_plot.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target,\n",
    "                                                    test_size=0.2, random_state=1)\n",
    "def get_model_simple():\n",
    "    model = Sequential([Dense(3, input_dim=30),Activation('relu'),\n",
    "    Dense(1),Activation('sigmoid')])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and compiling model by a function instead of creating it directly allows experimenting and fitting with the model multiple times. <br>\n",
    "If model is fitted again it will use the previously fitted weights which makes fitting experiment incorrect. <br>\n",
    "In order to reinitialize weights call function generating the model.  \n",
    "\n",
    "Fit the model on train set using 10 epochs and iterating on the data in batches of 32 samples.\n",
    "\n",
    "- Parameter `epoch` is an arbitrary cutoff, typically means one pass over entire dataset, used to separate training process in distinct phases. <br>\n",
    "- Parameter `batch_size` is size of batch, a set of samples (observations) that is a subsample processed simultaneously. Processing of each batch results in one update of the network weights. <br>\n",
    "- Batch approximates distribution of weights better than single observation. The larger the batch the better the approximation. But larger batch processing requires more memory. \n",
    "- For evaluation or prediction it is recommended to use as large batch size as mempry allows since it makes the inference faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "455/455 [==============================] - 0s 1ms/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 2/30\n",
      "455/455 [==============================] - 0s 112us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 3/30\n",
      "455/455 [==============================] - 0s 182us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 4/30\n",
      "455/455 [==============================] - 0s 148us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 5/30\n",
      "455/455 [==============================] - 0s 152us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 6/30\n",
      "455/455 [==============================] - 0s 186us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 7/30\n",
      "455/455 [==============================] - 0s 179us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 8/30\n",
      "455/455 [==============================] - 0s 168us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 9/30\n",
      "455/455 [==============================] - 0s 139us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 10/30\n",
      "455/455 [==============================] - 0s 187us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 11/30\n",
      "455/455 [==============================] - 0s 352us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 12/30\n",
      "455/455 [==============================] - 0s 218us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 13/30\n",
      "455/455 [==============================] - 0s 126us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 14/30\n",
      "455/455 [==============================] - 0s 164us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 15/30\n",
      "455/455 [==============================] - 0s 187us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 16/30\n",
      "455/455 [==============================] - 0s 247us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 17/30\n",
      "455/455 [==============================] - 0s 210us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 18/30\n",
      "455/455 [==============================] - 0s 275us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 19/30\n",
      "455/455 [==============================] - 0s 205us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 20/30\n",
      "455/455 [==============================] - 0s 192us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 21/30\n",
      "455/455 [==============================] - 0s 237us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 22/30\n",
      "455/455 [==============================] - 0s 139us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 23/30\n",
      "455/455 [==============================] - 0s 194us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 24/30\n",
      "455/455 [==============================] - 0s 159us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 25/30\n",
      "455/455 [==============================] - 0s 101us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 26/30\n",
      "455/455 [==============================] - 0s 160us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 27/30\n",
      "455/455 [==============================] - 0s 327us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 28/30\n",
      "455/455 [==============================] - 0s 195us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 29/30\n",
      "455/455 [==============================] - 0s 215us/step - loss: 5.9565 - acc: 0.6264\n",
      "Epoch 30/30\n",
      "455/455 [==============================] - 0s 132us/step - loss: 5.9565 - acc: 0.6264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f738363d550>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model_simple()\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no progress in training process: loss stalled and accuracy does not go high enough (results may vary from run to run). <br>\n",
    "Scale the input data and fit the model again. <br>\n",
    "Note that first call of the scaler is `fit_transform`, but the second is just `transform`: test sample are scaled by the same amounts as train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "455/455 [==============================] - 0s 744us/step - loss: 0.6692 - acc: 0.6593\n",
      "Epoch 2/30\n",
      "455/455 [==============================] - 0s 80us/step - loss: 0.5906 - acc: 0.7912\n",
      "Epoch 3/30\n",
      "455/455 [==============================] - 0s 195us/step - loss: 0.5298 - acc: 0.8527\n",
      "Epoch 4/30\n",
      "455/455 [==============================] - 0s 157us/step - loss: 0.4813 - acc: 0.8769\n",
      "Epoch 5/30\n",
      "455/455 [==============================] - 0s 178us/step - loss: 0.4432 - acc: 0.8857\n",
      "Epoch 6/30\n",
      "455/455 [==============================] - 0s 212us/step - loss: 0.4101 - acc: 0.9011\n",
      "Epoch 7/30\n",
      "455/455 [==============================] - 0s 193us/step - loss: 0.3827 - acc: 0.9099\n",
      "Epoch 8/30\n",
      "455/455 [==============================] - 0s 145us/step - loss: 0.3589 - acc: 0.9143\n",
      "Epoch 9/30\n",
      "455/455 [==============================] - 0s 169us/step - loss: 0.3376 - acc: 0.9143\n",
      "Epoch 10/30\n",
      "455/455 [==============================] - 0s 298us/step - loss: 0.3173 - acc: 0.9187\n",
      "Epoch 11/30\n",
      "455/455 [==============================] - 0s 255us/step - loss: 0.3000 - acc: 0.9231\n",
      "Epoch 12/30\n",
      "455/455 [==============================] - 0s 189us/step - loss: 0.2846 - acc: 0.9231\n",
      "Epoch 13/30\n",
      "455/455 [==============================] - 0s 223us/step - loss: 0.2702 - acc: 0.9297\n",
      "Epoch 14/30\n",
      "455/455 [==============================] - 0s 338us/step - loss: 0.2575 - acc: 0.9319\n",
      "Epoch 15/30\n",
      "455/455 [==============================] - 0s 223us/step - loss: 0.2456 - acc: 0.9341\n",
      "Epoch 16/30\n",
      "455/455 [==============================] - 0s 294us/step - loss: 0.2354 - acc: 0.9407\n",
      "Epoch 17/30\n",
      "455/455 [==============================] - 0s 218us/step - loss: 0.2255 - acc: 0.9429\n",
      "Epoch 18/30\n",
      "455/455 [==============================] - 0s 225us/step - loss: 0.2159 - acc: 0.9451\n",
      "Epoch 19/30\n",
      "455/455 [==============================] - 0s 255us/step - loss: 0.2066 - acc: 0.9473\n",
      "Epoch 20/30\n",
      "455/455 [==============================] - 0s 215us/step - loss: 0.1984 - acc: 0.9516\n",
      "Epoch 21/30\n",
      "455/455 [==============================] - 0s 205us/step - loss: 0.1906 - acc: 0.9516\n",
      "Epoch 22/30\n",
      "455/455 [==============================] - 0s 195us/step - loss: 0.1832 - acc: 0.9516\n",
      "Epoch 23/30\n",
      "455/455 [==============================] - 0s 169us/step - loss: 0.1771 - acc: 0.9516\n",
      "Epoch 24/30\n",
      "455/455 [==============================] - 0s 185us/step - loss: 0.1713 - acc: 0.9538\n",
      "Epoch 25/30\n",
      "455/455 [==============================] - 0s 241us/step - loss: 0.1661 - acc: 0.9538\n",
      "Epoch 26/30\n",
      "455/455 [==============================] - 0s 252us/step - loss: 0.1610 - acc: 0.9560\n",
      "Epoch 27/30\n",
      "455/455 [==============================] - 0s 174us/step - loss: 0.1563 - acc: 0.9538\n",
      "Epoch 28/30\n",
      "455/455 [==============================] - 0s 221us/step - loss: 0.1520 - acc: 0.9538\n",
      "Epoch 29/30\n",
      "455/455 [==============================] - 0s 165us/step - loss: 0.1479 - acc: 0.9538\n",
      "Epoch 30/30\n",
      "455/455 [==============================] - 0s 127us/step - loss: 0.1441 - acc: 0.9538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model = get_model_simple()\n",
    "hist = model.fit(X_train, y_train, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scaling both logloss and accuracy measures have improved very significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method `fit` returns a History object. <br>\n",
    "Its `.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'acc'])\n",
      "best train loss 0.144133388325\n"
     ]
    }
   ],
   "source": [
    "print(hist.history.keys())\n",
    "print('best train loss', min(hist.history[\"loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now predict test labels using method *'predict'* and evaluate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1 = model.predict(X_test)\n",
    "pred1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method `evaluate` returns values of all metrics the model calculates. Their names are in the attribute `metrics_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "[0.17968957988839401, 0.96491228070175439]\n"
     ]
    }
   ],
   "source": [
    "print(model.metrics_names)\n",
    "print(model.evaluate(X_test, y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Probability of class 1')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot predictions and true labels\n",
    "# blue for class 0, orange for class 1\n",
    "import matplotlib.pyplot as plt\n",
    "color = ['blue' if y==0 else 'orange' for y in y_test]\n",
    "plt.scatter(range(len(pred1)), pred1, color=color)\n",
    "plt.title('Probability of class 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "    \n",
    "**Assignment 3** <br>\n",
    "Define sequential model with two hidden layers of 5 and 10 units followed by `relu` activation and one output unit with `sigmoid` activation similar to the architecture in [MScA_31009_ML_DeepLearning_TF_Intro_3.html](https://ilykei.com/api/fileProxy/documents%2FAdvanced%20Data%20Mining%20and%20Predictive%20Analytics%2031009%2FLecture%20Deep%20Learning%2FMScA_31009_ML_DeepLearning_TF_Intro_3.html).\n",
    "Create a function `get_model_1()` that adds layers and compiles the model with loss as `binary_crossentropy`, optimizer `adam` and metrics `accuracy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Model plot](https://ilykei.com/api/fileProxy/documents%2FAdvanced%20Machine%20Learning%2FLecture%207%20AdvML%2FWBCD_5_10_Architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "Fit the model using 30 epochs and batch size 32. <br>\n",
    "Print best train loss. <br>\n",
    "Evaluate the model on test data using both metrics. <br>\n",
    "Plot predictions and true labels. <br>\n",
    "Plot predictions and true labels.\n",
    "Enter code in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipped code\n",
    "# function get_model_1(), compilation and fitting\n",
    "# evaluate model on test data, print both metrics\n",
    "# dict_keys(['loss', 'acc'])\n",
    "# best train loss 0.0752962507881\n",
    "# ['loss', 'acc']\n",
    "# [0.11679617080249284, 0.97368421052631582]\n",
    "# Plot predictions and true labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization in Keras\n",
    "\n",
    "Keras has its own tool for data scaling: `BatchNormalization` layer. <br>\n",
    "It normalizes the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation (previous layer' output) close to 0 and the activation standard deviation close to 1. <br>\n",
    "Sergey Ioffe and Christian Szegedy show in [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) that Batch Normalization improves convergence speed  of deep neural networks.  \n",
    "\n",
    "In order to apply BatchNormalization to the `previous_layer` in Keras functional API you need two lines of code:\n",
    "\n",
    "`from keras.layers import BatchNormalization`  \n",
    "`scaled = BatchNormalization()(input1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell: \n",
    "\n",
    "1. Modify function `get_model_1` into `get_model_2` in functional API and insert BatchNormalization layer after `input1`. <br>\n",
    "2. Currently `X_train`, `X_test` are scaled. In order to restore the original train and test split the data again. \n",
    "3. Run fitting 10 times and average the results. \n",
    "4. Set verbose=0 in the fit() call to remove unnecessary logging. \n",
    "5. Print loss function on every iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.222527108874\n",
      "1 0.39269849717\n",
      "2 0.196402124193\n",
      "3 0.286492787875\n",
      "4 0.188922199869\n",
      "5 0.216375500815\n",
      "6 0.359025253961\n",
      "7 0.34932534806\n",
      "8 0.145450019362\n",
      "9 0.374132324051\n",
      "average loss =  0.273135116423\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import BatchNormalization\n",
    "def get_model_2():\n",
    "    input1 = Input(shape=(30,), name='input_1')\n",
    "    scaled = BatchNormalization()(input1)\n",
    "    hidden1 = Dense(5, activation='relu',name='hidden1')(scaled)\n",
    "    hidden2 = Dense(10, activation='relu',name='hidden2')(hidden1)\n",
    "    output = Dense(1,activation='sigmoid')(hidden2)\n",
    "    model = Model(inputs=input1, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                  metrics=['accuracy'])    \n",
    "    return model\n",
    "\n",
    "n = 10\n",
    "loss = np.zeros(n)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data,data.target,\n",
    "                                                    test_size=0.2)\n",
    "for i in np.arange(n):\n",
    "    model = get_model_2()\n",
    "    hist = model.fit(X_train, y_train, epochs=10, batch_size=32,verbose=0)\n",
    "    loss[i] = min(hist.history[\"loss\"])\n",
    "    print(i,loss[i])\n",
    "print('average loss = ',np.mean(loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "\n",
    "Keras has a set of functions that can be used to get a view on internal states and statistics of the model during training. They are called callbacks and `History` mentioned above is one of them.\n",
    "\n",
    "A list of callbacks can be passed (as the keyword argument `callbacks`) to the  `fit()` method of the Sequential or Model classes. The relevant methods of the callbacks will then be called at each stage of the training.  \n",
    "\n",
    "Here are some usefull callbacks that will be used below.\n",
    "\n",
    "- `CSVLogger`  \n",
    "    `keras.callbacks.CSVLogger(filename, separator=',', append=False)`  \n",
    "    Callback that streams epoch results to a csv file.  \n",
    "\n",
    "\n",
    "- `ModelCheckpoint`  \n",
    "    `keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)`  \n",
    "    By default save the model to the file `filepath` after every epoch. If `save_best_only=True` only the latest best model is saved according to the quantity monitored. So one can restart trainning from the last saved state in case of failure. This is important when training deep learning models, which can often be a time-consuming task.  \n",
    "\n",
    "Here is an example of how we can save the model and restore it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: saving model to weights.hdf5\n",
      "\n",
      "Epoch 00002: saving model to weights.hdf5\n",
      "\n",
      "Epoch 00003: saving model to weights.hdf5\n",
      "\n",
      "Epoch 00004: saving model to weights.hdf5\n",
      "\n",
      "Epoch 00005: saving model to weights.hdf5\n",
      "\n",
      "Epoch 00006: saving model to weights.hdf5\n",
      "\n",
      "Epoch 00007: saving model to weights.hdf5\n",
      "\n",
      "Epoch 00008: saving model to weights.hdf5\n",
      "\n",
      "Epoch 00009: saving model to weights.hdf5\n",
      "\n",
      "Epoch 00010: saving model to weights.hdf5\n",
      "evaluation results of trained model\n",
      "['loss', 'acc']\n",
      "[0.3836186240639603, 0.89473683792248104]\n",
      "evaluation results of reinitialized model\n",
      "[1.2871111840532536, 0.1666666667973786]\n",
      "evaluation results of model with weights loaded from file\n",
      "[0.3836186240639603, 0.89473683792248104]\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# initialize model\n",
    "model = get_model_simple()\n",
    "# saves the model weights after each epoch\n",
    "checkpointer = ModelCheckpoint('weights.hdf5', verbose=1)\n",
    "model.fit(X_train, y_train, epochs=10, verbose=0, callbacks=[checkpointer])\n",
    "print('evaluation results of trained model')\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate(X_test, y_test,verbose=0))\n",
    "# reinitialize model\n",
    "model = get_model_simple()\n",
    "print('evaluation results of reinitialized model')\n",
    "print(model.evaluate(X_test, y_test,verbose=0))\n",
    "model.load_weights('weights.hdf5')\n",
    "print('evaluation results of model with weights loaded from file')\n",
    "print(model.evaluate(X_test, y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by loading weights from file we restore saved model.\n",
    "\n",
    "- `EarlyStopping`  \n",
    "    `keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')`  \n",
    "    Stop training when a monitored quantity has stopped improving. Using it we can set large number of epochs but the process will be stopped if validation loss is not improved during `patience` epochs.  \n",
    "\n",
    "In the example below define `EarlyStopping` callback with `patience=5` and validation loss (`val_loss`) as monitored quantity. \n",
    "\n",
    "Validation loss can be calculated only if `validation_data` parameter in the `fit` method call is defined. <br>\n",
    "In this case `validation_data=(X_test, y_test)`. <br>\n",
    "Allow 200 epochs to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 455 samples, validate on 114 samples\n",
      "Epoch 1/200\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.7036 - acc: 0.6154 - val_loss: 0.6850 - val_acc: 0.6930\n",
      "Epoch 2/200\n",
      "455/455 [==============================] - 0s 187us/step - loss: 0.6475 - acc: 0.6989 - val_loss: 0.6470 - val_acc: 0.7456\n",
      "Epoch 3/200\n",
      "455/455 [==============================] - 0s 186us/step - loss: 0.5991 - acc: 0.7626 - val_loss: 0.6127 - val_acc: 0.7632\n",
      "Epoch 4/200\n",
      "455/455 [==============================] - 0s 177us/step - loss: 0.5563 - acc: 0.8044 - val_loss: 0.5813 - val_acc: 0.7807\n",
      "Epoch 5/200\n",
      "455/455 [==============================] - 0s 287us/step - loss: 0.5185 - acc: 0.8418 - val_loss: 0.5536 - val_acc: 0.8070\n",
      "Epoch 6/200\n",
      "455/455 [==============================] - 0s 195us/step - loss: 0.4860 - acc: 0.8571 - val_loss: 0.5275 - val_acc: 0.8070\n",
      "Epoch 7/200\n",
      "455/455 [==============================] - 0s 146us/step - loss: 0.4569 - acc: 0.8725 - val_loss: 0.5023 - val_acc: 0.8246\n",
      "Epoch 8/200\n",
      "455/455 [==============================] - 0s 214us/step - loss: 0.4296 - acc: 0.8879 - val_loss: 0.4770 - val_acc: 0.8421\n",
      "Epoch 9/200\n",
      "455/455 [==============================] - 0s 307us/step - loss: 0.4030 - acc: 0.8989 - val_loss: 0.4534 - val_acc: 0.8421\n",
      "Epoch 10/200\n",
      "455/455 [==============================] - 0s 354us/step - loss: 0.3784 - acc: 0.9121 - val_loss: 0.4300 - val_acc: 0.8421\n",
      "Epoch 11/200\n",
      "455/455 [==============================] - 0s 345us/step - loss: 0.3556 - acc: 0.9165 - val_loss: 0.4073 - val_acc: 0.8772\n",
      "Epoch 12/200\n",
      "455/455 [==============================] - 0s 204us/step - loss: 0.3332 - acc: 0.9275 - val_loss: 0.3856 - val_acc: 0.8772\n",
      "Epoch 13/200\n",
      "455/455 [==============================] - 0s 381us/step - loss: 0.3132 - acc: 0.9275 - val_loss: 0.3665 - val_acc: 0.8860\n",
      "Epoch 14/200\n",
      "455/455 [==============================] - 0s 429us/step - loss: 0.2957 - acc: 0.9319 - val_loss: 0.3488 - val_acc: 0.9035\n",
      "Epoch 15/200\n",
      "455/455 [==============================] - 0s 216us/step - loss: 0.2792 - acc: 0.9385 - val_loss: 0.3329 - val_acc: 0.9035\n",
      "Epoch 16/200\n",
      "455/455 [==============================] - 0s 182us/step - loss: 0.2648 - acc: 0.9385 - val_loss: 0.3172 - val_acc: 0.9123\n",
      "Epoch 17/200\n",
      "455/455 [==============================] - 0s 220us/step - loss: 0.2510 - acc: 0.9385 - val_loss: 0.3041 - val_acc: 0.9123\n",
      "Epoch 18/200\n",
      "455/455 [==============================] - 0s 325us/step - loss: 0.2390 - acc: 0.9385 - val_loss: 0.2918 - val_acc: 0.9211\n",
      "Epoch 19/200\n",
      "455/455 [==============================] - 0s 151us/step - loss: 0.2275 - acc: 0.9407 - val_loss: 0.2793 - val_acc: 0.9211\n",
      "Epoch 20/200\n",
      "455/455 [==============================] - 0s 189us/step - loss: 0.2164 - acc: 0.9407 - val_loss: 0.2677 - val_acc: 0.9211\n",
      "Epoch 21/200\n",
      "455/455 [==============================] - 0s 220us/step - loss: 0.2063 - acc: 0.9451 - val_loss: 0.2566 - val_acc: 0.9211\n",
      "Epoch 22/200\n",
      "455/455 [==============================] - 0s 137us/step - loss: 0.1970 - acc: 0.9473 - val_loss: 0.2459 - val_acc: 0.9298\n",
      "Epoch 23/200\n",
      "455/455 [==============================] - 0s 248us/step - loss: 0.1879 - acc: 0.9495 - val_loss: 0.2358 - val_acc: 0.9386\n",
      "Epoch 24/200\n",
      "455/455 [==============================] - 0s 286us/step - loss: 0.1795 - acc: 0.9516 - val_loss: 0.2259 - val_acc: 0.9386\n",
      "Epoch 25/200\n",
      "455/455 [==============================] - 0s 198us/step - loss: 0.1719 - acc: 0.9538 - val_loss: 0.2156 - val_acc: 0.9386\n",
      "Epoch 26/200\n",
      "455/455 [==============================] - 0s 170us/step - loss: 0.1649 - acc: 0.9560 - val_loss: 0.2070 - val_acc: 0.9386\n",
      "Epoch 27/200\n",
      "455/455 [==============================] - 0s 364us/step - loss: 0.1585 - acc: 0.9582 - val_loss: 0.1997 - val_acc: 0.9386\n",
      "Epoch 28/200\n",
      "455/455 [==============================] - 0s 301us/step - loss: 0.1530 - acc: 0.9604 - val_loss: 0.1930 - val_acc: 0.9386\n",
      "Epoch 29/200\n",
      "455/455 [==============================] - 0s 326us/step - loss: 0.1479 - acc: 0.9604 - val_loss: 0.1864 - val_acc: 0.9474\n",
      "Epoch 30/200\n",
      "455/455 [==============================] - 0s 237us/step - loss: 0.1429 - acc: 0.9626 - val_loss: 0.1802 - val_acc: 0.9561\n",
      "Epoch 31/200\n",
      "455/455 [==============================] - 0s 165us/step - loss: 0.1384 - acc: 0.9648 - val_loss: 0.1742 - val_acc: 0.9649\n",
      "Epoch 32/200\n",
      "455/455 [==============================] - 0s 172us/step - loss: 0.1341 - acc: 0.9648 - val_loss: 0.1690 - val_acc: 0.9737\n",
      "Epoch 33/200\n",
      "455/455 [==============================] - 0s 229us/step - loss: 0.1302 - acc: 0.9648 - val_loss: 0.1640 - val_acc: 0.9737\n",
      "Epoch 34/200\n",
      "455/455 [==============================] - 0s 142us/step - loss: 0.1267 - acc: 0.9692 - val_loss: 0.1596 - val_acc: 0.9737\n",
      "Epoch 35/200\n",
      "455/455 [==============================] - 0s 212us/step - loss: 0.1233 - acc: 0.9692 - val_loss: 0.1552 - val_acc: 0.9737\n",
      "Epoch 36/200\n",
      "455/455 [==============================] - 0s 185us/step - loss: 0.1201 - acc: 0.9692 - val_loss: 0.1512 - val_acc: 0.9737\n",
      "Epoch 37/200\n",
      "455/455 [==============================] - 0s 243us/step - loss: 0.1170 - acc: 0.9714 - val_loss: 0.1475 - val_acc: 0.9737\n",
      "Epoch 38/200\n",
      "455/455 [==============================] - 0s 125us/step - loss: 0.1138 - acc: 0.9736 - val_loss: 0.1435 - val_acc: 0.9737\n",
      "Epoch 39/200\n",
      "455/455 [==============================] - 0s 154us/step - loss: 0.1109 - acc: 0.9736 - val_loss: 0.1400 - val_acc: 0.9737\n",
      "Epoch 40/200\n",
      "455/455 [==============================] - 0s 173us/step - loss: 0.1083 - acc: 0.9736 - val_loss: 0.1358 - val_acc: 0.9737\n",
      "Epoch 41/200\n",
      "455/455 [==============================] - 0s 128us/step - loss: 0.1056 - acc: 0.9758 - val_loss: 0.1330 - val_acc: 0.9737\n",
      "Epoch 42/200\n",
      "455/455 [==============================] - 0s 139us/step - loss: 0.1034 - acc: 0.9758 - val_loss: 0.1307 - val_acc: 0.9737\n",
      "Epoch 43/200\n",
      "455/455 [==============================] - 0s 204us/step - loss: 0.1013 - acc: 0.9758 - val_loss: 0.1281 - val_acc: 0.9737\n",
      "Epoch 44/200\n",
      "455/455 [==============================] - 0s 214us/step - loss: 0.0995 - acc: 0.9758 - val_loss: 0.1256 - val_acc: 0.9737\n",
      "Epoch 45/200\n",
      "455/455 [==============================] - 0s 251us/step - loss: 0.0976 - acc: 0.9758 - val_loss: 0.1234 - val_acc: 0.9737\n",
      "Epoch 46/200\n",
      "455/455 [==============================] - 0s 248us/step - loss: 0.0959 - acc: 0.9758 - val_loss: 0.1215 - val_acc: 0.9737\n",
      "Epoch 47/200\n",
      "455/455 [==============================] - 0s 245us/step - loss: 0.0943 - acc: 0.9758 - val_loss: 0.1198 - val_acc: 0.9737\n",
      "Epoch 48/200\n",
      "455/455 [==============================] - 0s 258us/step - loss: 0.0927 - acc: 0.9758 - val_loss: 0.1180 - val_acc: 0.9737\n",
      "Epoch 49/200\n",
      "455/455 [==============================] - 0s 242us/step - loss: 0.0913 - acc: 0.9758 - val_loss: 0.1160 - val_acc: 0.9737\n",
      "Epoch 50/200\n",
      "455/455 [==============================] - 0s 378us/step - loss: 0.0899 - acc: 0.9758 - val_loss: 0.1146 - val_acc: 0.9737\n",
      "Epoch 51/200\n",
      "455/455 [==============================] - 0s 266us/step - loss: 0.0885 - acc: 0.9758 - val_loss: 0.1131 - val_acc: 0.9737\n",
      "Epoch 52/200\n",
      "455/455 [==============================] - 0s 177us/step - loss: 0.0872 - acc: 0.9758 - val_loss: 0.1120 - val_acc: 0.9737\n",
      "Epoch 53/200\n",
      "455/455 [==============================] - 0s 191us/step - loss: 0.0860 - acc: 0.9758 - val_loss: 0.1108 - val_acc: 0.9737\n",
      "Epoch 54/200\n",
      "455/455 [==============================] - 0s 242us/step - loss: 0.0849 - acc: 0.9758 - val_loss: 0.1099 - val_acc: 0.9737\n",
      "Epoch 55/200\n",
      "455/455 [==============================] - 0s 169us/step - loss: 0.0837 - acc: 0.9758 - val_loss: 0.1083 - val_acc: 0.9737\n",
      "Epoch 56/200\n",
      "455/455 [==============================] - 0s 258us/step - loss: 0.0825 - acc: 0.9780 - val_loss: 0.1076 - val_acc: 0.9737\n",
      "Epoch 57/200\n",
      "455/455 [==============================] - 0s 252us/step - loss: 0.0815 - acc: 0.9780 - val_loss: 0.1063 - val_acc: 0.9737\n",
      "Epoch 58/200\n",
      "455/455 [==============================] - 0s 183us/step - loss: 0.0805 - acc: 0.9780 - val_loss: 0.1060 - val_acc: 0.9737\n",
      "Epoch 59/200\n",
      "455/455 [==============================] - 0s 273us/step - loss: 0.0794 - acc: 0.9780 - val_loss: 0.1051 - val_acc: 0.9737\n",
      "Epoch 60/200\n",
      "455/455 [==============================] - 0s 210us/step - loss: 0.0784 - acc: 0.9780 - val_loss: 0.1036 - val_acc: 0.9737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "455/455 [==============================] - 0s 260us/step - loss: 0.0774 - acc: 0.9802 - val_loss: 0.1025 - val_acc: 0.9737\n",
      "Epoch 62/200\n",
      "455/455 [==============================] - 0s 267us/step - loss: 0.0764 - acc: 0.9802 - val_loss: 0.1012 - val_acc: 0.9737\n",
      "Epoch 63/200\n",
      "455/455 [==============================] - 0s 378us/step - loss: 0.0756 - acc: 0.9802 - val_loss: 0.1003 - val_acc: 0.9737\n",
      "Epoch 64/200\n",
      "455/455 [==============================] - 0s 317us/step - loss: 0.0748 - acc: 0.9802 - val_loss: 0.0996 - val_acc: 0.9737\n",
      "Epoch 65/200\n",
      "455/455 [==============================] - 0s 298us/step - loss: 0.0738 - acc: 0.9802 - val_loss: 0.0986 - val_acc: 0.9737\n",
      "Epoch 66/200\n",
      "455/455 [==============================] - 0s 325us/step - loss: 0.0730 - acc: 0.9802 - val_loss: 0.0977 - val_acc: 0.9737\n",
      "Epoch 67/200\n",
      "455/455 [==============================] - 0s 374us/step - loss: 0.0722 - acc: 0.9802 - val_loss: 0.0969 - val_acc: 0.9737\n",
      "Epoch 68/200\n",
      "455/455 [==============================] - 0s 184us/step - loss: 0.0715 - acc: 0.9824 - val_loss: 0.0961 - val_acc: 0.9737\n",
      "Epoch 69/200\n",
      "455/455 [==============================] - 0s 135us/step - loss: 0.0707 - acc: 0.9824 - val_loss: 0.0956 - val_acc: 0.9737\n",
      "Epoch 70/200\n",
      "455/455 [==============================] - 0s 243us/step - loss: 0.0699 - acc: 0.9824 - val_loss: 0.0944 - val_acc: 0.9737\n",
      "Epoch 71/200\n",
      "455/455 [==============================] - 0s 456us/step - loss: 0.0690 - acc: 0.9824 - val_loss: 0.0940 - val_acc: 0.9737\n",
      "Epoch 72/200\n",
      "455/455 [==============================] - 0s 267us/step - loss: 0.0684 - acc: 0.9824 - val_loss: 0.0931 - val_acc: 0.9737\n",
      "Epoch 73/200\n",
      "455/455 [==============================] - 0s 152us/step - loss: 0.0675 - acc: 0.9824 - val_loss: 0.0926 - val_acc: 0.9737\n",
      "Epoch 74/200\n",
      "455/455 [==============================] - 0s 198us/step - loss: 0.0668 - acc: 0.9824 - val_loss: 0.0915 - val_acc: 0.9737\n",
      "Epoch 75/200\n",
      "455/455 [==============================] - 0s 171us/step - loss: 0.0660 - acc: 0.9824 - val_loss: 0.0909 - val_acc: 0.9737\n",
      "Epoch 76/200\n",
      "455/455 [==============================] - 0s 107us/step - loss: 0.0654 - acc: 0.9824 - val_loss: 0.0902 - val_acc: 0.9737\n",
      "Epoch 77/200\n",
      "455/455 [==============================] - 0s 144us/step - loss: 0.0647 - acc: 0.9824 - val_loss: 0.0896 - val_acc: 0.9737\n",
      "Epoch 78/200\n",
      "455/455 [==============================] - 0s 188us/step - loss: 0.0640 - acc: 0.9824 - val_loss: 0.0896 - val_acc: 0.9737\n",
      "Epoch 79/200\n",
      "455/455 [==============================] - 0s 172us/step - loss: 0.0635 - acc: 0.9824 - val_loss: 0.0894 - val_acc: 0.9737\n",
      "Epoch 80/200\n",
      "455/455 [==============================] - 0s 169us/step - loss: 0.0629 - acc: 0.9824 - val_loss: 0.0886 - val_acc: 0.9737\n",
      "Epoch 81/200\n",
      "455/455 [==============================] - 0s 187us/step - loss: 0.0623 - acc: 0.9824 - val_loss: 0.0882 - val_acc: 0.9737\n",
      "Epoch 82/200\n",
      "455/455 [==============================] - 0s 143us/step - loss: 0.0618 - acc: 0.9824 - val_loss: 0.0879 - val_acc: 0.9737\n",
      "Epoch 83/200\n",
      "455/455 [==============================] - 0s 156us/step - loss: 0.0613 - acc: 0.9824 - val_loss: 0.0879 - val_acc: 0.9737\n",
      "Epoch 84/200\n",
      "455/455 [==============================] - 0s 106us/step - loss: 0.0609 - acc: 0.9824 - val_loss: 0.0872 - val_acc: 0.9737\n",
      "Epoch 85/200\n",
      "455/455 [==============================] - 0s 133us/step - loss: 0.0604 - acc: 0.9846 - val_loss: 0.0867 - val_acc: 0.9737\n",
      "Epoch 86/200\n",
      "455/455 [==============================] - 0s 198us/step - loss: 0.0599 - acc: 0.9846 - val_loss: 0.0865 - val_acc: 0.9737\n",
      "Epoch 87/200\n",
      "455/455 [==============================] - 0s 181us/step - loss: 0.0594 - acc: 0.9846 - val_loss: 0.0862 - val_acc: 0.9737\n",
      "Epoch 88/200\n",
      "455/455 [==============================] - 0s 248us/step - loss: 0.0590 - acc: 0.9846 - val_loss: 0.0859 - val_acc: 0.9737\n",
      "Epoch 89/200\n",
      "455/455 [==============================] - 0s 133us/step - loss: 0.0586 - acc: 0.9846 - val_loss: 0.0857 - val_acc: 0.9737\n",
      "Epoch 90/200\n",
      "455/455 [==============================] - 0s 139us/step - loss: 0.0582 - acc: 0.9846 - val_loss: 0.0852 - val_acc: 0.9737\n",
      "Epoch 91/200\n",
      "455/455 [==============================] - 0s 185us/step - loss: 0.0578 - acc: 0.9868 - val_loss: 0.0853 - val_acc: 0.9737\n",
      "Epoch 92/200\n",
      "455/455 [==============================] - 0s 209us/step - loss: 0.0573 - acc: 0.9868 - val_loss: 0.0853 - val_acc: 0.9737\n",
      "Epoch 93/200\n",
      "455/455 [==============================] - 0s 376us/step - loss: 0.0569 - acc: 0.9846 - val_loss: 0.0852 - val_acc: 0.9737\n",
      "Epoch 94/200\n",
      "455/455 [==============================] - 0s 162us/step - loss: 0.0567 - acc: 0.9846 - val_loss: 0.0842 - val_acc: 0.9737\n",
      "Epoch 95/200\n",
      "455/455 [==============================] - 0s 131us/step - loss: 0.0562 - acc: 0.9846 - val_loss: 0.0839 - val_acc: 0.9737\n",
      "Epoch 96/200\n",
      "455/455 [==============================] - 0s 157us/step - loss: 0.0560 - acc: 0.9846 - val_loss: 0.0845 - val_acc: 0.9737\n",
      "Epoch 97/200\n",
      "455/455 [==============================] - 0s 167us/step - loss: 0.0556 - acc: 0.9846 - val_loss: 0.0844 - val_acc: 0.9737\n",
      "Epoch 98/200\n",
      "455/455 [==============================] - 0s 225us/step - loss: 0.0553 - acc: 0.9846 - val_loss: 0.0842 - val_acc: 0.9737\n",
      "Epoch 99/200\n",
      "455/455 [==============================] - 0s 231us/step - loss: 0.0550 - acc: 0.9868 - val_loss: 0.0851 - val_acc: 0.9737\n",
      "Epoch 100/200\n",
      "455/455 [==============================] - 0s 209us/step - loss: 0.0547 - acc: 0.9846 - val_loss: 0.0837 - val_acc: 0.9737\n",
      "Epoch 101/200\n",
      "455/455 [==============================] - 0s 100us/step - loss: 0.0545 - acc: 0.9846 - val_loss: 0.0829 - val_acc: 0.9737\n",
      "Epoch 102/200\n",
      "455/455 [==============================] - 0s 120us/step - loss: 0.0542 - acc: 0.9846 - val_loss: 0.0829 - val_acc: 0.9737\n",
      "Epoch 103/200\n",
      "455/455 [==============================] - 0s 203us/step - loss: 0.0538 - acc: 0.9846 - val_loss: 0.0825 - val_acc: 0.9737\n",
      "Epoch 104/200\n",
      "455/455 [==============================] - 0s 209us/step - loss: 0.0536 - acc: 0.9846 - val_loss: 0.0822 - val_acc: 0.9737\n",
      "Epoch 105/200\n",
      "455/455 [==============================] - 0s 191us/step - loss: 0.0533 - acc: 0.9846 - val_loss: 0.0820 - val_acc: 0.9737\n",
      "Epoch 106/200\n",
      "455/455 [==============================] - 0s 145us/step - loss: 0.0531 - acc: 0.9846 - val_loss: 0.0827 - val_acc: 0.9737\n",
      "Epoch 107/200\n",
      "455/455 [==============================] - 0s 127us/step - loss: 0.0528 - acc: 0.9846 - val_loss: 0.0823 - val_acc: 0.9737\n",
      "Epoch 108/200\n",
      "455/455 [==============================] - 0s 274us/step - loss: 0.0525 - acc: 0.9846 - val_loss: 0.0823 - val_acc: 0.9737\n",
      "Epoch 109/200\n",
      "455/455 [==============================] - 0s 279us/step - loss: 0.0522 - acc: 0.9846 - val_loss: 0.0820 - val_acc: 0.9737\n",
      "Epoch 110/200\n",
      "455/455 [==============================] - 0s 277us/step - loss: 0.0520 - acc: 0.9846 - val_loss: 0.0822 - val_acc: 0.9737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f736cc17400>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaler.transform(data.data), data.target,\n",
    "                                                    test_size=0.2, random_state=1)\n",
    "# initialize model\n",
    "model = get_model_simple()\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200,\n",
    "          callbacks=[early_stopping], verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EarlyStopping callback terminated process earlier, but the model now is not the best one (according to `val_loss` criterion). The best model was achieved (`patience`+1) epochs before the end.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "One of the major issues with artificial neural networks is that the models are quite complicated. For example, let us consider a neural network that is pulling data from an image from the MNIST database (28 by 28 pixels), feeds into two hidden layers with 30 neurons, and finally reaches a soft-max layer of 10 neurons. The total number of parameters in the network is nearly 25,000. Aparently the model with so many parameters is prone to overfititng.\n",
    "\n",
    "Keras has a number of layers that can help prevent overfitting.  \n",
    "\n",
    "### Dropout\n",
    "\n",
    "Dropout is a regularization technique for neural network models proposed by Srivastava, et al. in their 2014 paper [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf).\n",
    "\n",
    "Dropout is a technique where randomly selected neurons are ignored during training. <br>\n",
    "They are ‚Äúdropped-out‚Äù randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\n",
    "\n",
    "Dropout layer can be created as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "dropout_layer = Dropout(rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `rate` is float number between 0 and 1 meaning fraction of the input units to drop.\n",
    "\n",
    "Keras provides a number of Noise layers that are also useful to mitigate overfitting.  \n",
    "\n",
    "### GaussianNoise\n",
    "\n",
    "Layer `GaussianNoise(stddev)` applies additive zero-centered Gaussian noise with standard deviation `stdev`. <br>\n",
    "It could be treated as a form of random data augmentation. Since it is a regularization layer, it is only active at training time.\n",
    "\n",
    "### GaussianDropout\n",
    "\n",
    "Layer `GaussianDropout(rate)` applies multiplicative 1-centered Gaussian noise with standard deviation \n",
    "$$\\sqrt{\\frac{rate} { (1 - rate)}}.$$ \n",
    "It is only active at training time.\n",
    "\n",
    "The way of choosing appropriate dropout rate and some other parameters will be shown in TuningNetworks notebook."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
