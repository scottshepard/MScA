{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iLykei Lecture Series\n",
    "\n",
    "# Advanced Machine Learning and Artificial Intelligence (MScA 32017)\n",
    "\n",
    "# Project:  Generative Models\n",
    "\n",
    "# Topic: Conditional Generative Adversarial Networks (CGANs)\n",
    "\n",
    "## Notebook 1: Training Conditional GAN (CGAN)\n",
    "\n",
    "\n",
    "## Yuri Balasanov, &copy; iLykei 2019\n",
    "\n",
    "##### Main texts: \n",
    "\n",
    "**[Generative Adversarial Networks, Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, arXiv:1406.2661 [stat.ML]](https://arxiv.org/abs/1406.2661)**\n",
    "\n",
    "**[Conditional Generative Adversarial Nets, Mehdi Mirza, Simon Osindero, arXiv:1411.1784 [cs.LG]](https://arxiv.org/abs/1411.1784)**\n",
    "\n",
    "**[How to Develop a Conditional GAN (cGAN) From Scratch, Jason Brownlee, 2019](https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/)**    \n",
    "\n",
    "This notebook shows how to construct a conditional CGAN and train it on Fashion MNIST data.\n",
    "\n",
    "Code in this notebook is based on [this blog by Jason Brownlee](https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove when run on GPU\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data. Conditional GAN requires the training set of images, like unconditional GAN, and in addition training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.datasets import mnist, fashion_mnist\n",
    "# load MNIST or FASHION_MNIST dataset\n",
    "#(x_train, y_train), (_, _) = mnist.load_data()\n",
    "#(x_train, y_train), (_, _) = fashion_mnist.load_data()\n",
    "\n",
    "#np.save('x_train.npy',x_train)\n",
    "#np.save('y_train.npy',y_train)\n",
    "#np.save('x_train_fashion.npy',x_train)\n",
    "#np.save('x_train_fashion.npy',y_train)\n",
    "#x_train=np.load('x_train.npy')\n",
    "#y_train=np.load('y_train.npy')\n",
    "x_train=np.load('x_train_fashion.npy')\n",
    "y_train=np.load('y_train_fashion.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "\n",
    "Create function building discriminator model:\n",
    "\n",
    "- First input is label of one class used as condition. For example, 0 is the T-shirt class, this means that the generator generates variations of T-shirts. This label is one number from 0 to 9. This number is passed through embedding layer with output dimension 50 and then through dense layer with 784 units followed by reshaping into (28,28,1), same as input image\n",
    "- Second input is an image coming from either sample (\"real\" image) or from generator (\"fake\" image)\n",
    "- Transformed first input and second input get concatenated into shape equivalent to a 2-chanel image: (28,28,2)\n",
    "- Apply 2 consecutive 2d-convolutions, each with 128 filters and LeakyReLU activation with reduction of pixel size from (28,28) - to (14,14) - to (7,7)\n",
    "- Shape (7,7,128) flattened and regularized with dropout\n",
    "- Output layer is 1 unit with sigmoid activation for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(in_shape=(28,28,1), n_classes=10):\n",
    "        # label input\n",
    "    in_label = Input(shape=(1,))\n",
    "    # embedding for categorical input\n",
    "    li = Embedding(n_classes, 50)(in_label)\n",
    "    # scale up to image dimensions with linear activation\n",
    "    n_nodes = in_shape[0] * in_shape[1]\n",
    "    li = Dense(n_nodes)(li)\n",
    "    # reshape to additional channel\n",
    "    li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
    "    # image input\n",
    "    in_image = Input(shape=in_shape)\n",
    "    # concat label as a channel\n",
    "    merge = Concatenate()([in_image, li])\n",
    "    # downsample\n",
    "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(merge)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    # downsample\n",
    "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    # flatten feature maps\n",
    "    fe = Flatten()(fe)\n",
    "    # dropout\n",
    "    fe = Dropout(0.4)(fe)\n",
    "    # output\n",
    "    out_layer = Dense(1, activation='sigmoid')(fe)\n",
    "    # define model\n",
    "    model = Model([in_image, in_label], out_layer)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0721 14:24:34.803095 140534253528896 deprecation_wrapper.py:119] From /home/yuri/anaconda3/envs/newtf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0721 14:24:34.821180 140534253528896 deprecation_wrapper.py:119] From /home/yuri/anaconda3/envs/newtf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0721 14:24:34.825179 140534253528896 deprecation_wrapper.py:119] From /home/yuri/anaconda3/envs/newtf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0721 14:24:34.922662 140534253528896 deprecation_wrapper.py:119] From /home/yuri/anaconda3/envs/newtf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0721 14:24:34.987304 140534253528896 deprecation.py:506] From /home/yuri/anaconda3/envs/newtf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0721 14:24:35.040413 140534253528896 deprecation_wrapper.py:119] From /home/yuri/anaconda3/envs/newtf/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0721 14:24:35.050473 140534253528896 deprecation_wrapper.py:119] From /home/yuri/anaconda3/envs/newtf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0721 14:24:35.056250 140534253528896 deprecation.py:323] From /home/yuri/anaconda3/envs/newtf/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 50)        500         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 784)       39984       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 28, 28, 1)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 28, 28, 2)    0           input_2[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 14, 14, 128)  2432        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 14, 14, 128)  0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 7, 7, 128)    147584      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 7, 7, 128)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 6272)         0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 6272)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            6273        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 196,773\n",
      "Trainable params: 196,773\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Remove when run on GPU\n",
    "discriminator_structure = define_discriminator()\n",
    "plot_model(discriminator_structure, to_file='cgan_discriminator_structure.png',show_shapes=True,show_layer_names=True)\n",
    "discriminator_structure.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator has 196,773 trainable parameters.  \n",
    "\n",
    "Plot the architecture of the discriminator.\n",
    "\n",
    "![CGAN Discriminator](cgan_discriminator_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "Create function building generator of conditional GAN.\n",
    "\n",
    "- First input. Prepare the conditioning class information in a similar way to the discriminator: a single class input number between 0 and 9 is embedded into a 50-number vector, passed through a dense 49-unit layer and reshaped into (7,7,1) image-like input\n",
    "- Second input. Take a `latent_dim`-dimensional vector of random seed, pass it through a 6272-units dense layer and reshape it into (7,7,128), i.e. 128 feature maps, (7,7) each\n",
    "- Concatenate both inputs into (7,7,129) feature maps set\n",
    "- Apply sequence of 2 `Conv2DTranspose` layers with `LeakyReLU` activation increasing image size from (7,7) - to (14,14) - to (28,28)\n",
    "- Make output as (28,28,1) with `tanh` activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_dim, n_classes=10):\n",
    "    # label input\n",
    "    in_label = Input(shape=(1,))\n",
    "    # embedding for categorical input\n",
    "    li = Embedding(n_classes, 50)(in_label)\n",
    "    # linear multiplication\n",
    "    n_nodes = 7 * 7\n",
    "\n",
    "    li = Dense(n_nodes)(li)\n",
    "    # reshape to additional channel\n",
    "    li = Reshape((7, 7, 1))(li)\n",
    "    # image generator input\n",
    "    in_lat = Input(shape=(latent_dim,))\n",
    "    # foundation for 7x7 image\n",
    "    n_nodes = 128 * 7 * 7\n",
    "    gen = Dense(n_nodes)(in_lat)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    gen = Reshape((7, 7, 128))(gen)\n",
    "    # merge image gen and label input\n",
    "    merge = Concatenate()([gen, li])\n",
    "    # upsample to 14x14\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    # upsample to 28x28\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    # output\n",
    "    out_layer = Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n",
    "    # define model\n",
    "    model = Model([in_lat, in_label], out_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 6272)         633472      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 50)        500         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 6272)         0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 49)        2499        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 7, 7, 128)    0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 7, 7, 1)      0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 7, 7, 129)    0           reshape_3[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 14, 14, 128)  264320      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 14, 14, 128)  0           conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 28, 28, 128)  262272      leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 28, 28, 128)  0           conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 28, 28, 1)    6273        leaky_re_lu_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,169,336\n",
      "Trainable params: 1,169,336\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Remove when running on GPU\n",
    "generator_structure = define_generator(latent_dim=100)\n",
    "plot_model(generator_structure, to_file='cgan_generator_structure.png',show_shapes=True,show_layer_names=True)\n",
    "generator_structure.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator has 1,169,336 parameters, all trainable.\n",
    "\n",
    "![CGAN Model](cgan_generator_structure.png)\n",
    "\n",
    "## Adversarial model\n",
    "\n",
    "Combine the generator and discriminator models into a GAN with following steps.\n",
    "\n",
    "- Make the discriminator weights in the adversarial model untrainable\n",
    "- Make input to the CGAN like input to the generator, i.e. combination of random input noise and conditioning input label tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'input_4:0' shape=(?, 100) dtype=float32>,\n",
       " <tf.Tensor 'input_3:0' shape=(?, 1) dtype=float32>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove when running on GPU\n",
    "generator_structure.input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make input tensor to the discriminator from the output tensor of the generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor of generator:\n",
      " Tensor(\"conv2d_3/Tanh:0\", shape=(?, ?, ?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Remove when running on GPU\n",
    "print('Output tensor of generator:\\n',generator_structure.output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make adversarial model output tensor from the discriminator output tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output of discriminator as output of CGAN tensor:\n",
      " Tensor(\"model_1/dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Remove when running on GPU\n",
    "print('\\nOutput of discriminator as output of CGAN tensor:\\n',\n",
    "     discriminator_structure([generator_structure.output, generator_structure.input[1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the adversarial model from the generator's input tensors and discriminator's output tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # get noise and label inputs from generator model\n",
    "    gen_noise, gen_label = g_model.input\n",
    "    # get image output from the generator model\n",
    "    gen_output = g_model.output\n",
    "    # connect image output and label input from generator as inputs to discriminator\n",
    "    gan_output = d_model([gen_output, gen_label])\n",
    "    # define gan model as taking noise and label and outputting a classification\n",
    "    model = Model([gen_noise, gen_label], gan_output)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 6272)         633472      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 50)        500         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 6272)         0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 49)        2499        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 7, 7, 128)    0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 7, 7, 1)      0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 7, 7, 129)    0           reshape_3[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 14, 14, 128)  264320      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 14, 14, 128)  0           conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 28, 28, 128)  262272      leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 28, 28, 128)  0           conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 28, 28, 1)    6273        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 1)            196773      conv2d_3[0][0]                   \n",
      "                                                                 input_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,366,109\n",
      "Trainable params: 1,169,336\n",
      "Non-trainable params: 196,773\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Remove when running on GPU\n",
    "gan_structure = define_gan(generator_structure,discriminator_structure)\n",
    "plot_model(gan_structure, to_file='cgan_gan_structure.png',show_shapes=True,show_layer_names=True)\n",
    "gan_structure.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has total 1,366,109 parameters, of them 1,169,336 parameters trainable (generator) and 196,773 parameters non-trainable (discriminator).\n",
    "\n",
    "Plot the architecture of the CGAN.\n",
    "\n",
    "![CGAN_Structure](cgan_gan_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the CGAN model\n",
    "\n",
    "Create function preparing the data for the model: expand dimension of images from (28,28) to (28,28,1) and scale image arrays as real numbers in [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_samples():\n",
    "    # expand\n",
    "    X = np.expand_dims(x_train, axis=-1)\n",
    "    X = X.astype('float32')\n",
    "    # scale\n",
    "    X = (X - 127.5) / 127.5\n",
    "    return [X, y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function selecting real images from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "    # split into images and labels\n",
    "    images, labels = dataset\n",
    "    # create random index of selected images\n",
    "    ix = randint(0, images.shape[0], n_samples)\n",
    "    # select images and labels\n",
    "    X, labels = images[ix], labels[ix]\n",
    "    # label images as as 1 (real)\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return [X, labels], y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function generating `n_samples` random latent vectors of dimension `latent_dim` from standard normal distribution. These vectors are inputs to the generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples, n_classes=10):\n",
    "    # generate Gaussian latent vectors\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    z_input = x_input.reshape(n_samples, latent_dim)\n",
    "    # generate random labels of classes from 0 to n_classes\n",
    "    labels = randint(0, n_classes, n_samples)\n",
    "    return [z_input, labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function generating `n_samples` of fake images using `latent_dim`-dimensional random input vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # generate random latent inputs\n",
    "    z_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs with generator\n",
    "    images = generator.predict([z_input, labels_input])\n",
    "    # label fake images as 0\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return [images, labels_input], y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function training the adversarial model by alternating adjustments of weights of the generator and the  discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=128):\n",
    "    # number of batches in one epoch\n",
    "    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # enumerate batches in each epoch\n",
    "        for j in range(bat_per_epo):\n",
    "            # select halh-batch of random real images\n",
    "            [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
    "            # update discriminator model weights on real samples\n",
    "            d_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n",
    "            # generate half-batch of fake images\n",
    "            [X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            # update discriminator model weights on fake images\n",
    "            d_loss2, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n",
    "            # prepare full batch of random latent vectors as input for the generator\n",
    "            [z_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n",
    "            # create 'misleading' labels 1 (real image) for the fake samples\n",
    "            y_gan = np.ones((n_batch, 1))\n",
    "            # train weights of the generator on the discriminator's errors\n",
    "            g_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
    "            # summarize loss on this batch\n",
    "            print('>%d, %d/%d, d1= %.3f, d2= %.3f g= %.3f' %\n",
    "                (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
    "    # save the generator model\n",
    "    g_model.save('try_cgan_generator.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a quick training process for only 1 epoch and with `n_batch=1000` to check if everything works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuri/anaconda3/envs/newtf/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/home/yuri/anaconda3/envs/newtf/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, 1/60, d1= 0.695, d2= 0.695 g= 0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuri/anaconda3/envs/newtf/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, 2/60, d1= 0.621, d2= 0.699 g= 0.688\n",
      ">1, 3/60, d1= 0.563, d2= 0.706 g= 0.682\n",
      ">1, 4/60, d1= 0.498, d2= 0.718 g= 0.670\n",
      ">1, 5/60, d1= 0.445, d2= 0.740 g= 0.653\n",
      ">1, 6/60, d1= 0.394, d2= 0.770 g= 0.629\n",
      ">1, 7/60, d1= 0.346, d2= 0.809 g= 0.607\n",
      ">1, 8/60, d1= 0.321, d2= 0.846 g= 0.594\n",
      ">1, 9/60, d1= 0.297, d2= 0.853 g= 0.601\n",
      ">1, 10/60, d1= 0.289, d2= 0.824 g= 0.646\n",
      ">1, 11/60, d1= 0.287, d2= 0.747 g= 0.728\n",
      ">1, 12/60, d1= 0.297, d2= 0.652 g= 0.829\n",
      ">1, 13/60, d1= 0.309, d2= 0.569 g= 0.942\n",
      ">1, 14/60, d1= 0.303, d2= 0.488 g= 1.056\n",
      ">1, 15/60, d1= 0.306, d2= 0.445 g= 1.119\n",
      ">1, 16/60, d1= 0.319, d2= 0.447 g= 1.084\n",
      ">1, 17/60, d1= 0.310, d2= 0.495 g= 0.975\n",
      ">1, 18/60, d1= 0.305, d2= 0.564 g= 0.866\n",
      ">1, 19/60, d1= 0.253, d2= 0.629 g= 0.779\n",
      ">1, 20/60, d1= 0.238, d2= 0.699 g= 0.708\n",
      ">1, 21/60, d1= 0.221, d2= 0.815 g= 0.624\n",
      ">1, 22/60, d1= 0.200, d2= 0.981 g= 0.523\n",
      ">1, 23/60, d1= 0.184, d2= 1.140 g= 0.460\n",
      ">1, 24/60, d1= 0.162, d2= 1.239 g= 0.445\n",
      ">1, 25/60, d1= 0.147, d2= 1.209 g= 0.499\n",
      ">1, 26/60, d1= 0.140, d2= 0.989 g= 0.669\n",
      ">1, 27/60, d1= 0.123, d2= 0.664 g= 0.986\n",
      ">1, 28/60, d1= 0.113, d2= 0.431 g= 1.309\n",
      ">1, 29/60, d1= 0.096, d2= 0.365 g= 1.379\n",
      ">1, 30/60, d1= 0.089, d2= 0.390 g= 1.240\n",
      ">1, 31/60, d1= 0.078, d2= 0.458 g= 1.059\n",
      ">1, 32/60, d1= 0.079, d2= 0.534 g= 0.920\n",
      ">1, 33/60, d1= 0.068, d2= 0.602 g= 0.821\n",
      ">1, 34/60, d1= 0.065, d2= 0.676 g= 0.734\n",
      ">1, 35/60, d1= 0.056, d2= 0.772 g= 0.643\n",
      ">1, 36/60, d1= 0.054, d2= 0.857 g= 0.576\n",
      ">1, 37/60, d1= 0.054, d2= 0.937 g= 0.533\n",
      ">1, 38/60, d1= 0.069, d2= 1.000 g= 0.513\n",
      ">1, 39/60, d1= 0.074, d2= 1.006 g= 0.528\n",
      ">1, 40/60, d1= 0.093, d2= 0.943 g= 0.590\n",
      ">1, 41/60, d1= 0.118, d2= 0.819 g= 0.691\n",
      ">1, 42/60, d1= 0.156, d2= 0.679 g= 0.829\n",
      ">1, 43/60, d1= 0.184, d2= 0.571 g= 0.969\n",
      ">1, 44/60, d1= 0.217, d2= 0.502 g= 1.039\n",
      ">1, 45/60, d1= 0.218, d2= 0.498 g= 1.019\n",
      ">1, 46/60, d1= 0.224, d2= 0.524 g= 0.943\n",
      ">1, 47/60, d1= 0.211, d2= 0.569 g= 0.862\n",
      ">1, 48/60, d1= 0.217, d2= 0.623 g= 0.792\n",
      ">1, 49/60, d1= 0.198, d2= 0.669 g= 0.737\n",
      ">1, 50/60, d1= 0.178, d2= 0.705 g= 0.701\n",
      ">1, 51/60, d1= 0.168, d2= 0.733 g= 0.677\n",
      ">1, 52/60, d1= 0.138, d2= 0.755 g= 0.658\n",
      ">1, 53/60, d1= 0.121, d2= 0.775 g= 0.642\n",
      ">1, 54/60, d1= 0.125, d2= 0.802 g= 0.627\n",
      ">1, 55/60, d1= 0.118, d2= 0.829 g= 0.611\n",
      ">1, 56/60, d1= 0.110, d2= 0.854 g= 0.597\n",
      ">1, 57/60, d1= 0.148, d2= 0.876 g= 0.587\n",
      ">1, 58/60, d1= 0.150, d2= 0.893 g= 0.584\n",
      ">1, 59/60, d1= 0.150, d2= 0.888 g= 0.601\n",
      ">1, 60/60, d1= 0.192, d2= 0.867 g= 0.624\n"
     ]
    }
   ],
   "source": [
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# create the discriminator\n",
    "d_model = define_discriminator()\n",
    "# create the generator\n",
    "g_model = define_generator(latent_dim)\n",
    "# create the cgan\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "# prepare image data\n",
    "dataset = prepare_samples()\n",
    "# train model\n",
    "train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=1, n_batch=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run training for longer time to get more reasonable results.\n",
    "\n",
    "Recommendation: run on GPU with, for example, `n_epochs=100, n_batch=128`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
