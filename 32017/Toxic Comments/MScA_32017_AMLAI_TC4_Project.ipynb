{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to try:\n",
    "  - remove stopwords\n",
    "  - tfidf for tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import expanduser\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000997932d777bf</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000103f0d9cfb60f</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000113f07ec002fd</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001b41b1c6bb37e</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001d958c54c6e35</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "id                                                                      \n",
       "0000997932d777bf             0        0       0       0              0  \n",
       "000103f0d9cfb60f             0        0       0       0              0  \n",
       "000113f07ec002fd             0        0       0       0              0  \n",
       "0001b41b1c6bb37e             0        0       0       0              0  \n",
       "0001d958c54c6e35             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPath = expanduser(\"~/Datasets/32017/jigsaw-toxic-comments/\")\n",
    "train = pd.read_csv(dataPath + 'tc_train.csv',index_col=0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding index from file in .txt format. First line contains \n",
    "# dictionary size and embedding dim. Fields are space separated\n",
    "def get_embeddings(file_name):\n",
    "    embeddings_index = {}\n",
    "    with open(file_name, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            if len(values) > 2:\n",
    "                embeddings_index[values[0]] = np.asarray(values[1:], dtype=\"float32\")\n",
    "    return embeddings_index\n",
    "embeddings_path = expanduser('~/Datasets/text_embeddings/')\n",
    "embeddings_index = get_embeddings(embeddings_path+'crawl-300d-2M.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text\n",
    "\n",
    "Remove punctuation and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_clean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000997932d777bf</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000103f0d9cfb60f</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww  he matches this background colour i'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000113f07ec002fd</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man  i'm really not trying to edit war  it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001b41b1c6bb37e</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>more i can't make any real suggestions on im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001d958c54c6e35</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you  sir  are my hero  any chance you remember...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "id                                                                       \n",
       "0000997932d777bf             0        0       0       0              0   \n",
       "000103f0d9cfb60f             0        0       0       0              0   \n",
       "000113f07ec002fd             0        0       0       0              0   \n",
       "0001b41b1c6bb37e             0        0       0       0              0   \n",
       "0001d958c54c6e35             0        0       0       0              0   \n",
       "\n",
       "                                                      comment_clean  \n",
       "id                                                                   \n",
       "0000997932d777bf  explanation why the edits made under my userna...  \n",
       "000103f0d9cfb60f  d'aww  he matches this background colour i'm s...  \n",
       "000113f07ec002fd  hey man  i'm really not trying to edit war  it...  \n",
       "0001b41b1c6bb37e    more i can't make any real suggestions on im...  \n",
       "0001d958c54c6e35  you  sir  are my hero  any chance you remember...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "trans_table = str.maketrans({key: ' ' for key in string.digits + '\\r\\n' +\n",
    "                             string.punctuation.replace(\"\\'\",'')})\n",
    "def preprocess(text):\n",
    "    return ' '.join(text.lower().translate(trans_table).split(' '))\n",
    "\n",
    "train['comment_clean'] = train.comment_text.apply(preprocess)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = list(train)[1:]\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vocabulary of words occurred more than 5\n",
      "36366 top words\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "UNKNOWN_PROXY = 'unknown'\n",
    "MIN_WORD_OCCURRENCE = 5\n",
    "\n",
    "print(\"Creating the vocabulary of words occurred more than\", MIN_WORD_OCCURRENCE)\n",
    "vectorizer = CountVectorizer(lowercase=False, token_pattern=\"\\S+\", \n",
    "                             min_df=MIN_WORD_OCCURRENCE)\n",
    "vectorizer.fit(train.comment_clean)\n",
    "\n",
    "top_words = set(vectorizer.vocabulary_.keys())\n",
    "top_words.add(UNKNOWN_PROXY)\n",
    "print(len(top_words),'top words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenize the text using word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rowena/miniconda3/envs/MScA/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x12ec81cf8>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(train.comment_clean)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 1),\n",
       " ('to', 2),\n",
       " ('of', 3),\n",
       " ('and', 4),\n",
       " ('a', 5),\n",
       " ('you', 6),\n",
       " ('i', 7),\n",
       " ('is', 8),\n",
       " ('that', 9),\n",
       " ('in', 10),\n",
       " ('it', 11),\n",
       " ('for', 12),\n",
       " ('this', 13),\n",
       " ('not', 14),\n",
       " ('on', 15),\n",
       " ('be', 16)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "list(word_index.items())[:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data for Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 2 sequences in `seq`:  [[669, 75, 1, 126, 130, 175, 29, 655, 4305, 11421, 1032, 86, 330, 51, 2203, 11038, 50, 6544, 15, 60, 2654, 148, 7, 2829, 34, 117, 1185, 14720, 2713, 4, 45, 59, 241, 1, 363, 31, 1, 38, 27, 143, 73, 3322, 89], [91985, 52, 2528, 13, 551, 3652, 73, 4345, 2603, 21, 94, 38, 960, 180]]\n",
      "\n",
      "Shape of `data`:  (159571, 50)\n",
      "\n",
      "First prepared text in `data`: [  669    75     1   126   130   175    29   655  4305 11421  1032    86\n",
      "   330    51  2203 11038    50  6544    15    60  2654   148     7  2829\n",
      "    34   117  1185 14720  2713     4    45    59   241     1   363    31\n",
      "     1    38    27   143    73  3322    89     0     0     0     0     0\n",
      "     0     0]\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "seq = tokenizer.texts_to_sequences(train.comment_clean)\n",
    "data = pad_sequences(seq,maxlen=MAX_SEQUENCE_LENGTH,padding='post',\n",
    "                     truncating='post')\n",
    "with open(dataPath + 'comments.pkl','wb') as f: pickle.dump(data, f, -1)\n",
    "\n",
    "print('\\nFirst 2 sequences in `seq`: ',seq[:2])\n",
    "print('\\nShape of `data`: ',data.shape)\n",
    "print('\\nFirst prepared text in `data`:',data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dim = len(next(iter(embeddings_index.values())))\n",
    "embeddings_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix\n",
    "def get_embedding_matrix(word_index,embeddings_index):\n",
    "    nb_words = len(word_index) + 1 # +1 since min(word_index.values())=1\n",
    "    embedding_matrix = np.zeros((nb_words,embeddings_dim))\n",
    "    unknown = 0\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is None: unknown += 1\n",
    "        else: embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix, unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98379 unknown words\n"
     ]
    }
   ],
   "source": [
    "# Create embedding_layer and save it.\n",
    "def make_save_emb_layer(word_index,embeddings_index,layer_file_name):\n",
    "    embedding_matrix,unknown = get_embedding_matrix(word_index,embeddings_index)\n",
    "    embedding_layer = Embedding(embedding_matrix.shape[0],embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix],trainable=False)\n",
    "    with open(layer_file_name,'wb') as f: \n",
    "        pickle.dump(embedding_layer, f, -1)\n",
    "    return unknown\n",
    "\n",
    "EMBEDDING_LAYER_FILE = dataPath + 'toxic_comments_embed_layer.pkl'\n",
    "print(make_save_emb_layer(word_index,embeddings_index,EMBEDDING_LAYER_FILE),\n",
    "      'unknown words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into new train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: \n",
      " id\n",
      "0000997932d777bf    000000\n",
      "000103f0d9cfb60f    000000\n",
      "000113f07ec002fd    000000\n",
      "0001b41b1c6bb37e    000000\n",
      "0001d958c54c6e35    000000\n",
      "dtype: object\n",
      "\n",
      "Counts of labels: \n",
      " 000000    143346\n",
      "100000      5666\n",
      "101010      3800\n",
      "101000      1758\n",
      "100010      1215\n",
      "111010       989\n",
      "101011       618\n",
      "001000       317\n",
      "000010       301\n",
      "111011       265\n",
      "001010       181\n",
      "111000       158\n",
      "100001       136\n",
      "100011       134\n",
      "101110       131\n",
      "100100       113\n",
      "111110        64\n",
      "101111        56\n",
      "000001        54\n",
      "110000        41\n",
      "101001        35\n",
      "111111        31\n",
      "000011        28\n",
      "000100        22\n",
      "001011        18\n",
      "100110        16\n",
      "110010        14\n",
      "110100        11\n",
      "101100        11\n",
      "110011         7\n",
      "100101         7\n",
      "rare           6\n",
      "111001         6\n",
      "111100         4\n",
      "000110         3\n",
      "110001         3\n",
      "001001         3\n",
      "100111         3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# convert each vector of labels to the string\n",
    "labels = train[types].astype(str).apply(lambda x: ''.join(x),axis=1)\n",
    "print('Labels: \\n',labels.head())\n",
    "# aggregate rare combinations if any\n",
    "count = labels.value_counts()\n",
    "rare = count.index[count<=2]\n",
    "labels[np.isin(labels.values,rare)] = 'rare'\n",
    "print('\\nCounts of labels: \\n',labels.value_counts())\n",
    "train_index, val_index = train_test_split(list(range(data.shape[0])), test_size=0.2, \n",
    "                                      stratify = labels, random_state=0)\n",
    "# save train and validation indices for further calculations\n",
    "fname = dataPath + 'train_val_split.pkl'\n",
    "with open(fname, 'wb') as f: pickle.dump([train_index, val_index], f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense,Embedding,Input,Dropout,Conv1D\n",
    "from keras.layers import SpatialDropout1D, Flatten,LSTM,GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.utils import plot_model\n",
    "\n",
    "def get_model():\n",
    "    input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    x = embedding_layer(input_layer)\n",
    "    x = SpatialDropout1D(0.5)(x)\n",
    "    x = LSTM(10, return_sequences=True)(x)\n",
    "    x = Conv1D(5, kernel_size=2, padding=\"valid\")(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(.2)(x)\n",
    "    output_layer = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EMBEDDING_LAYER_FILE, 'rb') as f: embedding_layer = pickle.load(f)\n",
    "with open(dataPath + 'comments.pkl', 'rb') as f: data = pickle.load(f)   \n",
    "\n",
    "X_train = data[train_index]\n",
    "X_val = data[val_index]\n",
    "\n",
    "y_train = train.iloc[train_index][['toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n",
    "y_val = train.iloc[val_index][['toxic','severe_toxic','obscene','threat','insult','identity_hate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 300)           58741800  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 50, 10)            12440     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 49, 5)             105       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 5)                 20        \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 36        \n",
      "=================================================================\n",
      "Total params: 58,754,401\n",
      "Trainable params: 12,591\n",
      "Non-trainable params: 58,741,810\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "best_model_path = 'best_model.h5'\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "early_stopping = EarlyStopping(patience=2)\n",
    "model_checkpoint = ModelCheckpoint(best_model_path,\n",
    "                                   save_best_only=True, save_weights_only=True)\n",
    "\n",
    "model = get_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/28\n",
      " - 110s - loss: 0.5745 - val_loss: 0.3872\n",
      "Epoch 2/28\n",
      " - 121s - loss: 0.2940 - val_loss: 0.1857\n",
      "Epoch 3/28\n",
      " - 131s - loss: 0.1503 - val_loss: 0.0980\n",
      "Epoch 4/28\n",
      " - 133s - loss: 0.1048 - val_loss: 0.0725\n",
      "Epoch 5/28\n",
      " - 121s - loss: 0.0882 - val_loss: 0.0642\n",
      "Epoch 6/28\n",
      " - 105s - loss: 0.0797 - val_loss: 0.0586\n",
      "Epoch 7/28\n",
      " - 116s - loss: 0.0755 - val_loss: 0.0559\n",
      "Epoch 8/28\n",
      " - 123s - loss: 0.0720 - val_loss: 0.0541\n",
      "Epoch 9/28\n",
      " - 114s - loss: 0.0692 - val_loss: 0.0537\n",
      "Epoch 10/28\n",
      " - 124s - loss: 0.0678 - val_loss: 0.0531\n",
      "Epoch 11/28\n",
      " - 149s - loss: 0.0663 - val_loss: 0.0520\n",
      "Epoch 12/28\n",
      " - 112s - loss: 0.0660 - val_loss: 0.0512\n",
      "Epoch 13/28\n",
      " - 112s - loss: 0.0643 - val_loss: 0.0519\n",
      "Epoch 14/28\n",
      " - 113s - loss: 0.0632 - val_loss: 0.0503\n",
      "Epoch 15/28\n",
      " - 113s - loss: 0.0628 - val_loss: 0.0509\n",
      "Epoch 16/28\n",
      " - 117s - loss: 0.0623 - val_loss: 0.0506\n",
      "validation AUC 0.9742115410944733\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train,validation_data=(X_val, y_val),\n",
    "                 epochs=28, batch_size=BATCH_SIZE, shuffle=True, verbose=2,\n",
    "                 callbacks=[model_checkpoint, early_stopping])\n",
    "model.load_weights(best_model_path)\n",
    "val_pred = model.predict(X_val, batch_size=BATCH_SIZE, verbose=0)\n",
    "print('validation AUC',roc_auc_score(y_val, val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89186, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00001cee341fdb12</th>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000247867823ef7</th>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00013b17ad220c46</th>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00017563c3f7919a</th>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00017695ad8997eb</th>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text\n",
       "id                                                                 \n",
       "00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(dataPath + 'tc_test.csv',index_col=0)\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['comment_clean'] = test.comment_text.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test.comment_clean)\n",
    "test_data = pad_sequences(test_seq,maxlen=MAX_SEQUENCE_LENGTH,padding='post',\n",
    "                         truncating='post')\n",
    "with open(dataPath + 'test_comments.pkl','wb') as f: pickle.dump(data, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_data, batch_size=BATCH_SIZE, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame(test_pred, index=test.index)\n",
    "out.columns = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "out.to_csv(dataPath + 'tc_output1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89186, 6)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MScA",
   "language": "python",
   "name": "msca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
