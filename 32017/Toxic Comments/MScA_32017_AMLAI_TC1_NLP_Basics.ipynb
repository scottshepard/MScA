{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iLykei Lecture Series\n",
    "# Advanced Machine Learning and Artificial Intelligence (MScA 32017)\n",
    "\n",
    "# Project: Detection of Toxic Comments Online\n",
    "\n",
    "## Notebook 1: NLP Basics\n",
    "\n",
    "## Yuri Balasanov, Leonid Nazarov, &copy; iLykei 2018\n",
    "\n",
    "# Word embeddings\n",
    "\n",
    "Word embedding is the collective name for a set of language modeling in natural language processing where words or phrases from the vocabulary are mapped to vectors of real numbers. This idea is rather popular in modern machine learning and many embedding models were created in the recent years. The most famous are [Google's word2vec](https://code.google.com/archive/p/word2vec/), [Glove](https://nlp.stanford.edu/projects/glove/), [Lexvec]( https://github.com/alexandres/lexvec), [sent2vec](https://github.com/epfml/sent2vec), [Facebook's fastText](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md), [numberbatch](https://github.com/commonsense/conceptnet-numberbatch), [bpe](https://github.com/bheinzerling/bpemb).  \n",
    "\n",
    "# FastText\n",
    "\n",
    "Below we will use FastText embeddings [crawl-300d-2M.vec](https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M.vec.zip) - 2 million word vectors trained on Common Crawl (600B tokens). These vectors in dimension 300 were obtained using model described in [Bojanowski et al. (2016)](https://arxiv.org/abs/1607.04606). Authors proposed a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations.   \n",
    "File *crawl-300d-2M.vec* with word vectors has the following format.  \n",
    "The first line of the file contains the number of words in the vocabulary and the size of the vectors. Each line contains a word followed by its vectors, like in the default fastText text format. Each value is space separated. Words are ordered by descending frequency.  \n",
    "Create dictionary with words as keys and embeddings as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create embedding index from file in .txt format. First line contains dictionary size and embedding dim. Fields are space separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embeddings(file_name):\n",
    "    embeddings_index = {}\n",
    "    with open(file_name, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            # remove white spaces and split\n",
    "            values = line.rstrip().split(' ')\n",
    "            if len(values) > 2:\n",
    "                embeddings_index[values[0]] = np.asarray(values[1:], dtype=\"float32\")\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embeddings_path = './Embeddings/'\n",
    "embeddings_index = get_embeddings(embeddings_path+'crawl-300d-2M.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 25 items in the embeddings index, show the first key and the corresponding value of the index. Value is 300-long numeric vector corresponding to the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'the', '.', 'and', 'to', 'of', 'a', 'in', 'is', 'for', 'that', 'I', 'it', 'on', 'with', ')', ':', '\"', '(', 'The', 'you', 'was', 'are', 'or', 'this']\n",
      "\n",
      "First key:  , \n",
      "\n",
      "Vector length:  300 \n",
      "\n",
      "Vector for first key:  [-2.820e-02 -5.570e-02 -4.510e-02 -4.340e-02  7.120e-02 -8.550e-02\n",
      " -1.085e-01 -5.610e-02 -4.523e-01 -2.020e-02  9.750e-02  1.047e-01\n",
      "  1.962e-01 -6.930e-02  2.130e-02 -2.350e-02  1.336e-01 -4.200e-02\n",
      " -5.640e-02 -7.980e-02  4.240e-02 -4.090e-02 -5.360e-02 -2.520e-02\n",
      "  1.350e-02  6.400e-03  1.235e-01  4.610e-02  1.200e-02 -3.720e-02\n",
      "  6.500e-02  4.100e-03 -1.074e-01 -2.630e-02  1.133e-01 -2.900e-03\n",
      "  6.710e-02  1.065e-01  2.340e-02 -1.600e-02  7.000e-03  4.355e-01\n",
      " -7.520e-02 -4.328e-01  4.570e-02  6.040e-02 -7.400e-02 -5.500e-03\n",
      " -8.900e-03 -2.926e-01 -5.450e-02 -1.519e-01  9.900e-02 -1.930e-02\n",
      " -5.000e-03  5.110e-02  4.040e-02  1.023e-01 -1.280e-02  4.880e-02\n",
      " -1.567e-01 -7.590e-02 -1.900e-02  1.442e-01  4.700e-03 -1.860e-02\n",
      "  1.400e-02 -3.850e-02 -8.530e-02  1.572e-01  1.770e-01  8.400e-03\n",
      " -2.500e-02 -1.145e-01 -6.630e-02 -1.244e-01 -3.977e-01 -1.240e-02\n",
      " -4.586e-01 -2.200e-02  5.746e-01  2.180e-02 -7.540e-02  9.900e-03\n",
      "  3.970e-02 -1.540e-02  4.240e-02 -1.500e-02 -1.600e-03  3.050e-02\n",
      "  1.010e-02  2.266e-01  1.394e-01  1.890e-02  6.900e-03  3.940e-02\n",
      "  3.550e-02 -1.110e-02 -6.870e-02 -7.800e-03  2.240e-02  8.170e-02\n",
      " -1.949e-01  1.000e-04  4.047e-01 -2.370e-02 -6.560e-02 -6.840e-02\n",
      "  2.330e-02  4.380e-02  1.203e-01 -2.760e-02  4.160e-02  1.140e-02\n",
      " -4.529e-01  1.538e-01  1.323e-01 -1.860e-02 -9.140e-02 -3.120e-02\n",
      "  1.051e-01  2.120e-02  7.980e-02 -1.040e-02 -2.060e-02 -2.500e-03\n",
      "  4.300e-03 -3.780e-02  2.689e-01  7.470e-02 -4.180e-02 -4.800e-03\n",
      " -3.870e-02  4.320e-02  1.704e-01  6.140e-02  9.050e-02 -4.360e-02\n",
      " -1.410e-02 -3.150e-02  2.760e-02  1.510e-02 -1.030e-02 -2.660e-02\n",
      " -5.120e-02 -4.080e-02 -6.510e-02  6.620e-02 -9.360e-02  1.371e-01\n",
      "  4.580e-02 -1.366e-01 -7.500e-03 -1.040e-02 -7.320e-02  1.205e-01\n",
      "  1.035e-01  1.060e-02 -3.170e-02 -3.160e-02  6.639e-01 -2.200e-03\n",
      " -1.343e-01  1.440e-02 -3.380e-02  3.400e-03 -4.290e-02 -8.210e-02\n",
      "  3.700e-03  1.029e-01 -2.040e-02 -2.690e-02  5.200e-03 -1.034e-01\n",
      "  1.068e-01  1.210e-02  9.800e-02 -4.580e-02  1.990e-02 -1.320e-02\n",
      "  1.936e-01 -2.130e-02  2.090e-02 -2.500e-03  4.160e-02 -3.370e-02\n",
      "  5.160e-02 -1.014e-01  2.030e-02  1.980e-02 -3.050e-02 -3.130e-02\n",
      "  5.430e-02 -1.060e-02  1.441e-01 -1.780e-02 -6.270e-02  4.750e-02\n",
      "  3.520e-02 -2.540e-02 -9.490e-02  4.010e-02  3.170e-02  5.500e-03\n",
      " -5.360e-02  1.910e-02 -5.110e-02 -4.090e-02 -3.000e-03  1.582e-01\n",
      "  1.080e-02  5.237e-01  4.360e-02  3.060e-02 -3.920e-02  1.770e-02\n",
      "  6.900e-03  6.050e-02  1.206e-01 -2.160e-02 -6.330e-02 -2.965e-01\n",
      "  5.210e-02 -1.500e-02 -2.207e-01 -6.420e-02 -9.060e-02 -1.210e-02\n",
      "  5.690e-02  9.440e-02 -6.520e-02 -1.080e-02 -4.770e-02  2.300e-03\n",
      "  7.700e-03 -1.547e-01  4.630e-02  6.980e-02 -3.760e-02 -2.910e-02\n",
      "  3.300e-03 -1.020e-02 -7.430e-02  8.500e-03  8.050e-02 -2.910e-02\n",
      " -6.740e-02 -5.860e-02 -6.530e-02  2.830e-02 -2.550e-02  8.690e-02\n",
      " -8.680e-02  9.000e-03  3.245e-01 -5.730e-02 -2.890e-02  4.700e-02\n",
      " -1.170e-02  1.740e-02  1.320e-02 -2.260e-02 -6.640e-02  1.880e-02\n",
      "  2.630e-02  1.110e-02 -4.900e-03 -6.560e-02  2.950e-02  4.350e-02\n",
      "  2.900e-02  1.163e-01  4.480e-02 -1.139e-01 -5.530e-02 -5.280e-02\n",
      "  1.745e-01 -1.460e-02 -1.308e-01 -6.070e-02 -1.340e-02  7.810e-02\n",
      "  3.780e-02  2.280e-02 -7.280e-02 -5.900e-03  1.580e-02 -1.410e-02\n",
      " -2.000e-04  1.930e-02 -1.480e-02 -4.630e-02  4.440e-02  3.034e-01\n",
      "  1.020e-01 -8.710e-02  3.170e-02 -3.700e-02 -7.250e-02 -4.200e-03]\n"
     ]
    }
   ],
   "source": [
    "print(list(embeddings_index.keys())[:25])\n",
    "ke,va = list(embeddings_index.items())[0]\n",
    "print('\\nFirst key: ',ke,'\\n\\nVector length: ',len(va),'\\n\\nVector for first key: ',va)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing before creating fastText embeddings includes removing digits, isolating punctuation and replacing consecutive spaces with single one. Most of punctuation marks and some contractions (forms like can't and it's) are in vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", True\n",
      "! True\n",
      "!! False\n",
      "it's True\n",
      "I'll True\n",
      "* True\n",
      "> True\n",
      "¿ True\n",
      "£ True\n",
      "' True\n",
      "’ True\n"
     ]
    }
   ],
   "source": [
    "for word in [',','!','!!',\"it's\",\"I'll\",'*','>','¿','£',\"'\",\"’\"]:\n",
    "    print(word, word in embeddings_index.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim\n",
    "\n",
    "Embeddings can also be loaded with [*gensim*](https://radimrehurek.com/gensim/) package which provides a number of useful functions for handling similar words.\n",
    "\n",
    "### Warning.\n",
    "**Gensim may take too much nemory depending on available resources on your computer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = './Embeddings/'\n",
    "import gensim\n",
    "m = gensim.models.KeyedVectors.load_word2vec_format(embeddings_path+'crawl-300d-2M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.most_similar('recommend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that misspelled words *\"reccomend\", \"reccommend\", \"recomend\"* are among the most similar to *\"recommend\"*. This feature of fastText embeddings is extermly valuable in analysis of toxic comments with a lot of misspelled words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim also allows using expressions with words, like in this example: $$woman+king-man=queen.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.most_similar(positive=['woman', 'king'], negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "    \n",
    "**Assignment** <br>\n",
    "What is the result $x$ of the following expression:  \n",
    "$$IL-x=CA-LA$$ \n",
    "Find three \"most similar\" solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipped code\n",
    "# x = IL+LA-CA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for the project\n",
    "\n",
    "This section shows how to split the multilabel dataset of the project into train and validation sets in a stratified fashion.\n",
    "\n",
    "Read the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000997932d777bf</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000103f0d9cfb60f</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000113f07ec002fd</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001b41b1c6bb37e</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001d958c54c6e35</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00025465d4725e87</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "id                                                                      \n",
       "0000997932d777bf             0        0       0       0              0  \n",
       "000103f0d9cfb60f             0        0       0       0              0  \n",
       "000113f07ec002fd             0        0       0       0              0  \n",
       "0001b41b1c6bb37e             0        0       0       0              0  \n",
       "0001d958c54c6e35             0        0       0       0              0  \n",
       "00025465d4725e87             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataPath = \"./data/\"\n",
    "\n",
    "train = pd.read_csv(dataPath+'tc_train.csv',index_col=0)\n",
    "types = list(train)[1:]\n",
    "print(types)\n",
    "train.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check some of the bad comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi! I am back again!\\nLast warning!\\nStop undoing my edits or die!',\n",
       " \"I think that your a Fagget get a oife and burn in Hell I hate you 'm sorry we cant have any more sex i'm running out of conndoms\",\n",
       " \"I'm also a sock puppet of this account...SUPRISE!!\\n-sincerely,\\n            The man that will track you down from the Internet and kill you\",\n",
       " 'Fuck you, Smith. Please have me notified when you die. I want to dance on your grave.',\n",
       " \"WOULDN'T BE THE FIRST TIME BITCH. FUCK YOU I'LL FIND OUT WHERE YOU LIVE, SODOMIZE YOUR WIFE AND THEN BURN YOUR HOUSE DOWN. FUCK YOU YOU FUCKING QUEER.\",\n",
       " 'Whoever put a notices on my page. I will kill u']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train[train['threat']==1]['comment_text'][:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сomment can generally belong to several classes simultaneously. Here are the counts of combinations of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"12\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>201081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"29\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"15\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>7376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>1754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>2612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>5732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"14\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"7\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>1165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        comment_text\n",
       "toxic severe_toxic obscene threat insult identity_hate              \n",
       "0     0            0       0      0      0                    201081\n",
       "                                         1                        68\n",
       "                                  1      0                       365\n",
       "                                         1                        32\n",
       "                           1      0      0                        27\n",
       "                                  1      0                         4\n",
       "                   1       0      0      0                       366\n",
       "                                         1                         3\n",
       "                                  1      0                       196\n",
       "                                         1                        19\n",
       "                           1      0      0                         2\n",
       "                                  1      0                         2\n",
       "1     0            0       0      0      0                      7376\n",
       "                                         1                       203\n",
       "                                  1      0                      1754\n",
       "                                         1                       215\n",
       "                           1      0      0                       163\n",
       "                                         1                        11\n",
       "                                  1      0                        25\n",
       "                                         1                         3\n",
       "                   1       0      0      0                      2612\n",
       "                                         1                        55\n",
       "                                  1      0                      5732\n",
       "                                         1                       979\n",
       "                           1      0      0                        17\n",
       "                                  1      0                       196\n",
       "                                         1                        81\n",
       "      1            0       0      0      0                        41\n",
       "                                         1                         3\n",
       "                                  1      0                        14\n",
       "                                         1                         7\n",
       "                           1      0      0                        11\n",
       "                                         1                         5\n",
       "                                  1      0                         1\n",
       "                   1       0      0      0                       186\n",
       "                                         1                         7\n",
       "                                  1      0                      1165\n",
       "                                         1                       381\n",
       "                           1      0      0                         8\n",
       "                                  1      0                        88\n",
       "                                         1                        45"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(types).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By stratified splitting of the sample into train and validation based on single label does not preserve proportions of other labels.\n",
    "\n",
    "Split train into new train and validation subsets keeping up  original distribution of classes by transforming labels: transform vectors of labels into strings, then stratify by the column of such strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: \n",
      " id\n",
      "0000997932d777bf    000000\n",
      "000103f0d9cfb60f    000000\n",
      "000113f07ec002fd    000000\n",
      "0001b41b1c6bb37e    000000\n",
      "0001d958c54c6e35    000000\n",
      "dtype: object\n",
      "\n",
      "Counts of labels: \n",
      " 000000    201081\n",
      "100000      7376\n",
      "101010      5732\n",
      "101000      2612\n",
      "100010      1754\n",
      "111010      1165\n",
      "101011       979\n",
      "111011       381\n",
      "001000       366\n",
      "000010       365\n",
      "100011       215\n",
      "100001       203\n",
      "001010       196\n",
      "101110       196\n",
      "111000       186\n",
      "100100       163\n",
      "111110        88\n",
      "101111        81\n",
      "000001        68\n",
      "101001        55\n",
      "111111        45\n",
      "110000        41\n",
      "000011        32\n",
      "000100        27\n",
      "100110        25\n",
      "001011        19\n",
      "101100        17\n",
      "110010        14\n",
      "100101        11\n",
      "110100        11\n",
      "111100         8\n",
      "111001         7\n",
      "110011         7\n",
      "110101         5\n",
      "rare           5\n",
      "000110         4\n",
      "110001         3\n",
      "001001         3\n",
      "100111         3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# convert each vector of labels to the string\n",
    "labels = train[types].astype(str).apply(lambda x: ''.join(x),axis=1)\n",
    "print('Labels: \\n',labels.head())\n",
    "# aggregate rare combinations if any\n",
    "count = labels.value_counts()\n",
    "rare = count.index[count<=2]\n",
    "labels[np.isin(labels.values,rare)] = 'rare'\n",
    "print('\\nCounts of labels: \\n',labels.value_counts())\n",
    "train_index, val_index = train_test_split(train.index, test_size=0.2, \n",
    "                                      stratify = labels, random_state=0)\n",
    "# save train and validation indices for further calculations\n",
    "fname = dataPath + 'train_val_split.pkl'\n",
    "with open(fname, 'wb') as f: pickle.dump([train_index, val_index], f, -1),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tf–idf term weighting\n",
    "\n",
    "Tf-idf (or TFIDF) stands for term frequency-inverse document frequency.\n",
    "Tf-idf is often used in text mining to retrieve information from text. This weight is a statistical measure evaluating importance of a word for a document in a collection of documents or corpus.  \n",
    "\n",
    "Denote\n",
    "- $D$ - corpus of documents;\n",
    "- $n_D$  - total number of documents in the corpus $D$;\n",
    "- ${tf}(t,d)$ - **term frequency** (number of times term t occurs in document d);\n",
    "- ${df}(t,D)$ - **document frequency** (number of documents containing term t).  \n",
    "\n",
    "Then **inverse document frequency** is defined as \n",
    "$${idf}(t,D) = log{\\frac{1 + n_D}{1+{df}(t,D)}} + 1.$$\n",
    "This formula is implemented in the class  [sklearn.feature_extraction.text.TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) used below and differs slightly from the standard textbook notation.  \n",
    "The tf–idf term weight is the product of document frequency and inverse document frequency\n",
    "$$ {tfidf}(t,d,D)= {tf} (t,d) \\times {idf} (t,D).$$\n",
    "Importance of the word increases with the frequency of it in the document, but inversely proportional to the frequency of the word in the corpus.\n",
    "\n",
    "Calculate tf-idf of the corpus of sentences opening Shakespeare's \"Hamlet\": dialogue of two characters, one of them is Francisco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.4472136 , 0.4472136 , 0.4472136 , 0.        ,\n",
       "        0.        , 0.        , 0.4472136 , 0.        , 0.        ,\n",
       "        0.4472136 ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.57735027,\n",
       "        0.        , 0.57735027, 0.        , 0.57735027, 0.        ,\n",
       "        0.        ],\n",
       "       [0.57735027, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.57735027, 0.        , 0.        , 0.        , 0.57735027,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['Have you had quiet guard?',\n",
    "          'Not a mouse stirring.',\n",
    "          'Well, good night.']\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf = TfidfVectorizer()\n",
    "X = tf_idf.fit_transform(corpus)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus contains 11 words. All of them are unique. Three sublists correspond to 3 sentences and show equal weights of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method `fit_transform` returns sparse matrix of tf-idf weights with `len(corpus)` rows and columns corresponding to words in corpus dictionary. The mapping of words to column indices can be found by the attribute `vocabulary_`.  \n",
    "Note that the resulting tf-idf vectors (rows of the matrix) are then normalized by the Euclidean norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'have': 3, 'you': 10, 'had': 2, 'quiet': 7, 'guard': 1, 'not': 6, 'mouse': 4, 'stirring': 8, 'well': 9, 'good': 0, 'night': 5}\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf.vocabulary_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
