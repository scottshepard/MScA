---
title: "31010: Linear & Non-Linear Models -- Assignment 1"
subtitle: "lm() vs glm()"
author: "Scott Shepard"
date: "1/13/2019"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The assignment is to explore how lm and glm are related by calculating
outputs of glm using lm

Calculate:
   1. Coefficients
   2. Residuals
   3. Fitted Values
   4. Linear Predictors
   5. Deviance
      - 5.1 Using deviance()
      - 5.2 Manually
   6. Akaike Infomration Criterion
      - 6.1 Using aic()
      - 6.2 Manually
   7. y
   8. Null Deviance
   9. Dispersion

## 0. Reading Data & Building Models 

Data comes from homework assignment in ilykei.  
There is one output and three inputs

```{r read_data}
dataPath <- "~/Dropbox/MScA/31010 - Linear Non Linear/Assignment 1/"

df <- read.csv(
  file=paste(dataPath,"Week1_Homework_Project_Data.csv",sep="/"),
  header=TRUE,
  sep=",")

head(df, 10)
```

Calculate linear modle and general linear model using the identity link.
We will using LM to calculate values and GLM to confirm those calculations.

```{r}
LM  <-  lm(
  Output ~ Input1 + Input2 + Input3, 
  data=df)

GLM <- glm(
  Output ~ Input1 + Input2 + Input3, 
  family=gaussian(link="identity"),
  data=df)
```

## 1. Coefficients

```{r}
cbind(
  Output.LM=LM$coefficients,
  Output.GLM=GLM$coefficients)
```

## 2. Residuals

This plots the residuals of lm against those of the 
glm. We can see they are exactly the same.

```{r}
matplot(
  x=1:length(df[,1]),
  y=cbind(LM$residuals, GLM$residuals),
  type="l",
  ylab="Residuals",
  xlab="Count")
```

However the residuals aren't exactly the same. Since they were derived 
with different functions (maximum-likelihood vs least-squares) the values
are every so slightly different,

```{r}
head(LM$residuals == GLM$residuals)
```

The toal sum of the differance between the residuals is very very small
Rounding errors really
```{r}
sum(abs(LM$residuals-GLM$residuals))
```

## 3. Fitted Values

Similarly with residuals, the fitted values are not exactly the same

```{r}
head(LM$fitted.values == GLM$fitted.values)
```

However plotting them against each other leads to a perfectly straight line
So the differences are rounding errors

```{r}
plot(x=LM$fitted.values, y=GLM$fitted.values)
```

# 4. Linear Predictors

Linear predictors of glm are the same as fitted.values of lm
In the general case the predictors need to be transformed into fitted values
using the link function. In classic linear the link function is the identity
which is what was used before.

We should see the same plot as in part 3.
```{r}
plot(x=LM$fitted.values, y=GLM$linear.predictors)
```

# 5. Deviance

Deviance in the linear model is measured as sum of squares (SSE).
Deviance can be computed by using the `deviance()` function or by 
computing deviance manually, in this case summing the squares of the residuals.

```{r}
c(From.GLM=GLM$deviance,
  From.LM=sum(LM$residuals^2),
  Function.Deviance=deviance(LM))
```

# 6. AIC

AIC can be calculated from LM using the `AIC()` function. Use that to compare
to the aic computed as a part of glm.
```{r}
c(AIC(LM),
  GLM$aic)
```

AIC can also be calculated manually.

AIC = -2*log-likeihood + 2p

where p is number of predictors-1

```{r}
manual_log_likelihood <- function(linear_model) {
  n = nrow(linear_model$model)
  y = linear_model$model[1,]
  sigma = sd(linear_model$residuals)
  - n/2 * log(2*pi*sigma**2) - 1/(2*sigma**2) * sum(linear_model$residuals**2)
}

manual_aic <- function(linear_model) {
  log_likelihood <- manual_log_likelihood(linear_model)
  p = length(linear_model$coefficients)-1
  -2*log_likelihood+2*p
}

cbind(AIC.Manual=manual_aic(LM),
      AIC.From.Function=AIC(LM),
      AIC.From.GLM=GLM$aic)
```

# 7. y

```{r}
sum(abs(df[,1]-df$y))
```

# 8. Null Deviance

Null deviance of glm() is the deviance of the null model, i.e. the model that 
has only intercept. Since deviance of glm() in case of  
`family=gaussian(link="identity")` is equivalent to SSE of lm() we need to 
estimate the null model with only intercept and calculate its SSE.

```{r}
LM.Null <- lm(Output~1, data=df)

c(LM.Null.SSE=sum(LM.Null$residuals**2), GLM.Null.Deviance=GLM$null.deviance)
```

# 9. Dispersion

Dispersion for Gaussian models is the same as MSE.

We can get to that number from several starting points.

From LM sigma (residual standard deviation). 
MSE = sigma**2
```{r}
c(Disperson.From.GLM=summary(GLM)$dispersion,
  MSE.From.LM.Sigma=sigma(LM)**2)
```

From variance of residuals of glm
```{r}
var(GLM$residuals)*((nrow(df))/(nrow(df)-3))
```
