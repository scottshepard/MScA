{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31009 - Assignment 2: Regression with Gradient Descent\n",
    "\n",
    "Use linear regression with gradient descent to predict water temperature T_degC using the dataset from assignment 1.\n",
    "\n",
    "1) Only use 'Salnty', 'STheta' for predictors (same as HW #1)\n",
    "\n",
    "2) Remove NaN / NA values from dataset (prior to building train/test sets) (same as HW #1)\n",
    "\n",
    "3) Scale all features to improve convergence. It is highly encouraged that you review the appropriate method of handling normalization with train & test. Additional info on scaling with train & test sets: https://stats.stackexchange.com/questions/174823/how-to-apply-standardization-normalization-to-train-and-testset-if-prediction-i\n",
    "\n",
    "Feel free to use the sklearn tool \"StandardScaler\" - more info here: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html (Links to an external site.)Links to an external site. \n",
    "\n",
    "4) For the cost function we will use mean squared error. Please use the same function utilized in Chapter 4 of The Hands On Machine Learning (Equation 4.5) for the derivative of the cost function. \n",
    "\n",
    "Example code from chapter: \n",
    "\n",
    "derived_cost_function = 2/m * X_b.T.dot(X_b.dot(theta) - y)  \n",
    "\n",
    "5) Try mini batch sizes of 50, 2000 and 10,000. Comment on the prediction accuracy based on rmse, variance explained, and r-squared.\n",
    "\n",
    "Note: Feel free to use any value for eta and epochs, but 0.1 eta and 100 epochs are fine for this HW. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rowena/miniconda3/envs/msca/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import statsmodels.api as sm\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rowena/miniconda3/envs/msca/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (47,73) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/bottle.csv')\n",
    "df = df.loc[:,['Salnty', 'STheta', 'T_degC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 864863 rows in raw dataset\n",
      "\n",
      "Remove 47354 rows because of missing values in Salnty\n",
      "Remove 5335 rows because of missing values in STheta\n",
      "Remove 0 rows because of missing values in T_degC\n",
      "\n",
      "Clean dataset has 812174 rows\n"
     ]
    }
   ],
   "source": [
    "print('There are {0} rows in raw dataset\\n'.format(len(df)))\n",
    "\n",
    "print(\"Remove {0} rows because of missing values in Salnty\".format(sum(df.Salnty.isna())))\n",
    "df = df[~df.Salnty.isna()]\n",
    "\n",
    "print(\"Remove {0} rows because of missing values in STheta\".format(sum(df.STheta.isna())))\n",
    "df = df[~df.STheta.isna()]\n",
    "\n",
    "print(\"Remove {0} rows because of missing values in T_degC\\n\".format(sum(df.T_degC.isna())))\n",
    "df = df[~df.T_degC.isna()]\n",
    "\n",
    "print(\"Clean dataset has {0} rows\".format(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Outliers\n",
    "\n",
    "There are big outliers in STheta that need to be removed or else they will screw up the line of best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1113fb5f8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bd40898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.boxplot(column='STheta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.STheta <= 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Test / Train\n",
    "\n",
    "In this assignment I am using a 70/30 test/train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "msk = np.random.rand(len(df)) < 0.7\n",
    "tn = df[msk]\n",
    "tt = df[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the Data\n",
    "\n",
    "Scale each variable in train and test. The test data should be scaled with training mean and sd to avoid information leakage.  \n",
    "It is not necessary to scale the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = tn.mean()\n",
    "stdvs = tn.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rowena/miniconda3/envs/msca/lib/python3.6/site-packages/pandas/core/indexing.py:621: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "tn.loc[:,'Salnty'] = (tn.Salnty - means['Salnty']) / stdvs['Salnty']\n",
    "tn.loc[:,'STheta'] = (tn.STheta - means['STheta']) / stdvs['STheta']\n",
    "\n",
    "tt.loc[:,'Salnty'] = (tt.Salnty - means['Salnty']) / stdvs['Salnty']\n",
    "tt.loc[:,'STheta'] = (tt.STheta - means['STheta']) / stdvs['STheta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Gradient descent calculates a partial derivitaive at each step at then goes the other way to attempt to find a global mininum in the cost function. If done correctly it should reach the same answer as solving for the cost function directly but without the computational expense of inverting a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = tn[tn.columns[0:2]]\n",
    "X_b.insert(0, 'intercept', 1)\n",
    "y = tn.T_degC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intercept</th>\n",
       "      <th>Salnty</th>\n",
       "      <th>STheta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.871123</td>\n",
       "      <td>-0.167803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.871123</td>\n",
       "      <td>-0.160851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.877622</td>\n",
       "      <td>-0.162837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.914454</td>\n",
       "      <td>-0.173761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.890622</td>\n",
       "      <td>-0.165817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   intercept    Salnty    STheta\n",
       "0          1 -0.871123 -0.167803\n",
       "1          1 -0.871123 -0.160851\n",
       "2          1 -0.877622 -0.162837\n",
       "3          1 -0.914454 -0.173761\n",
       "5          1 -0.890622 -0.165817"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(X_b, y, n_epochs, minibatch_size, eta):\n",
    "    m = len(X_b)\n",
    "\n",
    "    np.random.seed(123)\n",
    "    theta = np.random.randn(3,1) \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_indices = np.random.choice(X_b.index, m, replace=False)\n",
    "        X_b_shuffled = X_b.loc[shuffled_indices]\n",
    "        y_shuffled = y.loc[shuffled_indices]\n",
    "        y_shuffled = np.asarray(y_shuffled).reshape((len(y_shuffled),1))\n",
    "        for i in range(0, m, minibatch_size):\n",
    "            xi = X_b_shuffled[i:i+minibatch_size]\n",
    "            yi = y_shuffled[i:i+minibatch_size]\n",
    "            derived_cost_function = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n",
    "            theta = theta - eta * derived_cost_function\n",
    "    return(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute estimates of the linear regression using the the mini-batch gradient descent on different batch sizes: 50, 2,000, and 50,000. For each batch size we'll use 20 iterations and a 0.1 learning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-batch Gradient Descent with a 50 minibatch size\n",
      "                   0\n",
      "intercept  10.853977\n",
      "Salnty      1.414366\n",
      "STheta     -5.053720\n",
      "MBGD 50 took about 469 seconds.\n",
      "\n",
      "Mini-batch Gradient Descent with a 2,000 minibatch size\n",
      "                   0\n",
      "intercept  10.846961\n",
      "Salnty      1.446968\n",
      "STheta     -5.074566\n",
      "MBGD 2,000 took about 17 seconds.\n",
      "\n",
      "Mini-batch Gradient Descent with a 50,000 minibatch size\n",
      "                   0\n",
      "intercept  10.845809\n",
      "Salnty      1.435034\n",
      "STheta     -5.073946\n",
      "MBGD 50,000 took about 7 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "theta_50 = mini_batch_gradient_descent(X_b, y, 20, 50, 0.1)\n",
    "end = time.time()\n",
    "time_50 = end - start\n",
    "print(\"Mini-batch Gradient Descent with a 50 minibatch size\")\n",
    "print(theta_50)\n",
    "print(\"MBGD 50 took about {} seconds.\".format(round(time_50)))\n",
    "\n",
    "start = time.time()\n",
    "theta_2000 = mini_batch_gradient_descent(X_b, y, 20, 2000, 0.1)\n",
    "end = time.time()\n",
    "time_2000 = end - start\n",
    "print(\"\\nMini-batch Gradient Descent with a 2,000 minibatch size\")\n",
    "print(theta_2000)\n",
    "print(\"MBGD 2,000 took about {} seconds.\".format(round(time_2000)))\n",
    "\n",
    "start = time.time()\n",
    "theta_50000 = mini_batch_gradient_descent(X_b, y, 20, 50000, 0.1)\n",
    "end = time.time()\n",
    "time_50000 = end - start\n",
    "print(\"\\nMini-batch Gradient Descent with a 50,000 minibatch size\")\n",
    "print(theta_50000)\n",
    "print(\"MBGD 50,000 took about {} seconds.\".format(round(time_50000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RSME: 0.4828\n",
      "Test  RSME: 0.484\n",
      "Train Variance: 17.79\n",
      "Test  Variance: 17.87\n",
      "Train Variance Explained: 17.56\n",
      "Test  Variance Explained: 17.63\n",
      "Train R2: 0.986901\n",
      "Test  R2: 0.986888\n"
     ]
    }
   ],
   "source": [
    "tn_pred_50 = theta_50[0][0] + theta_50[0][1] * tn.Salnty + theta_50[0][2] * tn.STheta\n",
    "tt_pred_50 = theta_50[0][0] + theta_50[0][1] * tt.Salnty + theta_50[0][2] * tt.STheta\n",
    "\n",
    "tn_rmse_50 = (((tn_pred_50 - tn.T_degC)**2).sum() / len(tn))**0.5\n",
    "tt_rmse_50 = (((tt_pred_50 - tt.T_degC)**2).sum() / len(tt))**0.5\n",
    "\n",
    "print(\"Train RSME: {}\".format(round(tn_rmse_50, 4)))\n",
    "print(\"Test  RSME: {}\".format(round(tt_rmse_50, 4)))\n",
    "\n",
    "tn_var = ((tn.T_degC - tn.T_degC.mean())**2).mean()\n",
    "tt_var = ((tt.T_degC - tt.T_degC.mean())**2).mean()\n",
    "\n",
    "print(\"Train Variance: {}\".format(round(tn_var, 2)))\n",
    "print(\"Test  Variance: {}\".format(round(tt_var, 2)))\n",
    "\n",
    "tn_var_explained_50 = tn_var - tn_rmse_50**2\n",
    "tt_var_explained_50 = tt_var - tt_rmse_50**2\n",
    "\n",
    "print(\"Train Variance Explained: {}\".format(round(tn_var_explained_50, 2)))\n",
    "print(\"Test  Variance Explained: {}\".format(round(tt_var_explained_50, 2)))\n",
    "\n",
    "tn_r2_50 = tn_var_explained_50 / tn_var\n",
    "tt_r2_50 = tt_var_explained_50 / tt_var\n",
    "\n",
    "print(\"Train R2: {}\".format(round(tn_r2_50, 6)))\n",
    "print(\"Test  R2: {}\".format(round(tt_r2_50, 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RSME: 0.4826\n",
      "Test  RSME: 0.4839\n",
      "Train Variance: 17.79\n",
      "Test  Variance: 17.87\n",
      "Train Variance Explained: 17.56\n",
      "Test  Variance Explained: 17.63\n",
      "Train R2: 0.986911\n",
      "Test  R2: 0.986896\n"
     ]
    }
   ],
   "source": [
    "tn_pred_2000 = theta_2000[0][0] + theta_2000[0][1] * tn.Salnty + theta_2000[0][2] * tn.STheta\n",
    "tt_pred_2000 = theta_2000[0][0] + theta_2000[0][1] * tt.Salnty + theta_2000[0][2] * tt.STheta\n",
    "\n",
    "tn_rmse_2000 = (((tn_pred_2000 - tn.T_degC)**2).sum() / len(tn))**0.5\n",
    "tt_rmse_2000 = (((tt_pred_2000 - tt.T_degC)**2).sum() / len(tt))**0.5\n",
    "\n",
    "print(\"Train RSME: {}\".format(round(tn_rmse_2000, 4)))\n",
    "print(\"Test  RSME: {}\".format(round(tt_rmse_2000, 4)))\n",
    "\n",
    "tn_var = ((tn.T_degC - tn.T_degC.mean())**2).mean()\n",
    "tt_var = ((tt.T_degC - tt.T_degC.mean())**2).mean()\n",
    "\n",
    "print(\"Train Variance: {}\".format(round(tn_var, 2)))\n",
    "print(\"Test  Variance: {}\".format(round(tt_var, 2)))\n",
    "\n",
    "tn_var_explained_2000 = tn_var - tn_rmse_2000**2\n",
    "tt_var_explained_2000 = tt_var - tt_rmse_2000**2\n",
    "\n",
    "print(\"Train Variance Explained: {}\".format(round(tn_var_explained_2000, 2)))\n",
    "print(\"Test  Variance Explained: {}\".format(round(tt_var_explained_2000, 2)))\n",
    "\n",
    "tn_r2_2000 = tn_var_explained_2000 / tn_var\n",
    "tt_r2_2000 = tt_var_explained_2000 / tt_var\n",
    "\n",
    "print(\"Train R2: {}\".format(round(tn_r2_2000, 6)))\n",
    "print(\"Test  R2: {}\".format(round(tt_r2_2000, 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RSME: 0.4825\n",
      "Test  RSME: 0.4838\n",
      "Train Variance: 17.79\n",
      "Test  Variance: 17.87\n",
      "Train Variance Explained: 17.56\n",
      "Test  Variance Explained: 17.63\n",
      "Train R2: 0.986918\n",
      "Test  R2: 0.986903\n"
     ]
    }
   ],
   "source": [
    "tn_pred_50000 = theta_50000[0][0] + theta_50000[0][1] * tn.Salnty + theta_50000[0][2] * tn.STheta\n",
    "tt_pred_50000 = theta_50000[0][0] + theta_50000[0][1] * tt.Salnty + theta_50000[0][2] * tt.STheta\n",
    "\n",
    "tn_rmse_50000 = (((tn_pred_50000 - tn.T_degC)**2).sum() / len(tn))**0.5\n",
    "tt_rmse_50000 = (((tt_pred_50000 - tt.T_degC)**2).sum() / len(tt))**0.5\n",
    "\n",
    "print(\"Train RSME: {}\".format(round(tn_rmse_50000, 4)))\n",
    "print(\"Test  RSME: {}\".format(round(tt_rmse_50000, 4)))\n",
    "\n",
    "tn_var = ((tn.T_degC - tn.T_degC.mean())**2).mean()\n",
    "tt_var = ((tt.T_degC - tt.T_degC.mean())**2).mean()\n",
    "\n",
    "print(\"Train Variance: {}\".format(round(tn_var, 2)))\n",
    "print(\"Test  Variance: {}\".format(round(tt_var, 2)))\n",
    "\n",
    "tn_var_explained_50000 = tn_var - tn_rmse_50000**2\n",
    "tt_var_explained_50000 = tt_var - tt_rmse_50000**2\n",
    "\n",
    "print(\"Train Variance Explained: {}\".format(round(tn_var_explained_50000, 2)))\n",
    "print(\"Test  Variance Explained: {}\".format(round(tt_var_explained_50000, 2)))\n",
    "\n",
    "tn_r2_50000 = tn_var_explained_50000 / tn_var\n",
    "tt_r2_50000 = tt_var_explained_50000 / tt_var\n",
    "\n",
    "print(\"Train R2: {}\".format(round(tn_r2_50000, 6)))\n",
    "print(\"Test  R2: {}\".format(round(tt_r2_50000, 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "This algorithm leads to shockingly accurate results. An R^2 of 0.99 is so good it makes me suspicious. The advantage of scaling data and removing outliers as compared to homework one is obvious. The gradient decent algorithm honed in on the right answer no matter the batch size, but the time it took to output was wildly different. It seems like in this case large batch sizes are better. \n",
    "\n",
    "The normalized results make it a bit more difficult to interpret the coefficients. Instead of units of X they are now in units of standard deviations of X."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
